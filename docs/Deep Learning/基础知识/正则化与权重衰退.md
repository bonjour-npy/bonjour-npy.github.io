# 正则化与权重衰退

## 一、什么是正则化

正则化(Regularization)是机器学习中用于控制模型过拟合的一种技术。在模型训练过程中，我们通常要最小化一个损失函数来得到最佳的模型参数。但是当模型过于复杂时，容易出现过拟合现象，即在训练数据上表现很好，但在测试数据上表现很差。这是因为**模型过于依赖训练数据的噪声和细节，而忽略了真正的规律。**

正则化通过在损失函数中增加一个**惩罚项(Penalty)**来对模型进行约束，防止其过分依赖训练数据。

常见的正则化方法包括L1正则化(硬性限制)、L2正则化(柔性限制)等。

L1正则化会使得一部分参数变为0，从而实现特征选择的效果；L2正则化则会使得模型参数尽量接近0，也就是使得模型更加平滑。在使用正则化时，需要调整正则化强度的超参数，以达到最优的泛化性能。

## 二、L1正则化

$$
min \space l(w, b) \space \text{subject to} \space \Vert w \Vert^2_1 \leq \theta \tag{1}
$$

- L1正则化限制权重参数的L1范数小于某一特定的超参数
- 通常不限制偏移$b$
- 更小的超参数$\theta$意味着更强的正则项

## 三、L2正则化与权重衰退

L2正则化是指在模型的损失函数中，加入对模型参数的L2范数进行惩罚的一种方法。公式如下所示：
$$
l(w, b) + \frac{\lambda}{2} \Vert w \Vert^2_1 \tag{2}
$$
其中，$\lambda$是一个正则化系数超参数

此时在更新梯度时，具有如下公式
$$
\frac{\partial}{\partial w} \big(l(w, b) + \frac{\lambda}{2} \Vert w \Vert^2_1 \big) = \frac{\partial l(w, b)}{\partial w} + \lambda w \tag{3}
$$

$$
w_{t+1}=(1-\eta \lambda)w_t + \eta \frac{\partial l(w_t, b_t)}{\partial w_t} \tag{4}
$$

通常$\eta \lambda < 1$，因此又叫做权重衰退
