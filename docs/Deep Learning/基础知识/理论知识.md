# 理论知识


1. 梯度下降算法需要求整个数据集上的计算损失函数以及梯度，计算代价太大，因此常采用小批量随机梯度下降。在每个batch上计算损失函数以及梯度，近似损失。此时，batchsize越大，近似效果越好。

   随机梯度下降的随机指的就是使用的数据是随机选择的mini batch数据，即Mini-Batch Gradient Descent。

   然而，batchsize越小，收敛效果越好。随机梯度下降理论上带来了噪音，batchsize较小时带来的噪音较大，可以增加模型的鲁棒性。

2. 前向传播（Forward Propagation）：已知权重、偏置和输入，计算出损失函数

   反向传播（Backward Propagation）：求出损失函数对于每一个权重的偏导

3. 交叉熵常来用于衡量两个概率之间的区别

   交叉熵损失函数的梯度是真实概率和预测概率的区别

4. softmax激活函数常用于多分类问题。经过softmax函数后得到的输出为一组概率，概率非负且相加和为1

5. 需要看的论文：ResNet，U-Net

6. 训练优化方法：

   - 初始化：恺明初始化方法
   - 学习率：
   - 动量：逃出局部最小值，可直观理解为惯性

7. 
