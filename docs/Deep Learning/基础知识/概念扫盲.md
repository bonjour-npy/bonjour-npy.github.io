# 概念扫盲


1. 梯度下降算法需要求整个数据集上的计算损失函数以及梯度，计算代价太大，因此常采用小批量随机梯度下降。在每个batch上计算损失函数以及梯度，近似损失。此时，batchsize越大，近似效果越好。

   随机梯度下降的随机指的就是使用的数据是随机选择的mini batch数据，即Mini-Batch Gradient Descent。

   然而，batchsize越小，收敛效果越好。随机梯度下降理论上带来了噪音，batchsize较小时带来的噪音较大，可以增加模型的鲁棒性。

2. 前向传播（Forward Propagation）：已知权重、偏置和输入，计算出损失函数

   反向传播（Backward Propagation）：求出损失函数对于每一个权重的偏导

3. 交叉熵常来用于衡量两个概率之间的区别

   交叉熵损失函数的梯度是真实概率和预测概率的区别

4. softmax激活函数常用于多分类问题。经过softmax函数后得到的输出为一组概率，概率非负且相加和为1

5. 需要看的论文：ResNet，U-Net

6. 训练优化方法：

   - 初始化：恺明初始化方法torch.nn.init.kaiming_normal

     恺明初始化考虑了激活函数对权重分布的影响，使得初始权重能够更好地适应激活函数的特点，从而提高网络性能和训练速度。

     具体来说，恺明初始化将权重初始化为一个均值为0、标准差为$\sqrt{\frac{2}{n}}$的正态分布，其中$n$为权重矩阵中输入单元的数量。对于ReLU激活函数，恺明初始化还可以在根据上述公式初始化后，将所有权重乘以$\sqrt{\frac{2}{1+a^2}}$，其中$a$为ReLU函数的斜率参数（通常为1），以进一步改善网络性能。

     恺明初始化是一种简单有效的**权重**初始化方法，帮助加速神经网络的收敛速度并提高性能。

   - 学习率

   - 动量：逃出局部最小值，可直观理解为惯性

7. Logits，也称为logistic activations，是神经网络模型的最后一层输出结果，通常不经过激活函数（或者只经过softmax函数）进行变换的原始值。在分类问题中，logits表示每个类别的得分，可以用于计算相应的概率分布。

   在深度学习中，logits是神经网络在前向传播计算完输入数据后，最后一层输出的结果。如果是一个二分类问题，通常采用Sigmoid激活函数将logits转化为0到1之间的实数，表示正类的概率；如果是多分类问题，则采用Softmax激活函数将logits进行归一化处理，得到各类别的概率分布。

   需要注意的是，logits是未经过概率推断的原始输出结果，其值可以是任何实数，包括负数和大于1的数，在这种情况下其意义与概率无关。因此，在使用logits作为模型输出进行预测时，需要进行相应的概率转换。
