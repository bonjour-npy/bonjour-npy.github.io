<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Deep-Learning/论文笔记/Meta-Movie-Gen" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">Movie Gen: A Cast of Media Foundation Models | 培洋的笔记本📒</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://bonjour-npy.github.io/./static/img/intro.png"><meta data-rh="true" name="twitter:image" content="https://bonjour-npy.github.io/./static/img/intro.png"><meta data-rh="true" property="og:url" content="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Meta-Movie-Gen"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Movie Gen: A Cast of Media Foundation Models | 培洋的笔记本📒"><meta data-rh="true" name="description" content="Meta 官方博客：https://ai.meta.com/blog/movie-gen-media-foundation-models-generative-ai-video/"><meta data-rh="true" property="og:description" content="Meta 官方博客：https://ai.meta.com/blog/movie-gen-media-foundation-models-generative-ai-video/"><link data-rh="true" rel="icon" href="/img/rockstar-games.svg"><link data-rh="true" rel="canonical" href="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Meta-Movie-Gen"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/en/docs/Deep-Learning/论文笔记/Meta-Movie-Gen" hreflang="en"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Meta-Movie-Gen" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Meta-Movie-Gen" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.2503678d.css">
<script src="/assets/js/runtime~main.d65ec461.js" defer="defer"></script>
<script src="/assets/js/main.e25fffbb.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const a=new URLSearchParams(window.location.search).entries();for(var[t,e]of a)if(t.startsWith("docusaurus-data-")){var n=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(n,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><div class="announcementBar_mb4j" role="banner"><div class="content_knG7 announcementBarContent_xLdY">✨ 求实求真，大气大为 ✨</div></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/navbar.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/navbar.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">培洋的笔记本</b></a><a class="navbar__item navbar__link" href="/docs/Deep-Learning/intro">🤖 深度学习</a><a class="navbar__item navbar__link" href="/docs/Tui-Mian/intro">🤡 推免</a><a class="navbar__item navbar__link" href="/docs/Algorithms/intro">🎰 算法</a><a class="navbar__item navbar__link" href="/docs/Curriculum/intro">📖 课程学习</a><a class="navbar__item navbar__link" href="/docs/Others/intro">☃️ 其他</a><a class="navbar__item navbar__link" href="/docs/Acknowledgement/intro">🍺 饮水思源</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/bonjour-npy" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Deep-Learning/intro">Welcome</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Deep-Learning/Fill-The-Gaps">查漏补缺</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/Deep-Learning/基础知识/AlexNet">基础知识</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/Deep-Learning/代码实现/Visdom-Visualization">代码实现</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/Deep-Learning/大模型基础/Self-Attention">大模型基础</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/Deep-Learning/生成模型总结/Quick-Notes-about-Main-Techs">生成模型总结</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need">论文笔记</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need">NeurIPS 2017: Attention Is All You Need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models">NeurIPS 2020: Denoising Diffusion Probabilistic Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning">CVPR 2023: Zero-shot Generative Model Adaptation via Image-specific Prompt Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation">Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization">Autoregressive Image Generation without Vector Quantization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Latent-Diffusion-Models">CVPR 2022: High-Resolution Image Synthesis with Latent Diffusion Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/MARS-Mixture-of-Auto-Regressive-Models-for-Fine-grained-Text-to-image-Synthesis">MARS: Mixture of Auto-Regressive Models for  Fine-grained Text-to-image Synthesis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Understanding-Diffusion-Models-A-Unified-Perspective">Understanding Diffusion Models: A Unified Perspective</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Deep-Learning/论文笔记/Meta-Movie-Gen">Movie Gen: A Cast of Media Foundation Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Text2LiDAR-Text-guided-LiDAR-Point-Cloud-Generation-via-Equirectangular-Transformer">Text2LiDAR:  Text-guided  LiDAR Point Cloud Generation via Equirectangular Transformer</a></li></ul></li></ul></nav><button type="button" title="收起侧边栏" aria-label="收起侧边栏" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">论文笔记</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Movie Gen: A Cast of Media Foundation Models</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>Movie Gen: A Cast of Media Foundation Models</h1></header>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>相关资料</div><div class="admonitionContent_BuS1"><p>Meta 官方博客：<a href="https://ai.meta.com/blog/movie-gen-media-foundation-models-generative-ai-video/" target="_blank" rel="noopener noreferrer">https://ai.meta.com/blog/movie-gen-media-foundation-models-generative-ai-video/</a></p><p>Meta 官方技术报告：<a href="https://ai.meta.com/static-resource/movie-gen-research-paper" target="_blank" rel="noopener noreferrer">https://ai.meta.com/static-resource/movie-gen-research-paper</a></p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="主要功能展示">主要功能展示<a href="#主要功能展示" class="hash-link" aria-label="主要功能展示的直接链接" title="主要功能展示的直接链接">​</a></h2>
<p>Meta Movie Gen 可以实现具有同步音频的视频生成、个性化角色的视频生成并支持视频编辑。</p>
<p>Movie Gen 实现的主要功能来自于提出的两个 foundation model，分别为 Movie Gen Video 以及 Movie Gen Audio。</p>
<ul>
<li>Movie Gen Video：30B 参数的大模型，支持 T2I 以及 T2V 的联合生成，最高可根据输入的  文本提示生成 16 秒的 1080P HD 视频。</li>
<li>Movie Gen Audio：13B 参数的大模型，支持 V2A 以及 T2A，最高可根据输入的视频以及文本提示生成 48kHz 的高质量同步音频。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="text-to-video-视频生成">Text-to-Video 视频生成<a href="#text-to-video-视频生成" class="hash-link" aria-label="Text-to-Video 视频生成的直接链接" title="Text-to-Video 视频生成的直接链接">​</a></h3>
<video width="100%" height="100%" autoplay="" loop="" controls="">
    <source src="https://video-xsp1-2.xx.fbcdn.net/o1/v/t2/f2/m69/AQPiVwlpt0o56n5kQnldQ-we0lKIfuMSlf2lM95Qmas72Go9TJysToEl6buU1jqT1QnEVTAizFxQpbhKHlJiFJiY.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6Im9lcF9oZCJ9&amp;_nc_ht=video-xsp1-2.xx.fbcdn.net&amp;_nc_cat=107&amp;strext=1&amp;vs=3d8ab693f43fa921&amp;_nc_vs=HBksFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HQWJwaHh2aWozQmFxeUVEQU1kUnNVTmt2RUl6Ym1kakFBQUYVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dPbVJoaHNvYTdpRHk4TURBQkVSZVhnUTJkSlhickZxQUFBRhUCAsgBAEsHiBJwcm9ncmVzc2l2ZV9yZWNpcGUBMQ1zdWJzYW1wbGVfZnBzABB2bWFmX2VuYWJsZV9uc3ViACBtZWFzdXJlX29yaWdpbmFsX3Jlc29sdXRpb25fc3NpbQAoY29tcHV0ZV9zc2ltX29ubHlfYXRfb3JpZ2luYWxfcmVzb2x1dGlvbgAddXNlX2xhbmN6b3NfZm9yX3ZxbV91cHNjYWxpbmcAEWRpc2FibGVfcG9zdF9wdnFzABUAJQAcjBdAAAAAAAAAABERAAAAJr7Mjd7xmusNFQIoAkMzGAt2dHNfcHJldmlldxwXQDob52yLQ5YYGWRhc2hfaDI2NC1iYXNpYy1nZW4yXzcyMHASABgYdmlkZW9zLnZ0cy5jYWxsYmFjay5wcm9kOBJWSURFT19WSUVXX1JFUVVFU1QbCogVb2VtX3RhcmdldF9lbmNvZGVfdGFnBm9lcF9oZBNvZW1fcmVxdWVzdF90aW1lX21zATAMb2VtX2NmZ19ydWxlB3VubXV0ZWQTb2VtX3JvaV9yZWFjaF9jb3VudAM5OTcRb2VtX2lzX2V4cGVyaW1lbnQADG9lbV92aWRlb19pZA81MjM1Mjk4MjA2MzY3ODQSb2VtX3ZpZGVvX2Fzc2V0X2lkDzM4ODk1MzMzNDI2ODM2MhVvZW1fdmlkZW9fcmVzb3VyY2VfaWQQMzg5NDkzMjEyNzQ2MjE3NRxvZW1fc291cmNlX3ZpZGVvX2VuY29kaW5nX2lkDzUzOTg0NDM1NTA4MjQxNw52dHNfcmVxdWVzdF9pZAAlAhwAJb4BGweIAXMEMTcwNQJjZAoyMDI0LTEwLTAzA3JjYgM5MDADYXBwBuinhumikQJjdBFDTVNfTUVESUFfTUFOQUdFUhNvcmlnaW5hbF9kdXJhdGlvbl9zCTI2LjEwOTQxNwJ0cxVwcm9ncmVzc2l2ZV9lbmNvZGluZ3MA&amp;ccb=9-4&amp;oh=00_AYCszJkcEKMLCeqWbFCBk5g-ZCs9Iy63w6lGSbQlp9tEPg&amp;oe=6709790A&amp;_nc_sid=1d576d&amp;_nc_rid=299321509316915&amp;_nc_store_type=1" type="video/mp4">
</video>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="个性化视频">个性化视频<a href="#个性化视频" class="hash-link" aria-label="个性化视频的直接链接" title="个性化视频的直接链接">​</a></h3>
<video width="100%" height="100%" autoplay="" loop="" controls="">
    <source src="https://video-xsp1-2.xx.fbcdn.net/o1/v/t2/f2/m69/AQOvFTvzMc4bgV0UnKk434s09calfDp_gjzyUeu1PT9805-esb4Ri0sObRJ-KI7JmOlqkTI-Q9jpRLi_oZufrdgL.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6Im9lcF9oZCJ9&amp;_nc_ht=video-xsp1-2.xx.fbcdn.net&amp;_nc_cat=109&amp;strext=1&amp;vs=6855ae7eccaeae68&amp;_nc_vs=HBksFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HRTNmaWh1cWd6XzFaczBOQUZLaXFrcmx6cjU1Ym1kakFBQUYVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dIWkxpeHNiT2pDSmM5OERBTmxnNThuMTVJWmZickZxQUFBRhUCAsgBAEsHiBJwcm9ncmVzc2l2ZV9yZWNpcGUBMQ1zdWJzYW1wbGVfZnBzABB2bWFmX2VuYWJsZV9uc3ViACBtZWFzdXJlX29yaWdpbmFsX3Jlc29sdXRpb25fc3NpbQAoY29tcHV0ZV9zc2ltX29ubHlfYXRfb3JpZ2luYWxfcmVzb2x1dGlvbgAddXNlX2xhbmN6b3NfZm9yX3ZxbV91cHNjYWxpbmcAEWRpc2FibGVfcG9zdF9wdnFzABUAJQAcjBdAAAAAAAAAABERAAAAJqyzz-av3twDFQIoAkMzGAt2dHNfcHJldmlldxwXQC-HrhR64UgYGWRhc2hfaDI2NC1iYXNpYy1nZW4yXzcyMHASABgYdmlkZW9zLnZ0cy5jYWxsYmFjay5wcm9kOBJWSURFT19WSUVXX1JFUVVFU1QbCogVb2VtX3RhcmdldF9lbmNvZGVfdGFnBm9lcF9oZBNvZW1fcmVxdWVzdF90aW1lX21zATAMb2VtX2NmZ19ydWxlB3VubXV0ZWQTb2VtX3JvaV9yZWFjaF9jb3VudAM5OTcRb2VtX2lzX2V4cGVyaW1lbnQADG9lbV92aWRlb19pZBA0MzMxOTcxNjgwMzYyMzgzEm9lbV92aWRlb19hc3NldF9pZBA4ODI4NTIxODU3MTYwNDkyFW9lbV92aWRlb19yZXNvdXJjZV9pZBAxMDQ4MzU2MzkzMTg0NDcwHG9lbV9zb3VyY2VfdmlkZW9fZW5jb2RpbmdfaWQQMjU4NTExOTI0ODU0MjQzNw52dHNfcmVxdWVzdF9pZAAlAhwAJb4BGweIAXMEOTU4OQJjZAoyMDI0LTEwLTAzA3JjYgM5MDADYXBwBuinhumikQJjdBFDTVNfTUVESUFfTUFOQUdFUhNvcmlnaW5hbF9kdXJhdGlvbl9zCDE1Ljc2NTc1AnRzFXByb2dyZXNzaXZlX2VuY29kaW5ncwA&amp;ccb=9-4&amp;oh=00_AYBxAm-MDVsX_8GtEyDxahhS6vwG2NEYpWkszJAApMFH3w&amp;oe=67098AF0&amp;_nc_sid=1d576d&amp;_nc_rid=519931164051780&amp;_nc_store_type=1" type="video/mp4">
</video>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="视频精确编辑">视频精确编辑<a href="#视频精确编辑" class="hash-link" aria-label="视频精确编辑的直接链接" title="视频精确编辑的直接链接">​</a></h3>
<video width="100%" height="100%" autoplay="" loop="" controls="">
    <source src="https://video-xsp1-2.xx.fbcdn.net/o1/v/t2/f2/m69/AQPADa5iZqEcmRQ9qzlM4unPbF35nZRKF9_l4ENCmQnjRtjlrmSUj3iaa_JcdqtpVhhJtS_1z5cY4OPR0X5hhsah.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6Im9lcF9oZCJ9&amp;_nc_ht=video-xsp1-2.xx.fbcdn.net&amp;_nc_cat=108&amp;strext=1&amp;vs=c39330cf123af574&amp;_nc_vs=HBksFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HTFhmaXh1cUhUa1BIRW9IQU9weWtGS2hGbmR3Ym1kakFBQUYVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dCeDZqQnRzWUs4U3lkUURBR3d6SzJVaXYzOTdickZxQUFBRhUCAsgBAEsHiBJwcm9ncmVzc2l2ZV9yZWNpcGUBMQ1zdWJzYW1wbGVfZnBzABB2bWFmX2VuYWJsZV9uc3ViACBtZWFzdXJlX29yaWdpbmFsX3Jlc29sdXRpb25fc3NpbQAoY29tcHV0ZV9zc2ltX29ubHlfYXRfb3JpZ2luYWxfcmVzb2x1dGlvbgAddXNlX2xhbmN6b3NfZm9yX3ZxbV91cHNjYWxpbmcAEWRpc2FibGVfcG9zdF9wdnFzABUAJQAcjBdAAAAAAAAAABERAAAAJqCVyY6TlIECFQIoAkMzGAt2dHNfcHJldmlldxwXQEGJul41P30YGWRhc2hfaDI2NC1iYXNpYy1nZW4yXzcyMHASABgYdmlkZW9zLnZ0cy5jYWxsYmFjay5wcm9kOBJWSURFT19WSUVXX1JFUVVFU1QbCogVb2VtX3RhcmdldF9lbmNvZGVfdGFnBm9lcF9oZBNvZW1fcmVxdWVzdF90aW1lX21zATAMb2VtX2NmZ19ydWxlB3VubXV0ZWQTb2VtX3JvaV9yZWFjaF9jb3VudAM5OTcRb2VtX2lzX2V4cGVyaW1lbnQADG9lbV92aWRlb19pZBAxMjk2ODQ3NjU4MzU3ODU3Em9lbV92aWRlb19hc3NldF9pZA84OTY2NTU5MDE4NDc2MDQVb2VtX3ZpZGVvX3Jlc291cmNlX2lkDzU2NTQ5NTEzOTQ3NjgxNhxvZW1fc291cmNlX3ZpZGVvX2VuY29kaW5nX2lkEDEwMzg0NDQ4ODQ2NDkxODYOdnRzX3JlcXVlc3RfaWQAJQIcACW-ARsHiAFzBDcyMzUCY2QKMjAyNC0xMC0wNANyY2IDOTAwA2FwcAbop4bpopECY3QRQ01TX01FRElBX01BTkFHRVITb3JpZ2luYWxfZHVyYXRpb25fcwkzNS4wNzY3MDgCdHMVcHJvZ3Jlc3NpdmVfZW5jb2RpbmdzAA&amp;ccb=9-4&amp;oh=00_AYAGuULjmCpQmV-vrwKfzdSXqaQiG44abmWhlu3yyk6USA&amp;oe=6709B657&amp;_nc_sid=1d576d&amp;_nc_rid=133263937467898&amp;_nc_store_type=1" type="video/mp4">
</video>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="音频生成">音频生成<a href="#音频生成" class="hash-link" aria-label="音频生成的直接链接" title="音频生成的直接链接">​</a></h3>
<video width="100%" height="100%" autoplay="" loop="" muted="" controls="">
    <source src="https://video-xsp1-2.xx.fbcdn.net/o1/v/t2/f2/m69/AQOpzEj_Z2RBkJ41rerEkfPgLmwqdJBSfXbrh42Q8udD2EN3kMqJdk_EBKDRWkqF07JkKjGNQcxpHtI84J3hefeh.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6Im9lcF9oZCJ9&amp;_nc_ht=video-xsp1-2.xx.fbcdn.net&amp;_nc_cat=103&amp;strext=1&amp;vs=a6425d4ff01d627f&amp;_nc_vs=HBksFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HQ3h3aHh0d1NSWk1xZFFCQUd6MV9nVkNWQndfYm1kakFBQUYVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dNd0NoaHVMR2pFN3lLMERBRjZGRldLNFBvbG1ickZxQUFBRhUCAsgBAEsHiBJwcm9ncmVzc2l2ZV9yZWNpcGUBMQ1zdWJzYW1wbGVfZnBzABB2bWFmX2VuYWJsZV9uc3ViACBtZWFzdXJlX29yaWdpbmFsX3Jlc29sdXRpb25fc3NpbQAoY29tcHV0ZV9zc2ltX29ubHlfYXRfb3JpZ2luYWxfcmVzb2x1dGlvbgAddXNlX2xhbmN6b3NfZm9yX3ZxbV91cHNjYWxpbmcAEWRpc2FibGVfcG9zdF9wdnFzABUAJQAcjBdAAAAAAAAAABERAAAAJtbZ5a_Ax_YBFQIoAkMzGAt2dHNfcHJldmlldxwXQD4HrhR64UgYGWRhc2hfaDI2NC1iYXNpYy1nZW4yXzcyMHASABgYdmlkZW9zLnZ0cy5jYWxsYmFjay5wcm9kOBJWSURFT19WSUVXX1JFUVVFU1QbCogVb2VtX3RhcmdldF9lbmNvZGVfdGFnBm9lcF9oZBNvZW1fcmVxdWVzdF90aW1lX21zATAMb2VtX2NmZ19ydWxlB3VubXV0ZWQTb2VtX3JvaV9yZWFjaF9jb3VudAM5OTcRb2VtX2lzX2V4cGVyaW1lbnQADG9lbV92aWRlb19pZA84OTYyNTA0NTIwNzg1NDUSb2VtX3ZpZGVvX2Fzc2V0X2lkEDEwNTI0NDA4MjYxNjQxODQVb2VtX3ZpZGVvX3Jlc291cmNlX2lkDzU0MjE4ODEzMTYyODY1MRxvZW1fc291cmNlX3ZpZGVvX2VuY29kaW5nX2lkEDEyMDA2MjgzNjc4MTEwOTMOdnRzX3JlcXVlc3RfaWQAJQIcACW-ARsHiAFzBDY2NDACY2QKMjAyNC0xMC0wMwNyY2IDOTAwA2FwcAbop4bpopECY3QRQ01TX01FRElBX01BTkFHRVITb3JpZ2luYWxfZHVyYXRpb25fcwUzMC4wMwJ0cxVwcm9ncmVzc2l2ZV9lbmNvZGluZ3MA&amp;ccb=9-4&amp;oh=00_AYC9juBXquGWIlhEZ5BBihwbyj1HfPYgiUNDhrjjzlwYhQ&amp;oe=6709770A&amp;_nc_sid=1d576d&amp;_nc_rid=670838906233448&amp;_nc_store_type=1" type="video/mp4">
</video>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="movie-gen-video">Movie Gen Video<a href="#movie-gen-video" class="hash-link" aria-label="Movie Gen Video的直接链接" title="Movie Gen Video的直接链接">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="joint-image-and-video-generation图像视频联合生成">Joint Image and Video Generation（图像视频联合生成）<a href="#joint-image-and-video-generation图像视频联合生成" class="hash-link" aria-label="Joint Image and Video Generation（图像视频联合生成）的直接链接" title="Joint Image and Video Generation（图像视频联合生成）的直接链接">​</a></h3>
<p>Meta 提出了 Movie Gen Video 这个统一的大模型来同时完成 T2I 以及 T2V 任务，模型将静态图像视为视频中的一帧，从而进行图像和视频生成的联合训练，即 Joint Image and Video Generation，使得模型可以同时生成图像和视频。</p>
<p>作者认为视频数据较为复杂，文本图像对的训练数据可以更好地帮助模型提高泛化性能。</p>
<p>下图展示了图像和视频联合生成的 pipeline。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagestypora_imagesimage-20241010225513173.png" alt="image-20241010225513173" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="多阶段训练策略">多阶段训练策略<a href="#多阶段训练策略" class="hash-link" aria-label="多阶段训练策略的直接链接" title="多阶段训练策略的直接链接">​</a></h3>
<p>为了提高训练效率和模型的扩展能力，作者采用了多阶段的训练过程。</p>
<ul>
<li>
<p>首先是 T2I 预热训练阶段。作者发现直接从头训练图像视频联合生成模型 T2I/V 会导致拟合缓慢，因此首先单独对 T2I 模型进行训练，作为预热阶段，并且在预热训练在较低分辨率（256 px）上进行，可以在相同  的计算开销上以更大的 batch size 训练更多的数据。</p>
</li>
<li>
<p>其次是图像视频联合生成模型 T2I/V 训练阶段。</p>
<p>为了可以成功实现联合训练，作者双倍增加了的空间位置编码层（spatial positional embedding layers）来适应更丰富的宽高比，同时增加了更多的时间位置编码层（temporal positional embedding layers）来支持多帧数图像（视频）的输入。</p>
<p>然后进行高分辨率的 T2I/V 联合训练。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagestypora_imagesimage-20241010223251874.png" alt="image-20241010223251874" class="img_ev3q"></p>
</li>
<li>
<p>在高质量的视频数据集上通过 Supervised Finetuing（监督微调，SFT）来优化生成质量。</p>
</li>
<li>
<p>最后可以后训练（Post Training）的方式来为 Movie Gen 增加个性化角色视频生成以及视频精确编辑等能力。</p>
</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20241010215842535.png" alt="image-20241010215842535" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="taetemporal-autoencoder">TAE（Temporal AutoEncoder）<a href="#taetemporal-autoencoder" class="hash-link" aria-label="TAE（Temporal AutoEncoder）的直接链接" title="TAE（Temporal AutoEncoder）的直接链接">​</a></h3>
<p>为了提高效率，作者提出了 TAE 模型将像素空间的视频和图像压缩到经过学习的时空压缩隐式空间（learned spatial-temporally compreseed latent space），并且学习从隐空间中生成视频。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20241010230313008.png" alt="image-20241010230313008" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="tae-architecturetae-结 构">TAE Architecture（TAE 结构）<a href="#tae-architecturetae-结构" class="hash-link" aria-label="TAE Architecture（TAE 结构）的直接链接" title="TAE Architecture（TAE 结构）的直接链接">​</a></h4>
<p>TAE 的设计采用了 LDM 使用的 Image Encoder 结构并且在<strong>时间维度</strong>进行扩展：</p>
<ol>
<li>
<p><strong>1D 时间卷积（Temporal Convolution）</strong>：在每个二维空间卷积（2D spatial convolution）之后加入 1D 的时间卷积。</p>
<p>这使得模型能够捕捉数据的时间变化特征，从而适应视频的帧间动态变化。在视频生成中，帧之间存在时间上的连续性，时间卷积通过沿时间维度进行卷积操作，使得模型能够从连续帧中提取时间相关的信息。每一帧不仅通过空间卷积捕捉其内容，还通过时间卷积捕捉其与前后帧之间的动态关系。这种操作通过引入 1D 卷积来进行，保证了时间维度上的特征捕获。</p>
</li>
<li>
<p><strong>1D 时间注意力机制（Temporal Attention）</strong>：在每次空间注意力（spatial attention）之后加入 1D 的时间注意力机制。</p>
<p>帮助模型更好地关注视频序列中不同时间点的重要特征。通过在空间注意力之后加入 1D 的时间注意力，模型可以更好地理解视频序列中的时间依赖关系，即哪些时间点的特征对最终生成结果至关重要。这种注意力机制使模型能够在生成过程中关注到不同帧之间的关键时刻，提高视频的生成质量。</p>
</li>
</ol>
<p>在将数据空间中的原始视频压缩到时空隐空间中的时间维度降采样过程中，模型采用<strong>步长为 2 的卷积操作</strong>对视频数据进行压缩，通过这种方式减少时间维度上的冗余信息。而在上采样过程中，模型使用最近邻插值法恢复时间维度的信息，再通过卷积来平滑和补充细节。这一过程保证了视频长度的灵活性，能够处理不同长度的视频序列 。</p>
<p>在下采样中采用在时间维度进行有步长的卷积方式来完成可以使模型能够处理任意帧长度的视频，包括图像（单帧视频），下图展示了时间维度卷积下采样的流程以及计算。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20241011112800493.png" alt="image-20241011112800493" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="improvements-to-the-training-objective损失函数的优化">Improvements to the Training Objective（损失函数的优化）<a href="#improvements-to-the-training-objective损失函数的优化" class="hash-link" aria-label="Improvements to the Training Objective（损失函数的优化）的直接链接" title="Improvements to the Training Objective（损失函数的优化）的直接链接">​</a></h4>
<p>作者发现如果 TAE 使用经典的 VAE 损失函数（Reconstruction Loss、Discriminator Loss、Perceptual Loss）进行训练会在数据空间解码出的视频中出现伪影点，如下图所示。</p>
<p>在 Latent Code 中伪影点位置出现了方差较大的取值，作者认为这是由于模型在伪影点学习存储了全局信息而导致的。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20241011095152080.png" alt="image-20241011095152080" class="img_ev3q"></p>
<p>为了解决这一问题，作者在损失函数中加入了正则项来惩罚模型在 latent code 中生成偏离全局方差的值的行为，具体公式如下所示。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20241011102549444.png" alt="image-20241011102549444" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="efficient-inference-using-temporal-tiling使用时间分片来提高推理效率">Efficient Inference using Temporal Tiling（使用时间分片来提高推理效率）<a href="#efficient-inference-using-temporal-tiling使用时间分片来提高推理效率" class="hash-link" aria-label="Efficient Inference using Temporal Tiling（使用时间分片来提高推理效率）的直接链接" title="Efficient Inference using Temporal Tiling（使用时间分片来提高推理效率）的直接链接">​</a></h4>
<p>由于算力和存储的限制模型直接处理和生成分辨率较高的长视频（如 16 秒的 1080p HD 视频）具有很大的困难，因此作者采用了时间分片（Temporal Tiling）策略，将输入视频以及条件 latent code 沿时间维度进行 tiling，使用 tile 来代替 frame，每一个 tile 的大小统一。</p>
<p>在模型处理时，对每一个 tile 进行 encode 和 decode，在输出时将每个 tile 进行 stitch。</p>
<p>在 tiling 时，由于每个 tile 的帧数需要统一，很有可能会出现重叠（overlaps），因此在输出进行 stitch 时，需要进行额外的加权平均方式来对相邻的 tile 进行混合（blend），分片策略的推理 pipeline 如下图所示，<strong>使得输出视频的帧过渡更加平滑</strong>。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20241011114453020.png" alt="image-20241011114453020" class="img_ev3q"></p>
<p>在 stitch 时具体的混合（blend）策略如下所示：</p>
<p>在 tile blending 时，对于相邻的帧 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7429em;vertical-align:-0.0833em"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>，</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>x</mi><mtext>blend </mtext><mi>j</mi></msubsup><mo>=</mo><munderover><mo>∑</mo><mi>j</mi><mi>N</mi></munderover><mrow><mo fence="true">[</mo><msup><mi>w</mi><mi>j</mi></msup><msubsup><mi>x</mi><mi>i</mi><mi>j</mi></msubsup><mo>+</mo><mrow><mo fence="true">(</mo><mn>1</mn><mo>−</mo><msup><mi>w</mi><mi>j</mi></msup><mo fence="true">)</mo></mrow><msubsup><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>j</mi></msubsup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">x_{\text {blend }}^j=\sum_j^N\left[w^j x_i^j+\left(1-w^j\right) x_{i+1}^j\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2439em;vertical-align:-0.3013em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9426em"><span style="top:-2.3987em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">blend </span></span></span></span></span><span style="top:-3.1809em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.2421em;vertical-align:-1.4138em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size2">[</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8747em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9426em"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size1">(</span></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8747em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9426em"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3352em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size2">]</span></span></span></span></span></span></span>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span> 是 overlapping frames 帧数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span> 的索引，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mi>j</mi></msup><mo>=</mo><mi>j</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">w^j=j / N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8247em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span>。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="training-objective训练目标与损失函数引入-flow-matching">Training Objective（训练目标与损失函数，引入 Flow Matching）<a href="#training-objective训练目标与损失函数引入-flow-matching" class="hash-link" aria-label="Training Objective（训练目标与损失函数，引入 Flow Matching）的直接链接" title="Training Objective（训练目标与损失函数，引入 Flow Matching）的直接链接">​</a></h3>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20241011115920941.png" alt="image-20241011115920941" class="img_ev3q"></p>
<p>在 Movie Gen Video 模型中，引入了 Flow Matching 的生成框架。<strong>流匹配（Flow Matching）它在生成过程中引导模型从噪声逐步逼近真实数据分布。流匹配通过解决微分方程来控制从初始噪声到生成目标的过程。</strong></p>
<p>流匹配的目标是通过微分方程（ODE）来定义生成路径，即让模型学习如何从一个随机初始化的噪声逐步转换为目标数据（如图像或视频）。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="训练阶段的工作机制">训练阶段的工作机制<a href="#训练阶段的工作机制" class="hash-link" aria-label="训练阶段的工作机制的直接链接" title="训练阶段的工作机制的直接链接">​</a></h4>
<ol>
<li>
<p><strong>从噪声开始训练</strong>：</p>
<ul>
<li>训练的起点是从标准正态分布中采样出的噪声。模型的任务是学习如何将这个噪声转化为符合数据分布的图像或视频。</li>
<li>流匹配的关键在于对时间的处理。不同于分数模型的扩散过程，这里通过 ODE 定义了生成路径，并通过优化过程学习如何随着时间演化数据表示 。</li>
</ul>
</li>
<li>
<p><strong>时间维度控制与 TAE 的作用</strong>：</p>
<ul>
<li>Movie Gen 模型中的 **TAE（Temporal Autoencoder）**在编码图像和视频时，会引入时间相关参数（如 1D 时间卷积、时间注意力机制），这使得模型能够学习视频中的时间动态。这对于流匹配至关重要，因为时间卷积使得生成过程中每个时间步都能被显式建模。</li>
<li>流匹配的任务是学习从初始噪声到真实数据的平滑过渡，TAE 为这个过程提供了对时间维度的细粒度控制。因此，流匹配不仅在生成过程中影响时间演化，还通过 TAE 的时间建模来确保生成的视频帧之间的连贯性。</li>
</ul>
</li>
<li>
<p><strong>空间与时间建模的结合与 Transformer 的作用</strong>：</p>
<ul>
<li>Movie Gen 模型中的 <strong>Transformer</strong> 通过自注意力机制，负责捕捉图像和视频中空间和时间的长程依赖关系。在视频生成过程中，Transformer 的作用是确保生成的视频帧在空间结构上是一致的，并在时间维度上保持逻辑连贯性。</li>
<li>流匹配与 Transformer 结合后，生成过程中的每一个时间步不仅会考虑时间维度的动态变化（由流匹配控制），还会通过 Transformer 捕捉到空间和时间上的依赖关系。这使得每一个生成的视频帧既能够遵循流匹配的演化轨迹，又能够在空间和时间上保持一致性。</li>
</ul>
</li>
<li>
<p><strong>训练中的优化目标</strong>：</p>
<ul>
<li>在训练过程中，流匹配的目标是通过最小化生成路径与真实数据分布的差异来优化模型。这个过程类似于分数模型中通过分数函数指导优化，只不过这里的优化是通过 ODE 求解每个时间步的变化率 来进行。</li>
<li>模型通过计算从噪声到数据分布的路径，学习如何将采样的噪声逐步演化成目标图像或视频。这种逐步演化过程在每个时间步都被 ODE 求解器所控制，优化的目标是让模型生成的路径尽可能逼近真实数据的路径。</li>
</ul>
</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="推理阶段的工作机制">推理阶段的工作机制<a href="#推理阶段的工作机制" class="hash-link" aria-label="推理阶段的工作机制的直接链接" title="推理阶段的工作机制的直接链接">​</a></h4>
<ol>
<li>
<p>初始采样： 首先， 从一个标准正态分布中采样一个初始的噪声向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">X_0 \sim \mathcal{N}(0,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathcal" style="margin-right:0.14736em">N</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span> 。这个噪声向量作为推理的起点， 代表了模型生成过程的初始状态。</p>
</li>
<li>
<p>ODE 求解： 通过模型估计出的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>d</mi><msub><mi>X</mi><mi>t</mi></msub></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{d X_t}{d t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2412em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8962em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.4101em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em"><span style="top:-2.357em;margin-left:-0.0785em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>（即噪声随时间的变化率）， 使用常微分方程（ODE）求解器来计算下一个状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">X_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 。ODE 求解器的任务是在给定初始状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 和模型估计的导数信息的基础上，逐步逼近生成的图像或视频帧的最终状态。</p>
</li>
<li>
<p>求解器配置的设计选择：</p>
<p>在具体的 ODE 求解器配置中， 有多种设计选择， 如：</p>
<ul>
<li>求解器阶数：可以选择一阶或更高阶的求解器， 阶数越高， 求解精度越高， 但计算复杂度也相应增加。</li>
<li>步长大小：求解器在每次迭代中前进的步长会影响生成过程的速度和精度， 步长越小， 生成的图像或视频细节越多， 但计算时间也会增加。</li>
<li>容差（Tolerance）：设置 ODE 求解的容差可以控制解的精度与计算成本之间的平衡。</li>
</ul>
</li>
<li>
<p>简单一阶欧拉求解器：论文中提到， 实际应用时， Movie Gen 模型使用了一个简单的一阶欧拉求解器（Euler ODE solver），这个求解器通过逐步估算下一时间步的状态， 适合高效地处理推理阶段的计算。</p>
<p>离散时间步长：模型根据特定的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span> 个离散时间步长进行推理， 这些时间步长是为模型量身定制的， 用以优化生成效果。</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="joint-image-and-video-generation-backbone-architecture骨干网络">Joint Image and Video Generation Backbone Architecture（骨干网络）<a href="#joint-image-and-video-generation-backbone-architecture骨干网络" class="hash-link" aria-label="Joint Image and Video Generation Backbone Architecture（骨干网络）的直接链接" title="Joint Image and Video Generation Backbone Architecture（骨干网络）的直接链接">​</a></h3>
<p>如下图所示，Transformer Backbone 在时空压缩的隐空间中学习如何生成视频的隐式代码（latent code），对于像素空间的视频 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>×</mo><msup><mi>C</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>×</mo><msup><mi>H</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>×</mo><msup><mi>W</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">T&#x27; \times C&#x27; \times H&#x27; \times W&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8352em;vertical-align:-0.0833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8352em;vertical-align:-0.0833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8352em;vertical-align:-0.0833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> 经过时空压缩后得到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>×</mo><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">T \times C \times H \times W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span> 的 latent code，latent code 在送入 Transformer Backbone 之前还需要经过以 3D 卷积为方式实现的 patchify 并最终展平。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagestypora_imagestypora_imagesimage-20241010225513173.png" alt="image-20241010225513173" class="img_ev3q"></p>
<p>作者使用了与 LLaMa3 相似的 Transformer Backbone（即使用了 RMSNorm、SwiGLU 的版本），并且主要做了以下三个改变来适应 Flow Matching 的视频生成：</p>
<ol>
<li>为了使输入的文本条件产生引导作用，在前馈网络 FFN 与自注意力机制之间加入了交叉注意力，同时使用了多个 Text Encoder 来更好地提升条件引导表现。</li>
<li>加入了 Adaptive Layer Normalization Block（adaLN）来适配 Transformer 中的时间步。</li>
<li>使用双向注意力（bi-directional attention）代替单向注意力（casual attention）。</li>
</ol>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20241011144514986.png" alt="image-20241011144514986" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rich-text-embedding-and-visual-text-generation丰富的文本嵌入以及视觉文本生成">Rich Text Embedding and Visual-text Generation（丰富的文本嵌入以及视觉文本生成）<a href="#rich-text-embedding-and-visual-text-generation丰富的文本嵌入以及视觉文本生成" class="hash-link" aria-label="Rich Text Embedding and Visual-text Generation（丰富的文本嵌入以及视觉文本生成）的直接链接" title="Rich Text Embedding and Visual-text Generation（丰富的文本嵌入以及视觉文本生成）的直接链接">​</a></h3>
<p>Movie Gen Video 中使用了两类共三种不同的预训练 Text Encoder 来完成文本嵌入和理解。分别为语义级别的 UL2、Long-prompt MetaCLIP 以及字符级  别的 ByT5。</p>
<ol>
<li>UL2（语义级别）：在大量纯文本数据集上训练，可以提供丰富的文本推理能力。</li>
<li>Long-prompt MetaCLIP（语义级别）：通过在更长的输入文本数据上微调 MetaCLIP 得到的，输入 token 数从 77 增加到了 256。具备多模态的文本-视觉对齐能力。</li>
<li>ByT5（字符级别）：ByT5 主要对输入文本中有关要求在输出的视觉图像中生成文字相关图形的 token 进行编码。</li>
</ol>
<p>三个 Text Encoder 的输出 Text Embedding 在分别经过线性投影以及层归一化后得到 6144 维的向量，并通过 concat 连接起来。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="spatial-upsampling空间上采样">Spatial Upsampling（空间上采样）<a href="#spatial-upsampling空间上采样" class="hash-link" aria-label="Spatial Upsampling（空间上采样）的直接链接" title="Spatial Upsampling（空间上采样）的直接链接">​</a></h3>
<p>Movie Gen Video 单独设计了一个 Spatial Upsampler Transformer 来进行超分辨率扩展，将从 TAE Decoder 中输出的 768 px 视频上采样为 1080p 的高清 HD 视频。分开的设计使得 T2V 模型可以处理更少的 token，因此减少了直接生成高分辨率视频的计算开销。</p>
<p>如下图所示，空间上采样模型的任务可以认为是一个 Video-to-Video 的生成任务。</p>
<ol>
<li>
<p>低分辨率的视频首先在像素空间通过双线性插值（bilinear interploation）扩展到期望的高分辨率。</p>
</li>
<li>
<p>扩展后的高分辨率像素空间视频通过一个 VAE 的 Encoder 编码到 Latent Space 中。</p>
</li>
<li>
<p>隐空间中的视频生成模型（Spatial Upsampler Transformer）生成重建后高分辨率视频的 latent code。</p>
<p>该 Transformer 模型是 T2V Transformer 的小型变种，拥有 7B 的参数量，并由一个在 1024 px 分辨率图像数据集上预训练的 T2I Transformer 来初始化参数。</p>
<p>编码后的原始视频与  生成的输入在通道维度进行 concatnation 后被送入 Spatial Upsampler Transformer 中。</p>
<blockquote>
<p>Implementation details. Our Spatial Upsampler model architecture is a smaller variant (7B parameters) of the text-to-video Transformer initialized from a text-to-image model trained at 1024 px resolution, allowing for better utilization of high-resolution image data. The Spatial Upsampler is trained to predict the latents of a video which are then decoded frame-wise using the VAE’s decoder. Similar to (Girdhar et al., 2024), the encoded video is concatenated channel-wise with the generation input and is fed to the Spatial Upsampler Transformer. The additional parameters at the input, due to concatenation, are zero initialized (Singer et al., 2023; Girdhar et al., 2024). We train our Spatial Upsampler on clips of 14 frames at 24 FPS on ~400K HD videos.</p>
</blockquote>
</li>
<li>
<p>最后通过 VAE 的 Decoder 映射到像素空间得到超分辨率重建的结果。</p>
</li>
</ol>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20241011142226244.png" alt="image-20241011142226244" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="movie-gen-audio">Movie Gen Audio<a href="#movie-gen-audio" class="hash-link" aria-label="Movie Gen Audio的直接链接" title="Movie Gen Audio的直接链接">​</a></h2></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/bonjour-npy/bonjour-npy.github.io/tree/master/docs/Deep-Learning/7-论文笔记/10-Meta-Movie-Gen.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Deep-Learning/论文笔记/Understanding-Diffusion-Models-A-Unified-Perspective"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">Understanding Diffusion Models: A Unified Perspective</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Deep-Learning/论文笔记/Text2LiDAR-Text-guided-LiDAR-Point-Cloud-Generation-via-Equirectangular-Transformer"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">Text2LiDAR:  Text-guided  LiDAR Point Cloud Generation via Equirectangular Transformer</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#主要功能展示" class="table-of-contents__link toc-highlight">主要功能展示</a><ul><li><a href="#text-to-video-视频生成" class="table-of-contents__link toc-highlight">Text-to-Video 视频生成</a></li><li><a href="#个性化视频" class="table-of-contents__link toc-highlight">个性化视频</a></li><li><a href="#视频精确编辑" class="table-of-contents__link toc-highlight">视频精确编辑</a></li><li><a href="#音频生成" class="table-of-contents__link toc-highlight">音频生成</a></li></ul></li><li><a href="#movie-gen-video" class="table-of-contents__link toc-highlight">Movie Gen Video</a><ul><li><a href="#joint-image-and-video-generation图像视频联合生成" class="table-of-contents__link toc-highlight">Joint Image and Video Generation（图像视频联合生成）</a></li><li><a href="#多阶段训练策略" class="table-of-contents__link toc-highlight">多阶段训练策略</a></li><li><a href="#taetemporal-autoencoder" class="table-of-contents__link toc-highlight">TAE（Temporal AutoEncoder）</a><ul><li><a href="#tae-architecturetae-结构" class="table-of-contents__link toc-highlight">TAE Architecture（TAE 结构）</a></li><li><a href="#improvements-to-the-training-objective损失函数的优化" class="table-of-contents__link toc-highlight">Improvements to the Training Objective（损失函数的优化）</a></li><li><a href="#efficient-inference-using-temporal-tiling使用时间分片来提高推理效率" class="table-of-contents__link toc-highlight">Efficient Inference using Temporal Tiling（使用时间分片来提高推理效率）</a></li></ul></li><li><a href="#training-objective训练目标与损失函数引入-flow-matching" class="table-of-contents__link toc-highlight">Training Objective（训练目标与损失函数，引入 Flow Matching）</a><ul><li><a href="#训练阶段的工作机制" class="table-of-contents__link toc-highlight">训练阶段的工作机制</a></li><li><a href="#推理阶段的工作机制" class="table-of-contents__link toc-highlight">推理阶段的工作机制</a></li></ul></li><li><a href="#joint-image-and-video-generation-backbone-architecture骨干网络" class="table-of-contents__link toc-highlight">Joint Image and Video Generation Backbone Architecture（骨干网络）</a></li><li><a href="#rich-text-embedding-and-visual-text-generation丰富的文本嵌入以及视觉文本生成" class="table-of-contents__link toc-highlight">Rich Text Embedding and Visual-text Generation（丰富的文本嵌入以及视觉文本生成）</a></li><li><a href="#spatial-upsampling空间上采样" class="table-of-contents__link toc-highlight">Spatial Upsampling（空间上采样）</a></li></ul></li><li><a href="#movie-gen-audio" class="table-of-contents__link toc-highlight">Movie Gen Audio</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">👋 联系我</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/WeChat_QR_Code.jpg" target="_blank" rel="noopener noreferrer" class="footer__link-item">WeChat<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.douyin.com/user/self?modal_id=7157246567970360614" target="_blank" rel="noopener noreferrer" class="footer__link-item">TikTok<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/bonjour-npy" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">✈️ 外部链接</div><ul class="footer__items clean-list"><li class="footer__item"><a href="http://www.mod.gov.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">共和国国防部<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.xuexi.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">学习强国<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://peacekeeping.un.org/zh" target="_blank" rel="noopener noreferrer" class="footer__link-item">联合国维持和平<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">🎅 彩蛋</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.rockstargames.com/gta-v" target="_blank" rel="noopener noreferrer" class="footer__link-item">欢迎来到洛圣都<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.starwars.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">星球大战<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apple.com.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Apple(中国大陆)<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">🦄 教育官网</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.uestc.edu.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">电子科技大学<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.guet.edu.cn" target="_blank" rel="noopener noreferrer" class="footer__link-item">桂林电子科技大学<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://cfm.uestc.edu.cn/index" target="_blank" rel="noopener noreferrer" class="footer__link-item">未来媒体研究中心<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright"><br>本网站所展示的标识、链接均属于个人创作和喜好表达，不代表任何国家、政府、企业或组织的官方立场或行为。<br>
                    尽管本网站努力确保信息的准确性和时效性，但所有信息仅供参考，并不构成任何形式的法律、财务或商业建议。<br>
                    <br>Copyright © 2024 bonjour-npy. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>