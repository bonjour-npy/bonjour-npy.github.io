<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">CVPR 2023: Zero-shot Generative Model Adaptation via Image-specific Prompt Learning | 培洋的笔记本📒</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://bonjour-npy.github.io/./static/img/intro.png"><meta data-rh="true" name="twitter:image" content="https://bonjour-npy.github.io/./static/img/intro.png"><meta data-rh="true" property="og:url" content="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="CVPR 2023: Zero-shot Generative Model Adaptation via Image-specific Prompt Learning | 培洋的笔记本📒"><meta data-rh="true" name="description" content="论文：CVPR 2023 open access"><meta data-rh="true" property="og:description" content="论文：CVPR 2023 open access"><link data-rh="true" rel="icon" href="/img/rockstar-games.svg"><link data-rh="true" rel="canonical" href="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning" hreflang="en"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.165e4dc2.css">
<link rel="preload" href="/assets/js/runtime~main.249e2c80.js" as="script">
<link rel="preload" href="/assets/js/main.2be429d9.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><div class="announcementBar_mb4j" role="banner"><div class="content_knG7 announcementBarContent_xLdY">求实求真，大气大为</div></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/UESTC_logo.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/UESTC_logo.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">培洋的笔记本</b></a><a class="navbar__item navbar__link" href="/docs/Deep-Learning/intro">🤖深度学习</a><a class="navbar__item navbar__link" href="/docs/Tui-Mian/intro">🤡推免</a><a class="navbar__item navbar__link" href="/docs/Algorithms/intro">🎰算法</a><a class="navbar__item navbar__link" href="/docs/Curriculum/intro">📖课程学习</a><a class="navbar__item navbar__link" href="/docs/Others/intro">☃️其他</a><a class="navbar__item navbar__link" href="/docs/Acknowledgement/intro">🍺饮水思源</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/bonjour-npy" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Deep-Learning/intro">Welcome</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Deep-Learning/Fill-The-Gaps">查漏补缺</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Deep-Learning/基础知识/AlexNet">基础知识</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Deep-Learning/实战练习/Visdom Visualization">实战练习</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Deep-Learning/大模型笔记/Self-Attention">大模型笔记</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need">论文笔记</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need">NeurIPS 2017: Attention Is All You Need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models">NeurIPS 2020: Denoising Diffusion Probabilistic Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models">CVPR 2022: High-Resolution Image Synthesis with Latent Diffusion Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning">CVPR 2023: Zero-shot Generative Model Adaptation via Image-specific Prompt Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Recent-Advances-in-Image-Generative-Foundation-Models-CVPR24">CVPR2024 Tutorial：图像生成基座模型的近期进展</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Video-and-3D-Generation-CVPR24">CVPR2024 Tutorial：视频和 3D 生成</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Image-and-Video-Generative-Foundation-Model">图像生成和视频生成基座模型</a></li></ul></li></ul></nav><button type="button" title="收起侧边栏" aria-label="收起侧边栏" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">论文笔记</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">CVPR 2023: Zero-shot Generative Model Adaptation via Image-specific Prompt Learning</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><h1>CVPR 2023: Zero-shot Generative Model Adaptation via Image-specific Prompt Learning</h1><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>相关链接</div><div class="admonitionContent_S0QG"><p>论文：<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Zero-Shot_Generative_Model_Adaptation_via_Image-Specific_Prompt_Learning_CVPR_2023_paper.pdf" target="_blank" rel="noopener noreferrer">CVPR 2023 open access</a></p><p>代码：<a href="https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation" target="_blank" rel="noopener noreferrer">Piscart-AI-Research</a></p><p>文章的命名风格借鉴了CVPR 2022的文章<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xiao_Few_Shot_Generative_Model_Adaption_via_Relaxed_Spatial_Structural_Alignment_CVPR_2022_paper.pdf" target="_blank" rel="noopener noreferrer">Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment</a></p></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="摘要">摘要<a href="#摘要" class="hash-link" aria-label="摘要的直接链接" title="摘要的直接链接">​</a></h2><p>本文提出了Image-specific Prompt Learning（IPL）方法来解决<strong>风格迁移任务</strong>中生成模型<strong>从源域到目标域的适应</strong>问题。一个Latent Mapper来从源域图像中学习出<strong>包含图像特征</strong>且<strong>适应目标域</strong>的prompt，从而指导目标域生成器的训练。</p><blockquote><p>This produces a more precise adaptation direction for every cross-domain image pair, endowing the target-domain generator with greatly enhanced flexibility.</p></blockquote><p>训练资料是源域和目标域的文字标签以及源域的图像，<strong>并不需要目标域的图像</strong>。此外，IPL独立于生成模型，可以自由选择Diffusion Model或GAN等。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="相关工作">相关工作<a href="#相关工作" class="hash-link" aria-label="相关工作的直接链接" title="相关工作的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="generative-model-adaption">Generative Model Adaption<a href="#generative-model-adaption" class="hash-link" aria-label="Generative Model Adaption的直接链接" title="Generative Model Adaption的直接链接">​</a></h3><p>Generative Model Adaption的任务是使在大规模源域图片上训练的生成模型适应到数据有限的目标域中，根据目标域训练资料的大小可以分为few-shot和zero-shot。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="few-shot">few-shot<a href="#few-shot" class="hash-link" aria-label="few-shot的直接链接" title="few-shot的直接链接">​</a></h4><p>对于few-shot任务，一般是通过有限的目标域训练集资料fine-tune预训练模型。</p><p>然而，fine-tune通常会导致过拟合。为了解决过拟合问题，通常使用的方法是施加强正则化、使用扰动法、跨域对齐或数据增强。</p><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>相关文献方法</div><div class="admonitionContent_S0QG"><ul><li>强正则化：Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for generative adversarial networks. In ICLR, 2019.</li><li>扰动法：Sangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze the discriminator: a simple baseline for fine-tuning GANs. In CVPR Workshops, 2020.</li><li>跨域对齐：Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A Efros, Yong Jae Lee, Eli Shechtman, and Richard Zhang. Fewshot image generation via cross-domain correspondence. In CVPR, 2021.</li><li>数据增强：Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and Ngai-Man Cheung. On data augmentation for GAN training. TIP, 2021.</li></ul></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="zero-shot">zero-shot<a href="#zero-shot" class="hash-link" aria-label="zero-shot的直接链接" title="zero-shot的直接链接">​</a></h4><p>对于零样本的图像生成模型的适应任务，<a href="https://arxiv.org/pdf/2108.00946.pdf" target="_blank" rel="noopener noreferrer">NADA</a>率先引入了CLIP模型来获取必须的先验知识，通过预训练大模型的语言理解能力实现<strong>在目标域只需要文字标签</strong>而不需要图片，将源域和目标域之间的差距编码为在CLIP空间上文字引导的适应方向。</p><p>此后，CVPR 2022发表的<a href="https://arxiv.org/pdf/2110.02711.pdf" target="_blank" rel="noopener noreferrer">DiffusionCLIP</a>使用了Diffusion模型代替NADA中的StyleGANs，获得了更好的特征保存能力。</p><p>然而这些方法都是采用了固定的适应方向，只包含基础的域知识，而不是图片特定的特征。在本文中，作者发现这种共享的、固定的适应方向会导致Mode Collapse（模式坍塌），因此提出了从每个源域图像中学习出多样且准确的prompt，为生成模型向目标域的适应提供更精确的方向。</p><p><img loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231221214755712.png" alt="image-20231221214755712" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-learning">Prompt Learning<a href="#prompt-learning" class="hash-link" aria-label="Prompt Learning的直接链接" title="Prompt Learning的直接链接">​</a></h3><p>Prompt工程最初是一种Knowledge Probing（知识探测）方法，给定完形填空（cloze-style）类的prompt，引导模型产生相对应的答案。</p><p>然而人工设计的prompt通常不是最优的，可能提供不准确的适应方向。为了解决这个问题，在NLP领域的Prompt Learning发展迅速，并随着视觉-语言大模型的发展，应用在了视觉任务中。</p><p>Kaiyang Zhou等人首先在图像分类任务中采用上下文优化，在词嵌入空间中对具有连续向量的上下文词进行建模。随后Prompt Learning在计算机视觉中的许多下游任务都得到了探索，例如目标检测、视频理解和迁移学习等。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="主要方法">主要方法<a href="#主要方法" class="hash-link" aria-label="主要方法的直接链接" title="主要方法的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="概述">概述<a href="#概述" class="hash-link" aria-label="概述的直接链接" title="概述的直接链接">​</a></h3><p>IPL方法分两个阶段。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="第一阶段训练latent-mapper">第一阶段：训练Latent Mapper<a href="#第一阶段训练latent-mapper" class="hash-link" aria-label="第一阶段：训练Latent Mapper的直接链接" title="第一阶段：训练Latent Mapper的直接链接">​</a></h4><p>第一阶段的主要任务是训练Lantent Mapper来为每一个训练集的源域图片生成一组prompt。Latent Mapper接收源域图像的latent representation，生成一组prompt向量。第一阶段需要解决两个问题，即在zero-shot的背景下，如何实现prompt与源域图像特征的对齐以及prompt与目标域空间的对齐，因此第一阶段的训练分两部分进行。</p><p>第一部分是Latent Mapper输出的prompt与目标域标签concat后送入来自CLIP的Text Encoder得到目标域图片prompt在CLIP空间的编码表示，并与目标域标签经过Text Encoder后的编码共同作为Domain Loss的输入来约束从源域中学习到的prompt与目标域空间对齐。</p><p>第二部分是Latent Mapper输出的prompt与源域标签concat后送入来自CLIP的Text Encoder得到源域图片prompt描述在CLIP空间的编码表示，同时源域图像再经过来自CLIP的Image Encoder后得到其在CLIP空间的编码表示。将源域的prompt文字和图像编码表示作为contrastive learning loss的输入，约束学习到的prompt与源域图像的特征对齐。</p><p><img loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesprompt%E4%BA%A7%E7%94%9F%E7%AD%96%E7%95%A5.jpg" alt="prompt产生策略" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="第二阶段将latent-mapper插入目标域生成器的训练过程">第二阶段：将Latent Mapper插入目标域生成器的训练过程<a href="#第二阶段将latent-mapper插入目标域生成器的训练过程" class="hash-link" aria-label="第二阶段：将Latent Mapper插入目标域生成器的训练过程的直接链接" title="第二阶段：将Latent Mapper插入目标域生成器的训练过程的直接链接">​</a></h4><p>第二阶段利用Directional CLIP Loss来训练目标域生成器，使源于生成器向目标域迁移学习。需要输入源域以及目标域图像、源域以及目标域的prompt描述。源域图像的latent representation分别输入至源域生成器和目标域生成器中得到对应的图像，同时指导风格迁移方向的源域以及目标域的prompt描述由Latent Mapper接收源域图像的隐式表示后输出再分别与源域和目标域标签concat而得到。分别将源域图像、生成的目标域图像以及源域、目标域的图片prompt描述一起输入至Directional CLIP Loss，从而约束由源域图像生成器初始化的目标域图像生成器向目标域的迁移学习。</p><p><img loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231221231045323.png" alt="image-20231221231045323" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-specific-prompt-learning">Image-specific Prompt Learning<a href="#image-specific-prompt-learning" class="hash-link" aria-label="Image-specific Prompt Learning的直接链接" title="Image-specific Prompt Learning的直接链接">​</a></h3></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Deep-Learning/论文笔记/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">CVPR 2022: High-Resolution Image Synthesis with Latent Diffusion Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Deep-Learning/论文笔记/Recent-Advances-in-Image-Generative-Foundation-Models-CVPR24"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">CVPR2024 Tutorial：图像生成基座模型的近期进展</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#摘要" class="table-of-contents__link toc-highlight">摘要</a></li><li><a href="#相关工作" class="table-of-contents__link toc-highlight">相关工作</a><ul><li><a href="#generative-model-adaption" class="table-of-contents__link toc-highlight">Generative Model Adaption</a><ul><li><a href="#few-shot" class="table-of-contents__link toc-highlight">few-shot</a></li><li><a href="#zero-shot" class="table-of-contents__link toc-highlight">zero-shot</a></li></ul></li><li><a href="#prompt-learning" class="table-of-contents__link toc-highlight">Prompt Learning</a></li></ul></li><li><a href="#主要方法" class="table-of-contents__link toc-highlight">主要方法</a><ul><li><a href="#概述" class="table-of-contents__link toc-highlight">概述</a><ul><li><a href="#第一阶段训练latent-mapper" class="table-of-contents__link toc-highlight">第一阶段：训练Latent Mapper</a></li><li><a href="#第二阶段将latent-mapper插入目标域生成器的训练过程" class="table-of-contents__link toc-highlight">第二阶段：将Latent Mapper插入目标域生成器的训练过程</a></li></ul></li><li><a href="#image-specific-prompt-learning" class="table-of-contents__link toc-highlight">Image-specific Prompt Learning</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">👋联系我</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/WeChat_QR_Code.jpg" target="_blank" rel="noopener noreferrer" class="footer__link-item">WeChat<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.douyin.com/user/self?modal_id=7157246567970360614" target="_blank" rel="noopener noreferrer" class="footer__link-item">TikTok<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/bonjour-npy" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">✈️外部链接</div><ul class="footer__items clean-list"><li class="footer__item"><a href="http://www.mod.gov.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">共和国国防部<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.xuexi.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">学习强国<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://peacekeeping.un.org/zh" target="_blank" rel="noopener noreferrer" class="footer__link-item">联合国维持和平<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">🎅彩蛋</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.rockstargames.com/gta-v" target="_blank" rel="noopener noreferrer" class="footer__link-item">欢迎来到洛圣都<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.starwars.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">星球大战<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apple.com.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Apple(中国大陆)<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">🦄教育官网</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.uestc.edu.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">电子科技大学<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.guet.edu.cn" target="_blank" rel="noopener noreferrer" class="footer__link-item">桂林电子科技大学<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://cfm.uestc.edu.cn/index" target="_blank" rel="noopener noreferrer" class="footer__link-item">未来媒体研究中心<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright"><br>本网站所展示的标识、链接均属于个人创作和喜好表达，不代表任何国家、政府、企业或组织的官方立场或行为。<br>
                    尽管本网站努力确保信息的准确性和时效性，但所有信息仅供参考，并不构成任何形式的法律、财务或商业建议。<br>
                    <br>Copyright © 2024 bonjour-npy. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.249e2c80.js"></script>
<script src="/assets/js/main.2be429d9.js"></script>
</body>
</html>