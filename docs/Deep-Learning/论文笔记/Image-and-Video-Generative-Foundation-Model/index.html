<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Deep-Learning/论文笔记/Image-and-Video-Generative-Foundation-Model" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">图像生成和视频生成基座模型 | 培洋的笔记本📒</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://bonjour-npy.github.io/./static/img/intro.png"><meta data-rh="true" name="twitter:image" content="https://bonjour-npy.github.io/./static/img/intro.png"><meta data-rh="true" property="og:url" content="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Image-and-Video-Generative-Foundation-Model"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="图像生成和视频生成基座模型 | 培洋的笔记本📒"><meta data-rh="true" name="description" content="图像生成基座模型"><meta data-rh="true" property="og:description" content="图像生成基座模型"><link data-rh="true" rel="icon" href="/img/rockstar-games.svg"><link data-rh="true" rel="canonical" href="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Image-and-Video-Generative-Foundation-Model"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/en/docs/Deep-Learning/论文笔记/Image-and-Video-Generative-Foundation-Model" hreflang="en"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Image-and-Video-Generative-Foundation-Model" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/docs/Deep-Learning/论文笔记/Image-and-Video-Generative-Foundation-Model" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.165e4dc2.css">
<link rel="preload" href="/assets/js/runtime~main.249e2c80.js" as="script">
<link rel="preload" href="/assets/js/main.2be429d9.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><div class="announcementBar_mb4j" role="banner"><div class="content_knG7 announcementBarContent_xLdY">求实求真，大气大为</div></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/UESTC_logo.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/UESTC_logo.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">培洋的笔记本</b></a><a class="navbar__item navbar__link" href="/docs/Deep-Learning/intro">🤖深度学习</a><a class="navbar__item navbar__link" href="/docs/Tui-Mian/intro">🤡推免</a><a class="navbar__item navbar__link" href="/docs/Algorithms/intro">🎰算法</a><a class="navbar__item navbar__link" href="/docs/Curriculum/intro">📖课程学习</a><a class="navbar__item navbar__link" href="/docs/Others/intro">☃️其他</a><a class="navbar__item navbar__link" href="/docs/Acknowledgement/intro">🍺饮水思源</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/bonjour-npy" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Deep-Learning/intro">Welcome</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Deep-Learning/Fill-The-Gaps">查漏补缺</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Deep-Learning/基础知识/AlexNet">基础知识</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Deep-Learning/实战练习/Visdom Visualization">实战练习</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Deep-Learning/大模型笔记/Self-Attention">大模型笔记</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need">论文笔记</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need">NeurIPS 2017: Attention Is All You Need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models">NeurIPS 2020: Denoising Diffusion Probabilistic Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models">CVPR 2022: High-Resolution Image Synthesis with Latent Diffusion Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning">CVPR 2023: Zero-shot Generative Model Adaptation via Image-specific Prompt Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Recent-Advances-in-Image-Generative-Foundation-Models-CVPR24">CVPR2024 Tutorial：图像生成基座模型的近期进展</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/论文笔记/Video-and-3D-Generation-CVPR24">CVPR2024 Tutorial：视频和 3D 生成</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Deep-Learning/论文笔记/Image-and-Video-Generative-Foundation-Model">图像生成和视频生成基座模型</a></li></ul></li></ul></nav><button type="button" title="收起侧边栏" aria-label="收起侧边栏" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">论文笔记</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">图像生成和视频生成基座模型</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><h1>图像生成和视频生成基座模型</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="图像生成基座模型">图像生成基座模型<a href="#图像生成基座模型" class="hash-link" aria-label="图像生成基座模型的直接链接" title="图像生成基座模型的直接链接">​</a></h2><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>参考链接</div><div class="admonitionContent_S0QG"><p>原文 URL：<a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2024/Zhengyuan_Image_Generation.pdf" target="_blank" rel="noopener noreferrer">Recent Advances in (Image) Generative Foundation Models</a></p></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="text-to-image-生成范式">Text-To-Image 生成范式<a href="#text-to-image-生成范式" class="hash-link" aria-label="Text-To-Image 生成范式的直接链接" title="Text-To-Image 生成范式的直接链接">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-gan">1. GAN<a href="#1-gan" class="hash-link" aria-label="1. GAN的直接链接" title="1. GAN的直接链接">​</a></h4><p>使用对抗生成策略，判别器根据真实图像判断生成器生成的图像是否逼真，二者交替训练。</p><p><img loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesGAN.png" alt="GAN" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-autoregressivear">2. Autoregressive（AR）<a href="#2-autoregressivear" class="hash-link" aria-label="2. Autoregressive（AR）的直接链接" title="2. Autoregressive（AR）的直接链接">​</a></h4><p>自回归生成范式，利用输入自身之前各期 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_1,...,x_{t-1}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span> 来预测本期 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>​ 的表现。在图像生成中，自回归模型可以逐像素或逐块生成图像，每一步的生成基于之前已经生成的部分。自回归模型的优点在于能够捕捉图像中的复杂依赖关系，从而生成更加逼真的图像。</p><h5 class="anchor anchorWithStickyNavbar_LWe7" id="代表模型vit-vqgan">代表模型：<a href="https://arxiv.org/pdf/2110.04627" target="_blank" rel="noopener noreferrer">ViT-VQGAN</a><a href="#代表模型vit-vqgan" class="hash-link" aria-label="代表模型vit-vqgan的直接链接" title="代表模型vit-vqgan的直接链接">​</a></h5><p><img loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20240705102041673.png" alt="image-20240705102041673" class="img_ev3q"></p><p><strong>VQ (Vector Quantization) 的改进</strong>：在原有的 VQ-VAE 基础上进行了改进，通过引入更复杂的量化器和更强大的解码器，使得生成的图像质量得到了显著提升。</p><p><strong>GAN (Generative Adversarial Network) 的结合</strong>：将 VQ 和 GAN 结合，利用 GAN 的判别器来提升生成图像的细节和逼真度。</p><h5 class="anchor anchorWithStickyNavbar_LWe7" id="代表模型var">代表模型：<a href="https://arxiv.org/pdf/2404.02905" target="_blank" rel="noopener noreferrer">VAR</a><a href="#代表模型var" class="hash-link" aria-label="代表模型var的直接链接" title="代表模型var的直接链接">​</a></h5><p><img loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20240705104016645.png" alt="image-20240705104016645" class="img_ev3q"></p><p>上图展示了不同方式的自回归生成模型，VAR 方法在每个时间序列节点上都根据之前各时间步的输出预测出当前时间步的，且每个时间步均预测出完整的目标图像，且分辨率随时间推移逐步提升至高清图像，即 <strong>next-resolution prediction</strong>。</p><p><img loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20240705103947258.png" alt="image-20240705103947258" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3-non-ar-transformer">3. Non-AR Transformer<a href="#3-non-ar-transformer" class="hash-link" aria-label="3. Non-AR Transformer的直接链接" title="3. Non-AR Transformer的直接链接">​</a></h4><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>参考资料</div><div class="admonitionContent_S0QG"><p><a href="https://wrong.wang/blog/20230107-%E7%94%9F%E6%88%90%E5%91%A8%E5%88%8A%E7%AC%AC%E4%B8%80%E6%9C%9F/" target="_blank" rel="noopener noreferrer">生成周刊·第一期</a></p></div></div><h5 class="anchor anchorWithStickyNavbar_LWe7" id="代表模型maskgit-masked-generative-image-transformer">代表模型：<a href="https://arxiv.org/pdf/2202.04200v1" target="_blank" rel="noopener noreferrer">MaskGIT: Masked Generative Image Transformer</a><a href="#代表模型maskgit-masked-generative-image-transformer" class="hash-link" aria-label="代表模型maskgit-masked-generative-image-transformer的直接链接" title="代表模型maskgit-masked-generative-image-transformer的直接链接">​</a></h5><p><img loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20240705110446519.png" alt="image-20240705110446519" class="img_ev3q"></p><p>这种生成模型依赖一个预训练好的 VQGAN，能将图片 tokenize 成一组量化后的 visual tokens。VQGAN 编码图片得到的 tokens 是离散的，所有可能的 tokens 构成一个 codebook，假设其中包含 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> 个 token 选项。MVTM 训练就是指给定 masked tokens，让网络预测这些被 masked 掉的 tokens。对于每个被 masked 掉的 token，网络给出一个 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> 维向量预测当前 token 属于 codebook 中每个 token 的可能性，类似于完成一个 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> 分类任务。</p><p><img loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20240705110625208.png" alt="image-20240705110625208" class="img_ev3q"></p><p>上图展示了传统 AR 模型和 MaskGIT 在推理过程中的区别。与之前 SOTA 使用的 Autoregressive 方法——<strong>逐行再逐列依次生成</strong> image token 不同，MaskGIT 在推理时的每次迭代从一组 masked tokens 中预测出每个位置出现 visual token 的可能性，然后仅保留那些置信度足够高的位置的 visual token，然后继续将当前预测结果再送入网络进行下一轮预测，直到所有位置的 visual token 都被预测出来。与之前常用的自回归方法不同，每轮预测都是基于对图片的<strong>全局感知</strong>，可以<strong>并行预测</strong>。这样网络仅需 8 次前向传播就能生成高质量的图片。</p><blockquote><p>顺序解码与 MaskGIT 计划并行解码的比较。第 1 行和第 3 行是每次迭代时的输入潜在掩码，第 2 行和第 4 行是每次迭代时每个模型生成的样本。MaskGIT 的解码从所有未知代码（浅灰色标记）开始，逐渐用更多更分散的预测并行填充潜表征（深灰色标记），预测标记的数量随着迭代急剧增加。MaskGIT 只用了 8 次迭代就完成了解码，而顺序法需要 256 轮。</p></blockquote><h5 class="anchor anchorWithStickyNavbar_LWe7" id="代表模型mage-masked-generative-encoder-to-unify-representation-learning-and-image-synthesis">代表模型：<a href="https://arxiv.org/pdf/2211.09117" target="_blank" rel="noopener noreferrer">MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis</a><a href="#代表模型mage-masked-generative-encoder-to-unify-representation-learning-and-image-synthesis" class="hash-link" aria-label="代表模型mage-masked-generative-encoder-to-unify-representation-learning-and-image-synthesis的直接链接" title="代表模型mage-masked-generative-encoder-to-unify-representation-learning-and-image-synthesis的直接链接">​</a></h5><h4 class="anchor anchorWithStickyNavbar_LWe7" id="4diffusion-model">4.Diffusion Model<a href="#4diffusion-model" class="hash-link" aria-label="4.Diffusion Model的直接链接" title="4.Diffusion Model的直接链接">​</a></h4><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>参考链接</div><div class="admonitionContent_S0QG"><p><a href="https://www.cnblogs.com/wxkang/p/17128108.html" target="_blank" rel="noopener noreferrer">深度理解变分自编码器(VAE) | 从入门到精通 </a></p></div></div><h5 class="anchor anchorWithStickyNavbar_LWe7" id="代表模型vae">代表模型：VAE<a href="#代表模型vae" class="hash-link" aria-label="代表模型：VAE的直接链接" title="代表模型：VAE的直接链接">​</a></h5><p>VAE 和 VQ-VAE 都通过学习数据分布的潜在表示来生成新的样本。VAE 使用高斯分布来表示潜在空间，而 VQ-VAE 使用离散的代码簿来表示潜在空间。</p><p>具体来说，VAE 的工作原理是通过一个编码器将输入数据映射到一个潜在空间，然后通过一个解码器将潜在空间中的向量重构为原始数据。在训练过程中，VAE 会学习到数据分布的潜在表示，并能够生成与训练数据类似的新样本。</p><p>VQ-VAE 的工作原理与 VAE 类似，但它使用离散的代码簿来表示潜在空间。VQ-VAE 首先将编码器输出的向量进行量化，将其映射到代码簿中的最近向量。然后，解码器使用代码簿中的向量来重构原始数据。VQ-VAE 的优势在于，它可以学习到数据中的离散结构和语义信息，并可以避免过拟合。</p><h5 class="anchor anchorWithStickyNavbar_LWe7" id="代表模型vq-vae">代表模型：VQ-VAE<a href="#代表模型vq-vae" class="hash-link" aria-label="代表模型：VQ-VAE的直接链接" title="代表模型：VQ-VAE的直接链接">​</a></h5><h3 class="anchor anchorWithStickyNavbar_LWe7" id="如何训练优秀的生成基座模型">如何训练优秀的生成基座模型？<a href="#如何训练优秀的生成基座模型" class="hash-link" aria-label="如何训练优秀的生成基座模型？的直接链接" title="如何训练优秀的生成基座模型？的直接链接">​</a></h3><ol><li>数据：Re-caption 与 text encoder（T5）</li><li>结构：从 U-Net 到纯 Transformer，代表论文 <a href="https://arxiv.org/pdf/2212.09748" target="_blank" rel="noopener noreferrer">Scalable Diffusion Models with Transformers</a></li><li>训练范式：使用 Rectified Flow 加速生成过程，参考链接 <a href="https://zhuanlan.zhihu.com/p/638118847" target="_blank" rel="noopener noreferrer">Diffusion学习笔记（十二）——Rectified Flow</a></li></ol><p><img loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20240705151423108.png" alt="image-20240705151423108" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="视频生成基座模型">视频生成基座模型<a href="#视频生成基座模型" class="hash-link" aria-label="视频生成基座模型的直接链接" title="视频生成基座模型的直接链接">​</a></h2><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>参考链接</div><div class="admonitionContent_S0QG"><p>原文 URL：<a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2024/Kevin_Video_3D_Generation.pdf" target="_blank" rel="noopener noreferrer">Video and 3D Generation</a></p></div></div><p>先驱性工作：</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Deep-Learning/论文笔记/Video-and-3D-Generation-CVPR24"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">CVPR2024 Tutorial：视频和 3D 生成</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#图像生成基座模型" class="table-of-contents__link toc-highlight">图像生成基座模型</a><ul><li><a href="#text-to-image-生成范式" class="table-of-contents__link toc-highlight">Text-To-Image 生成范式</a><ul><li><a href="#1-gan" class="table-of-contents__link toc-highlight">1. GAN</a></li><li><a href="#2-autoregressivear" class="table-of-contents__link toc-highlight">2. Autoregressive（AR）</a><ul><li><a href="#代表模型vit-vqgan" class="table-of-contents__link toc-highlight">代表模型：ViT-VQGAN</a></li><li><a href="#代表模型var" class="table-of-contents__link toc-highlight">代表模型：VAR</a></li></ul></li><li><a href="#3-non-ar-transformer" class="table-of-contents__link toc-highlight">3. Non-AR Transformer</a><ul><li><a href="#代表模型maskgit-masked-generative-image-transformer" class="table-of-contents__link toc-highlight">代表模型：MaskGIT: Masked Generative Image Transformer</a></li><li><a href="#代表模型mage-masked-generative-encoder-to-unify-representation-learning-and-image-synthesis" class="table-of-contents__link toc-highlight">代表模型：MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis</a></li></ul></li><li><a href="#4diffusion-model" class="table-of-contents__link toc-highlight">4.Diffusion Model</a><ul><li><a href="#代表模型vae" class="table-of-contents__link toc-highlight">代表模型：VAE</a></li><li><a href="#代表模型vq-vae" class="table-of-contents__link toc-highlight">代表模型：VQ-VAE</a></li></ul></li></ul></li><li><a href="#如何训练优秀的生成基座模型" class="table-of-contents__link toc-highlight">如何训练优秀的生成基座模型？</a></li></ul></li><li><a href="#视频生成基座模型" class="table-of-contents__link toc-highlight">视频生成基座模型</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">👋联系我</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/WeChat_QR_Code.jpg" target="_blank" rel="noopener noreferrer" class="footer__link-item">WeChat<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.douyin.com/user/self?modal_id=7157246567970360614" target="_blank" rel="noopener noreferrer" class="footer__link-item">TikTok<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/bonjour-npy" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">✈️外部链接</div><ul class="footer__items clean-list"><li class="footer__item"><a href="http://www.mod.gov.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">共和国国防部<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.xuexi.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">学习强国<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://peacekeeping.un.org/zh" target="_blank" rel="noopener noreferrer" class="footer__link-item">联合国维持和平<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">🎅彩蛋</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.rockstargames.com/gta-v" target="_blank" rel="noopener noreferrer" class="footer__link-item">欢迎来到洛圣都<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.starwars.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">星球大战<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apple.com.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Apple(中国大陆)<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">🦄教育官网</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.uestc.edu.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">电子科技大学<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.guet.edu.cn" target="_blank" rel="noopener noreferrer" class="footer__link-item">桂林电子科技大学<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://cfm.uestc.edu.cn/index" target="_blank" rel="noopener noreferrer" class="footer__link-item">未来媒体研究中心<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright"><br>本网站所展示的标识、链接均属于个人创作和喜好表达，不代表任何国家、政府、企业或组织的官方立场或行为。<br>
                    尽管本网站努力确保信息的准确性和时效性，但所有信息仅供参考，并不构成任何形式的法律、财务或商业建议。<br>
                    <br>Copyright © 2024 bonjour-npy. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.249e2c80.js"></script>
<script src="/assets/js/main.2be429d9.js"></script>
</body>
</html>