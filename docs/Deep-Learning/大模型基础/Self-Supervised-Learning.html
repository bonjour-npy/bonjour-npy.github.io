<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Deep-Learning/大模型基础/Self-Supervised-Learning" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">自监督学习（Self-Supervised Learning） | 培洋的笔记本📒</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://bonjour-npy.github.io/./static/img/intro.png"><meta data-rh="true" name="twitter:image" content="https://bonjour-npy.github.io/./static/img/intro.png"><meta data-rh="true" property="og:url" content="https://bonjour-npy.github.io/docs/Deep-Learning/大模型基础/Self-Supervised-Learning"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="自监督学习（Self-Supervised Learning） | 培洋的笔记本📒"><meta data-rh="true" name="description" content="在自监督学习的模型中，出现了很多以芝麻街任务命名的经典模型和论文。"><meta data-rh="true" property="og:description" content="在自监督学习的模型中，出现了很多以芝麻街任务命名的经典模型和论文。"><link data-rh="true" rel="icon" href="/img/rockstar-games.svg"><link data-rh="true" rel="canonical" href="https://bonjour-npy.github.io/docs/Deep-Learning/大模型基础/Self-Supervised-Learning"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning" hreflang="en"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/docs/Deep-Learning/大模型基础/Self-Supervised-Learning" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://bonjour-npy.github.io/docs/Deep-Learning/大模型基础/Self-Supervised-Learning" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.2503678d.css">
<script src="/assets/js/runtime~main.0cdfe5a1.js" defer="defer"></script>
<script src="/assets/js/main.bb8dae28.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const a=new URLSearchParams(window.location.search).entries();for(var[t,e]of a)if(t.startsWith("docusaurus-data-")){var n=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(n,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><div class="announcementBar_mb4j" role="banner"><div class="content_knG7 announcementBarContent_xLdY">✨ 求实求真，大气大为 ✨</div></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/navbar.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/navbar.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">培洋的笔记本</b></a><a class="navbar__item navbar__link" href="/docs/Deep-Learning/intro">🤖 深度学习</a><a class="navbar__item navbar__link" href="/docs/Tui-Mian/intro">🤡 推免</a><a class="navbar__item navbar__link" href="/docs/Algorithms/intro">🎰 算法</a><a class="navbar__item navbar__link" href="/docs/Curriculum/intro">📖 课程学习</a><a class="navbar__item navbar__link" href="/docs/Others/intro">☃️ 其他</a><a class="navbar__item navbar__link" href="/docs/Acknowledgement/intro">🍺 饮水思源</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/bonjour-npy" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Deep-Learning/intro">Welcome</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Deep-Learning/Fill-The-Gaps">查漏补缺</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/Deep-Learning/基础知识/AlexNet">基础知识</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/Deep-Learning/代码实现/Visdom-Visualization">代码实现</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/Deep-Learning/大模型基础/Self-Attention">大模型基础</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/大模型基础/Self-Attention">自注意力（Self-Attention）</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need">NeurIPS 2017: Attention Is All You Need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Deep-Learning/大模型基础/Self-Supervised-Learning">自监督学习（Self-Supervised Learning）</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/大模型基础/Image-Generation-Models">图像生成模型</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/大模型基础/GAN">生成式对抗网络（GAN）</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/大模型基础/Diffusion-Model">扩散模型（Diffusion Model）</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Deep-Learning/大模型基础/Sampling-for-Generation">生成模型中的采样技巧</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation">Prompt Learning</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/Deep-Learning/生成模型总结/Quick-Notes-about-Main-Techs">生成模型总结</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need">论文笔记</a></div></li></ul></nav><button type="button" title="收起侧边栏" aria-label="收起侧边栏" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">大模型基础</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">自监督学习（Self-Supervised Learning）</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>自监督学习（Self-Supervised Learning）</h1></header>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>在自监督学 习的模型中，出现了很多以芝麻街任务命名的经典模型和论文。</p><p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121113727141.png" alt="image-20231121113727141" class="img_ev3q"></p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="介绍">介绍<a href="#介绍" class="hash-link" aria-label="介绍的直接链接" title="介绍的直接链接">​</a></h2>
<p>自监督学习是无监督学习的一种方法，利用未标记的数据来训练模型。与传统的监督学习不同，自监督学习不需要依赖人工标注的标签数据，而是通过自动构建任务来生成伪标签，从而指导模型的学习。</p>
<p>自监督学习的基本原理是，通过对<strong>输入数据</strong>进行某种<strong>变换</strong>或<strong>操作</strong>，使得模型能够从中<strong>提取有用的特征和语义信息</strong>。例如，在自然语言处理领域，一种常见的自监督学习任务是预测下一个单词；在计算机视觉领域，一种常见的自监督学习任务是预测图像中的缺失部分。这些任务可以帮助模型学习到<strong>输入数据中的潜在结构和规律</strong>，从而提高其泛化能力和性能。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121124909759.png" alt="image-20231121124909759" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bertbidirectional-encoder-representation-from-transformers">BERT（Bidirectional Encoder Representation from Transformers）<a href="#bertbidirectional-encoder-representation-from-transformers" class="hash-link" aria-label="BERT（Bidirectional Encoder Representation from Transformers）的直接链接" title="BERT（Bidirectional Encoder Representation from Transformers）的直接链接">​</a></h2>
<p>下面以BERT为例，介绍自监督模型 。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="结构">结构<a href="#结构" class="hash-link" aria-label="结构的直接链接" title="结构的直接链接">​</a></h3>
<p>BERT的结构其实是Transformer的Encoder部分，仅使用Encoder做特征抽取器。</p>
<p>BERT（Bidirectional Encoder Representations from Transformers）本身是一种预训练的模型架构，通常是在大规模无标签数据上进行预训练，然后在特定任务上进行微调。BERT并不是一个用于特定任务的模型，而是一个通用的语言表示模型。</p>
<p>使用 BERT 的一般步骤包括：</p>
<ol>
<li><strong>预训练（Pretraining）</strong>：在大规模无标签数据上对 BERT 进行预训练，学习通用的语言表示。</li>
<li><strong>微调（Fine-tuning）</strong>：将预训练的 BERT 模型应用于特定任务，并在有标签的数据上进行微调，以适应该任务。</li>
<li><strong>应用于下游任务（Downstream Tasks）</strong>：微调后的 BERT 模型可以被用于执行特定的下游任务，如文本分类、命名实体识别等。</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="self-supervised-pretraining">Self-Supervised Pretraining<a href="#self-supervised-pretraining" class="hash-link" aria-label="Self-Supervised Pretraining的直接链接" title="Self-Supervised Pretraining的直接链接">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="masking-input">Masking Input<a href="#masking-input" class="hash-link" aria-label="Masking Input的直接链接" title="Masking Input的直接链接">​</a></h4>
<p>BERT模型的自监督性质主要体现在其训练数据并不需要人为标注label，而是通过对输入句子中的部分词汇做mask，将输入数据的部分内容使用special token或random token进行遮挡后，喂入Encoder中。对于每个被mask掉的词汇，BERT输出一个概率分布向量，表示这个词汇属于词汇表中的哪一个。</p>
<p>BERT的损失函数主要是Masked Language Model（MLM）任务的交叉熵损失，通过<strong>最小化</strong>Encoder输出的概率分布与Ground Truth之间的<strong>交叉熵损失函数</strong>来训练模型。</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>L</mi><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>y</mi><mi>i</mi></msub><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">L=-\frac1N\sum_{i=1}^Ny_i\log(p_i)\tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:3.106em;vertical-align:-1.2777em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span>
<p>其中：</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span>是输出的概率分布向量的维度。</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>是概率分布向量标签。</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>是模型预测的概率分布向量。</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121134420134.png" alt="image-20231121134420134" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="next-sentence-prediction">Next Sentence Prediction<a href="#next-sentence-prediction" class="hash-link" aria-label="Next Sentence Prediction的直接链接" title="Next Sentence Prediction的直接链接">​</a></h4>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121171808764.png" alt="image-20231121171808764" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="fine-tuning">Fine-tuning<a href="#fine-tuning" class="hash-link" aria-label="Fine-tuning的直接链接" title="Fine-tuning的直接链接">​</a></h3>
<p>在预训练之后，BERT 的模型参数可以被用于多个下游任务，如文本分类、命名实体识别、问答等。</p>
<p>首先，我们先来了解一下NLP任务中很重要的一个Benchmark：GLUE。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="benchmark-glue">Benchmark: GLUE<a href="#benchmark-glue" class="hash-link" aria-label="Benchmark: GLUE的直接链接" title="Benchmark: GLUE的直接链接">​</a></h4>
<p><a href="https://gluebenchmark.com/" target="_blank" rel="noopener noreferrer">GLUE（General Language Understanding Evaluation）</a>是一个评估自然语言处理模型在多个任务上综合性能的基准（benchmark）。它旨在测试模型对各种语言任务的通用理解能力。GLUE benchmark 包含了多个任务，每个任务都有一个对应的数据集和评估标准。</p>
<ol>
<li><strong>MNLI（MultiNLI）</strong>：自然语言推理任务，要求模型判断给定的两个句子之间的关系是蕴含、矛盾还是中立。</li>
<li><strong>QQP（Quora Question Pairs）</strong>：问题匹配任务，要求模型判断两个问题是否语义上等价。</li>
<li><strong>QNLI（Question-answering Natural Language Inference）</strong>：句子分类任务，要求模型判断给定问题和句子之间的关系。</li>
<li><strong>RTE（Recognizing Textual Entailment）</strong>：文本蕴涵任务，要求模型判断给定的两个文本之间是否存在蕴涵关系。</li>
<li><strong>STS-B（Semantic Textual Similarity Benchmark）</strong>：语义文本相似度任务，要求模型度量两个文本之间的语义相似度。</li>
<li><strong>CoLA（Corpus of Linguistic Acceptability）</strong>：语言可接受性判断任务，要求模型判断一个句子是否语法上正确。</li>
<li><strong>MRPC（Microsoft Research Paraphrase Corpus）</strong>：短语匹配任务，要求模型判断两个句子是否语义上等价。</li>
<li><strong>SST-2（Stanford Sentiment Treebank）</strong>：情感分类任务，要求模型判断给定句子的情感极性。</li>
<li><strong>WNLI（Winograd NLI）</strong>：自然语言推理任务，属于 Winograd 模式的变体，要求模型判断一个给定的句子对是否存在蕴含关系。</li>
</ol>
<p>GLUE 提供了一个全面的测试平台，有助于评估和比较不同自然语言处理模型在多个任务上的性能。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121172718354.png" alt="image-20231121172718354" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="downstream-tasks">Downstream Tasks<a href="#downstream-tasks" class="hash-link" aria-label="Downstream Tasks的直接链接" title="Downstream Tasks的直接链接">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="sentiment-analysis">Sentiment Analysis<a href="#sentiment-analysis" class="hash-link" aria-label="Sentiment Analysis的直接链接" title="Sentiment Analysis的直接链接">​</a></h5>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121203549320.png" alt="image-20231121203549320" class="img_ev3q"></p>
<p>BERT作为自监督的预训练模型，从大语料库中学习到了一定的语言知识，在做文字情感分析时，只需要在下游连接上对应的分类器网络，即使只有比较少量的训练资料也能得到比较好的效果。</p>
<p>下图将Pre-training&amp;Fine-tuning范式与Scratch范式的训练效果做了对比，其中Scratch范式即使用传统的随机初始化的方式从头训练整个分类网络。可以看到预训练&amp;微调的训练范式可以加速模型的收敛（Convergence）并且效果也更好。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121203701613.png" alt="image-20231121203701613" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="立场分析">立场分析<a href="#立场分析" class="hash-link" aria-label="立场分析的直接链接" title="立场分析的直接链接">​</a></h5>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121205359709.png" alt="image-20231121205359709" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="extraction-based-question-answering">Extraction-based Question Answering<a href="#extraction-based-question-answering" class="hash-link" aria-label="Extraction-based Question Answering的直接链接" title="Extraction-based Question Answering的直接链接">​</a></h5>
<p>BERT也可以用来完成截取式问答任务，提供一篇文章以及问题，要求输出两个integer代表答案短语在该文章中的起始位置以及结束位置。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121205619437.png" alt="image-20231121205619437" class="img_ev3q"></p>
<p>具体的解决方案：选择输入文章的所有token所对应的输出向量，随机初始化两个相同维度的向量，分别与输出向量做Dot Product，在经过Softmax之后选择最大的得分所对应的索引。得到的两个索引分别是答案短语在文章中开始的位置以及结束的位置。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121211218801.png" alt="image-20231121211218801" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231121210938519.png" alt="image-20231121210938519" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="其他应用">其他应用<a href="#其他应用" class="hash-link" aria-label="其他应用的直接链接" title="其他应用的直接链接">​</a></h5>
<p>虽然以上的应用都是NLP领域的，但是BERT是Seq2Seq模型，图片、语音等信号也都可以作为Sequence输入至BERT中，因此BERT也可以通过迁移学习应用至多模态领域。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-does-bert-work">Why does BERT work?<a href="#why-does-bert-work" class="hash-link" aria-label="Why does BERT work?的直接链接" title="Why does BERT work?的直接链接">​</a></h3>
<p>BERT在大规模语料库（Corpus）上预训练后，输出的向量表示了对应输入token的意思，特别地，是考虑了<strong>上下文信息</strong>的意思。比如，对于水果的苹果已经苹果公司的苹果，BERT对一样的苹果有不同的输出。即对一个处在不同上下文信息中的相同词汇，BERT有不同的Embedding。类似于Word Embedding中的CBOW，BERT可以看作是<strong>Contextualized Word Embedding</strong>。</p>
<blockquote>
<p>BERT模型的核心就是通过预训练来学习上下文信息，进而对每个输入token生成相应的向量表示。这个向量表示考虑了上下文信息，所以对于处在不同上下文中的相同词汇，BERT会有不同的Embedding。</p>
<p>——来自讯飞星火大模型</p>
</blockquote>
<blockquote>
<p>虽然 BERT 的确是一种基于大规模语料库的预训练模型，但它并不是一种简单的词嵌入方法，而是一种深度神经网络模型。</p>
<p>在 BERT 中，每个单词都被嵌入到一个高维空间中，并且这些嵌入向量是通过多层 Transformer 编码器生成的。这些编码器的每一层都包含多个自注意力子层和一个前馈神经网络子层，它们共同作用来捕捉文本中的上下文关系和其他语义特征。</p>
<p>因此，BERT 中的嵌入向量不仅仅是单个单词的词义表征，还包括整个句子或段落中的语境信息。这意味着即使两个单词在不同的上下文中出现，它们的嵌入向量也可能非常相似，因为它们共享相同的语义结构。</p>
<p>总之，BERT 可以被视为一种<strong>上下文感知词嵌入</strong>技术，但它的实现方式比传统的词嵌入方法要复杂得多。</p>
<p>——来自腾讯混元大模型</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="gpt-generative-pre-trained-transformer">GPT: Generative Pre-trained Transformer<a href="#gpt-generative-pre-trained-transformer" class="hash-link" aria-label="GPT: Generative Pre-trained Transformer的直接链接" title="GPT: Generative Pre-trained Transformer的直接链接">​</a></h2>
<p>GPT的结构是Transformer的Decoder部分，可以承担生成的任务。</p>
<p>GPT的自监督学习的特征体现在：在训练过程中，GPT根据输入的token预测输入的下一个token应该是什么，对输出的distribution与Ground Truth做Cross Entropy Loss来更新参数。</p>
<p><img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/typora_imagesimage-20231122093825510.png" alt="image-20231122093825510" class="img_ev3q"></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/bonjour-npy/bonjour-npy.github.io/tree/master/docs/Deep-Learning/5-大模型基础/3-Self-Supervised-Learning.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">NeurIPS 2017: Attention Is All You Need</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Deep-Learning/大模型基础/Image-Generation-Models"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">图像生成模型</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#介绍" class="table-of-contents__link toc-highlight">介绍</a></li><li><a href="#bertbidirectional-encoder-representation-from-transformers" class="table-of-contents__link toc-highlight">BERT（Bidirectional Encoder Representation from Transformers）</a><ul><li><a href="#结构" class="table-of-contents__link toc-highlight">结构</a></li><li><a href="#self-supervised-pretraining" class="table-of-contents__link toc-highlight">Self-Supervised Pretraining</a><ul><li><a href="#masking-input" class="table-of-contents__link toc-highlight">Masking Input</a></li><li><a href="#next-sentence-prediction" class="table-of-contents__link toc-highlight">Next Sentence Prediction</a></li></ul></li><li><a href="#fine-tuning" class="table-of-contents__link toc-highlight">Fine-tuning</a><ul><li><a href="#benchmark-glue" class="table-of-contents__link toc-highlight">Benchmark: GLUE</a></li><li><a href="#downstream-tasks" class="table-of-contents__link toc-highlight">Downstream Tasks</a><ul><li><a href="#sentiment-analysis" class="table-of-contents__link toc-highlight">Sentiment Analysis</a></li><li><a href="#立场分析" class="table-of-contents__link toc-highlight">立场分析</a></li><li><a href="#extraction-based-question-answering" class="table-of-contents__link toc-highlight">Extraction-based Question Answering</a></li><li><a href="#其他应用" class="table-of-contents__link toc-highlight">其他应用</a></li></ul></li></ul></li><li><a href="#why-does-bert-work" class="table-of-contents__link toc-highlight">Why does BERT work?</a></li></ul></li><li><a href="#gpt-generative-pre-trained-transformer" class="table-of-contents__link toc-highlight">GPT: Generative Pre-trained Transformer</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">👋 联系我</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://raw.githubusercontent.com/bonjour-npy/Image-Hosting-Service/main/WeChat_QR_Code.jpg" target="_blank" rel="noopener noreferrer" class="footer__link-item">WeChat<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.douyin.com/user/self?modal_id=7157246567970360614" target="_blank" rel="noopener noreferrer" class="footer__link-item">TikTok<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/bonjour-npy" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">✈️ 外部链接</div><ul class="footer__items clean-list"><li class="footer__item"><a href="http://www.mod.gov.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">共和国国防部<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.xuexi.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">学习强国<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://peacekeeping.un.org/zh" target="_blank" rel="noopener noreferrer" class="footer__link-item">联合国维持和平<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">🎅 彩蛋</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.rockstargames.com/gta-v" target="_blank" rel="noopener noreferrer" class="footer__link-item">欢迎来到洛圣都<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.starwars.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">星球大战<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apple.com.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Apple(中国大陆)<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">🦄 教育官网</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.uestc.edu.cn/" target="_blank" rel="noopener noreferrer" class="footer__link-item">电子科技大学<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.guet.edu.cn" target="_blank" rel="noopener noreferrer" class="footer__link-item">桂林电子科技大学<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://cfm.uestc.edu.cn/index" target="_blank" rel="noopener noreferrer" class="footer__link-item">未来媒体研究中心<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright"><br>本网站所展示的标识、链接均属于个 人创作和喜好表达，不代表任何国家、政府、企业或组织的官方立场或行为。<br>
                    尽管本网站努力确保信息的准确性和时效性，但所有信息仅供参考，并不构成任何形式的法律、财务或商业建议。<br>
                    <br>Copyright © 2024 bonjour-npy. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>