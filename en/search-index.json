[{"documents":[{"i":800,"t":"反序输出","u":"/en/docs/Algorithms/题解/反序输出","b":["题解"]},{"i":806,"t":"鸣谢","u":"/en/docs/Acknowledgement/intro","b":["🍺 饮水思源"]},{"i":811,"t":"排列组合（求30的倍数）","u":"/en/docs/Algorithms/题解/排列组合（求30的倍数）","b":["题解"]},{"i":817,"t":"Welcome","u":"/en/docs/Algorithms/intro","b":["🎰 算法"]},{"i":821,"t":"一维前缀和（刷出一道墙）","u":"/en/docs/Algorithms/题解/一维前缀和（刷出一道墙）","b":["题解"]},{"i":827,"t":"Linux 系统下 GeekOS 的环境配置","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","b":["操作系统课设"]},{"i":841,"t":"GeekOS project 0的实现","u":"/en/docs/Curriculum/操作系统课设/GeekOS-project-0","b":["操作系统课设"]},{"i":847,"t":"编译原理笔记","u":"/en/docs/Curriculum/编译原理/Note","b":["编译原理"]},{"i":905,"t":"Welcome","u":"/en/docs/Curriculum/intro","b":["📖 课程学习"]},{"i":909,"t":"生成式对抗网络（GAN）","u":"/en/docs/Deep-Learning/大模型基础/GAN","b":["大模型基础"]},{"i":926,"t":"NeurIPS 2017: Attention Is All You Need","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","b":["大模型基础"]},{"i":949,"t":"数字图像处理复习笔记","u":"/en/docs/Curriculum/数字图像处理/Note","b":["数字图像处理"]},{"i":1002,"t":"扩散模型（Diffusion Model）","u":"/en/docs/Deep-Learning/大模型基础/Diffusion-Model","b":["大模型基础"]},{"i":1010,"t":"图像生成模型","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","b":["大模型基础"]},{"i":1037,"t":"本科毕业论文：基于 Prompt Learning 的视觉-语言大模型在图像生成中的应用与研究","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","b":["大模型基础","Prompt Learning"]},{"i":1072,"t":"自监督学习（Self-Supervised Learning）","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","b":["大模型基础"]},{"i":1088,"t":"生成模型中的采样技巧","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","b":["大模型基础"]},{"i":1106,"t":"Visdom可视化","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","b":["代码实现"]},{"i":1117,"t":"Speaker Classification","u":"/en/docs/Deep-Learning/代码实现/Speaker-Classification","b":["代码实现"]},{"i":1125,"t":"自注意力（Self-Attention）","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","b":["大模型基础"]},{"i":1155,"t":"深度可分离卷积","u":"/en/docs/Deep-Learning/基础知识/深度可分离卷积","b":["基础知识"]},{"i":1163,"t":"本科毕业论文：基于 Prompt Learning 的视觉-语言大模型在图像生成中的应用与研究","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","b":["代码实现"]},{"i":1198,"t":"激活函数与Loss的梯度","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","b":["基础知识"]},{"i":1209,"t":"正则化与权重衰退","u":"/en/docs/Deep-Learning/基础知识/正则化与权重衰退","b":["基础知识"]},{"i":1217,"t":"AlexNet","u":"/en/docs/Deep-Learning/基础知识/AlexNet","b":["基础知识"]},{"i":1224,"t":"K-fold cross-validation","u":"/en/docs/Deep-Learning/基础知识/K-foldCross-validation","b":["基础知识"]},{"i":1231,"t":"卷积层","u":"/en/docs/Deep-Learning/基础知识/ConvolutionalLayer","b":["基础知识"]},{"i":1236,"t":"从全连接到卷积","u":"/en/docs/Deep-Learning/基础知识/FromFullyConnectedLayerToConvolutionalLayer","b":["基础知识"]},{"i":1243,"t":"关于Logistic Regression","u":"/en/docs/Deep-Learning/基础知识/LogisticRegression","b":["基础知识"]},{"i":1252,"t":"池化层","u":"/en/docs/Deep-Learning/基础知识/PoolingLayer","b":["基础知识"]},{"i":1259,"t":"Perceptron","u":"/en/docs/Deep-Learning/基础知识/Perceptron","b":["基础知识"]},{"i":1266,"t":"LeNet","u":"/en/docs/Deep-Learning/基础知识/LeNet","b":["基础知识"]},{"i":1273,"t":"PyTorch基础","u":"/en/docs/Deep-Learning/基础知识/PytorchBasics","b":["基础知识"]},{"i":1276,"t":"Autoregressive Image Generation without Vector Quantization","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","b":["论文笔记"]},{"i":1298,"t":"NeurIPS 2017: Attention Is All You Need","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","b":["论文笔记"]},{"i":1321,"t":"CVPR 2022: High-Resolution Image Synthesis with Latent Diffusion Models","u":"/en/docs/Deep-Learning/论文笔记/Latent-Diffusion-Models","b":["论文笔记"]},{"i":1326,"t":"NeurIPS 2020: Denoising Diffusion Probabilistic Models","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","b":["论文笔记"]},{"i":1337,"t":"Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","b":["论文笔记"]},{"i":1348,"t":"CVPR 2023: Zero-shot Generative Model Adaptation via Image-specific Prompt Learning","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","b":["论文笔记"]},{"i":1361,"t":"自回归模型：MAR","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","b":["图像生成与视频生成大模型"]},{"i":1383,"t":"图像生成：自回归模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","b":["图像生成与视频生成大模型"]},{"i":1404,"t":"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","b":["论文笔记"]},{"i":1418,"t":"自回归模型：LlamaGen","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","b":["图像生成与视频生成大模型"]},{"i":1429,"t":"图像生成和视频生成基座模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Image-and-Video-Generative-Foundation-Model","b":["图像生成与视频生成大模型"]},{"i":1438,"t":"查漏补缺","u":"/en/docs/Deep-Learning/Fill-The-Gaps","b":[]},{"i":1448,"t":"图像生成：扩散模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","b":["图像生成与视频生成大模型"]},{"i":1460,"t":"Welcome","u":"/en/docs/Deep-Learning/intro","b":["🤖 深度学习"]},{"i":1464,"t":"更新至 Docusaurus V3","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","b":["博客搭建"]},{"i":1474,"t":"告示栏","u":"/en/docs/Others/博客搭建/announcement_bar","b":["博客搭建"]},{"i":1476,"t":"自回归模型：VAR","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","b":["图像生成与视频生成大模型"]},{"i":1493,"t":"Welcome","u":"/en/docs/Others/intro","b":["☃️ 其他"]},{"i":1497,"t":"终端代理","u":"/en/docs/Others/Linux/实用工具/终端代理","b":["Linux","实用工具"]},{"i":1505,"t":"挂载Windows磁盘为只读文件","u":"/en/docs/Others/Linux/问题解决/双系统挂载Windows磁盘为只读文件","b":["Linux","问题解决"]},{"i":1510,"t":"如何让你的Kde Plasma看起来更像macOS","u":"/en/docs/Others/Linux/客制化/如何让你的KDE看起来更像macOS","b":["Linux","客制化"]},{"i":1515,"t":"数据结构","u":"/en/docs/Tui-Mian/计算机基础综合/数据结构","b":["计算机基础综合"]},{"i":1520,"t":"大数除法","u":"/en/docs/Tui-Mian/机试/大数除法","b":["机试"]},{"i":1528,"t":"概率论","u":"/en/docs/Tui-Mian/数学/概率论","b":["数学"]},{"i":1531,"t":"线性代数","u":"/en/docs/Tui-Mian/数学/线性代数","b":["数学"]},{"i":1537,"t":"夏令营面试数学部分复习","u":"/en/docs/Tui-Mian/数学/夏令营面试数学部分复习","b":["数学"]},{"i":1542,"t":"简历面试准备","u":"/en/docs/Tui-Mian/简历/简历面试准备","b":["简历"]},{"i":1584,"t":"Welcome","u":"/en/docs/Tui-Mian/intro","b":["🤡 推免"]},{"i":1588,"t":"经验贴：2023年双非计算机保研经历","u":"/en/docs/Tui-Mian/Summary","b":[]}],"index":{"version":"2.3.9","fields":["t"],"fieldVectors":[["t/800",[0,1.024]],["t/806",[0,1.024]],["t/811",[1,4.825]],["t/817",[2,3.148]],["t/821",[0,1.024]],["t/827",[0,0.891,3,2.819,4,2.433]],["t/841",[4,2.825,5,3.272,6,3.272]],["t/847",[0,1.024]],["t/905",[2,3.148]],["t/909",[7,4.825]],["t/926",[8,2.18,9,2.433,10,2.18,11,2.433]],["t/949",[0,1.024]],["t/1002",[12,2.753,13,2.37]],["t/1010",[0,1.024]],["t/1037",[0,0.984,14,1.914,15,1.748]],["t/1072",[15,2.31,16,2.825,17,3.272]],["t/1088",[0,1.024]],["t/1106",[18,4.825]],["t/1117",[19,3.9,20,3.9]],["t/1125",[10,3.016,16,3.367]],["t/1155",[0,1.024]],["t/1163",[0,0.984,14,1.914,15,1.748]],["t/1198",[21,4.825]],["t/1209",[0,1.024]],["t/1217",[22,4.825]],["t/1224",[23,2.819,24,2.819,25,2.819,26,2.819]],["t/1231",[0,1.024]],["t/1236",[0,1.024]],["t/1243",[27,3.9,28,3.9]],["t/1252",[0,1.024]],["t/1259",[29,4.825]],["t/1266",[30,4.825]],["t/1273",[31,4.825]],["t/1276",[32,1.707,33,1.44,34,1.558,35,2.207,36,2.207,37,2.207]],["t/1298",[8,2.18,9,2.433,10,2.18,11,2.433]],["t/1321",[12,1.175,13,1.012,33,1.086,38,1.437,39,1.665,40,1.665,41,1.665,42,1.665,43,1.665]],["t/1326",[8,1.707,12,1.558,13,1.341,44,2.207,45,2.207,46,2.207]],["t/1337",[12,1.28,13,1.102,32,1.402,33,1.183,34,1.28,47,1.813,48,1.813,49,1.565]],["t/1348",[13,0.812,14,1.034,15,0.944,33,0.872,34,0.944,38,1.154,50,1.154,51,1.337,52,1.337,53,1.337,54,1.154,55,1.337]],["t/1361",[56,4.825]],["t/1383",[0,1.024]],["t/1404",[13,0.935,32,1.19,33,1.004,34,1.087,49,1.329,54,1.329,57,1.539,58,1.539,59,1.539,60,1.539]],["t/1418",[61,4.825]],["t/1429",[0,1.024]],["t/1438",[0,1.024]],["t/1448",[0,1.024]],["t/1460",[2,3.148]],["t/1464",[0,0.694,62,3.272,63,3.272]],["t/1474",[0,1.024]],["t/1476",[64,4.825]],["t/1493",[2,3.148]],["t/1497",[0,1.024]],["t/1505",[65,4.825]],["t/1510",[66,3.9,67,3.9]],["t/1515",[0,1.024]],["t/1520",[0,1.024]],["t/1528",[0,1.024]],["t/1531",[0,1.024]],["t/1537",[0,1.024]],["t/1542",[0,1.024]],["t/1584",[2,3.148]],["t/1588",[50,4.165]]],"invertedIndex":[["",{"_index":0,"t":{"800":{"position":[[0,4]]},"806":{"position":[[0,2]]},"821":{"position":[[0,12]]},"827":{"position":[[6,3],[17,5]]},"847":{"position":[[0,6]]},"949":{"position":[[0,10]]},"1010":{"position":[[0,6]]},"1037":{"position":[[0,9],[26,3],[30,17]]},"1088":{"position":[[0,10]]},"1155":{"position":[[0,7]]},"1163":{"position":[[0,9],[26,3],[30,17]]},"1209":{"position":[[0,8]]},"1231":{"position":[[0,3]]},"1236":{"position":[[0,7]]},"1252":{"position":[[0,3]]},"1383":{"position":[[0,10]]},"1429":{"position":[[0,13]]},"1438":{"position":[[0,4]]},"1448":{"position":[[0,9]]},"1464":{"position":[[0,3]]},"1474":{"position":[[0,3]]},"1497":{"position":[[0,4]]},"1515":{"position":[[0,4]]},"1520":{"position":[[0,4]]},"1528":{"position":[[0,3]]},"1531":{"position":[[0,4]]},"1537":{"position":[[0,11]]},"1542":{"position":[[0,6]]}}}],["0",{"_index":6,"t":{"841":{"position":[[15,4]]}}}],["2017",{"_index":9,"t":{"926":{"position":[[8,5]]},"1298":{"position":[[8,5]]}}}],["2020",{"_index":44,"t":{"1326":{"position":[[8,5]]}}}],["2022",{"_index":39,"t":{"1321":{"position":[[5,5]]}}}],["2023",{"_index":50,"t":{"1348":{"position":[[5,5]]},"1588":{"position":[[0,18]]}}}],["30",{"_index":1,"t":{"811":{"position":[[0,12]]}}}],["adapt",{"_index":53,"t":{"1348":{"position":[[38,10]]}}}],["alexnet",{"_index":22,"t":{"1217":{"position":[[0,7]]}}}],["attent",{"_index":10,"t":{"926":{"position":[[14,9]]},"1125":{"position":[[10,10]]},"1298":{"position":[[14,9]]}}}],["autoregress",{"_index":32,"t":{"1276":{"position":[[0,14]]},"1337":{"position":[[0,14]]},"1404":{"position":[[7,14]]}}}],["beat",{"_index":47,"t":{"1337":{"position":[[21,5]]}}}],["classif",{"_index":20,"t":{"1117":{"position":[[8,14]]}}}],["cross",{"_index":25,"t":{"1224":{"position":[[7,5]]}}}],["cvpr",{"_index":38,"t":{"1321":{"position":[[0,4]]},"1348":{"position":[[0,4]]}}}],["denois",{"_index":45,"t":{"1326":{"position":[[14,9]]}}}],["diffus",{"_index":12,"t":{"1002":{"position":[[0,14]]},"1321":{"position":[[55,9]]},"1326":{"position":[[24,9]]},"1337":{"position":[[27,10]]}}}],["docusauru",{"_index":62,"t":{"1464":{"position":[[4,10]]}}}],["fold",{"_index":24,"t":{"1224":{"position":[[2,4]]}}}],["gan",{"_index":7,"t":{"909":{"position":[[0,12]]}}}],["geeko",{"_index":4,"t":{"827":{"position":[[10,6]]},"841":{"position":[[0,6]]}}}],["gener",{"_index":34,"t":{"1276":{"position":[[21,10]]},"1337":{"position":[[63,10]]},"1348":{"position":[[21,10]]},"1404":{"position":[[47,10]]}}}],["high",{"_index":40,"t":{"1321":{"position":[[11,4]]}}}],["imag",{"_index":33,"t":{"1276":{"position":[[15,5]]},"1321":{"position":[[27,5]]},"1337":{"position":[[57,5]]},"1348":{"position":[[53,5]]},"1404":{"position":[[41,5]]}}}],["k",{"_index":23,"t":{"1224":{"position":[[0,1]]}}}],["kde",{"_index":66,"t":{"1510":{"position":[[0,8]]}}}],["latent",{"_index":43,"t":{"1321":{"position":[[48,6]]}}}],["learn",{"_index":15,"t":{"1037":{"position":[[17,8]]},"1072":{"position":[[22,9]]},"1163":{"position":[[17,8]]},"1348":{"position":[[75,8]]}}}],["lenet",{"_index":30,"t":{"1266":{"position":[[0,5]]}}}],["linux",{"_index":3,"t":{"827":{"position":[[0,5]]}}}],["llama",{"_index":48,"t":{"1337":{"position":[[38,5]]}}}],["llamagen",{"_index":61,"t":{"1418":{"position":[[0,14]]}}}],["logist",{"_index":27,"t":{"1243":{"position":[[0,10]]}}}],["loss",{"_index":21,"t":{"1198":{"position":[[0,12]]}}}],["mar",{"_index":56,"t":{"1361":{"position":[[0,9]]}}}],["model",{"_index":13,"t":{"1002":{"position":[[15,6]]},"1321":{"position":[[65,6]]},"1326":{"position":[[48,6]]},"1337":{"position":[[15,5]]},"1348":{"position":[[32,5]]},"1404":{"position":[[22,9]]}}}],["need",{"_index":11,"t":{"926":{"position":[[35,4]]},"1298":{"position":[[35,4]]}}}],["neurip",{"_index":8,"t":{"926":{"position":[[0,7]]},"1298":{"position":[[0,7]]},"1326":{"position":[[0,7]]}}}],["next",{"_index":58,"t":{"1404":{"position":[[62,4]]}}}],["perceptron",{"_index":29,"t":{"1259":{"position":[[0,10]]}}}],["plasma看起来更像maco",{"_index":67,"t":{"1510":{"position":[[9,16]]}}}],["predict",{"_index":60,"t":{"1404":{"position":[[73,10]]}}}],["probabilist",{"_index":46,"t":{"1326":{"position":[[34,13]]}}}],["project",{"_index":5,"t":{"841":{"position":[[7,7]]}}}],["prompt",{"_index":14,"t":{"1037":{"position":[[10,6]]},"1163":{"position":[[10,6]]},"1348":{"position":[[68,6]]}}}],["pytorch",{"_index":31,"t":{"1273":{"position":[[0,9]]}}}],["quantiz",{"_index":37,"t":{"1276":{"position":[[47,12]]}}}],["regress",{"_index":28,"t":{"1243":{"position":[[11,10]]}}}],["resolut",{"_index":41,"t":{"1321":{"position":[[16,10]]}}}],["scalabl",{"_index":49,"t":{"1337":{"position":[[48,8]]},"1404":{"position":[[32,8]]}}}],["scale",{"_index":59,"t":{"1404":{"position":[[67,5]]}}}],["self",{"_index":16,"t":{"1072":{"position":[[0,10]]},"1125":{"position":[[0,9]]}}}],["shot",{"_index":52,"t":{"1348":{"position":[[16,4]]}}}],["speaker",{"_index":19,"t":{"1117":{"position":[[0,7]]}}}],["specif",{"_index":55,"t":{"1348":{"position":[[59,8]]}}}],["supervis",{"_index":17,"t":{"1072":{"position":[[11,10]]}}}],["synthesi",{"_index":42,"t":{"1321":{"position":[[33,9]]}}}],["v3",{"_index":63,"t":{"1464":{"position":[[15,2]]}}}],["valid",{"_index":26,"t":{"1224":{"position":[[13,10]]}}}],["var",{"_index":64,"t":{"1476":{"position":[[0,9]]}}}],["vector",{"_index":36,"t":{"1276":{"position":[[40,6]]}}}],["via",{"_index":54,"t":{"1348":{"position":[[49,3]]},"1404":{"position":[[58,3]]}}}],["visdom",{"_index":18,"t":{"1106":{"position":[[0,9]]}}}],["visual",{"_index":57,"t":{"1404":{"position":[[0,6]]}}}],["welcom",{"_index":2,"t":{"817":{"position":[[0,7]]},"905":{"position":[[0,7]]},"1460":{"position":[[0,7]]},"1493":{"position":[[0,7]]},"1584":{"position":[[0,7]]}}}],["window",{"_index":65,"t":{"1505":{"position":[[0,16]]}}}],["without",{"_index":35,"t":{"1276":{"position":[[32,7]]}}}],["zero",{"_index":51,"t":{"1348":{"position":[[11,4]]}}}]],"pipeline":["stemmer"]}},{"documents":[{"i":802,"t":"参考代码","u":"/en/docs/Algorithms/题解/反序输出","h":"#参考代码","p":800},{"i":804,"t":"题解","u":"/en/docs/Algorithms/题解/反序输出","h":"#题解","p":800},{"i":807,"t":"饮水思源","u":"/en/docs/Acknowledgement/intro","h":"#饮水思源","p":806},{"i":809,"t":"Disclaimer","u":"/en/docs/Acknowledgement/intro","h":"#disclaimer","p":806},{"i":813,"t":"参考代码","u":"/en/docs/Algorithms/题解/排列组合（求30的倍数）","h":"#参考代码","p":811},{"i":815,"t":"题解","u":"/en/docs/Algorithms/题解/排列组合（求30的倍数）","h":"#题解","p":811},{"i":819,"t":"支持我！","u":"/en/docs/Algorithms/intro","h":"#支持我","p":817},{"i":823,"t":"参考代码","u":"/en/docs/Algorithms/题解/一维前缀和（刷出一道墙）","h":"#参考代码","p":821},{"i":825,"t":"题解","u":"/en/docs/Algorithms/题解/一维前缀和（刷出一道墙）","h":"#题解","p":821},{"i":828,"t":"必须要知道的原理","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#必须要知道的原理","p":827},{"i":829,"t":"GeekOS:","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#geekos","p":827},{"i":831,"t":"bochs:","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#bochs","p":827},{"i":833,"t":"二者之间的关系","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#二者之间的关系","p":827},{"i":835,"t":"安装与配置","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#安装与配置","p":827},{"i":837,"t":"安装","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#安装","p":827},{"i":839,"t":"配置","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#配置","p":827},{"i":843,"t":"编写C语言代码","u":"/en/docs/Curriculum/操作系统课设/GeekOS-project-0","h":"#编写c语言代码","p":841},{"i":845,"t":"使用Linux的编译系统对C语言代码进行编译","u":"/en/docs/Curriculum/操作系统课设/GeekOS-project-0","h":"#使用linux的编译系统对c语言代码进行编译","p":841},{"i":848,"t":"第一章：前言","u":"/en/docs/Curriculum/编译原理/Note","h":"#第一章前言","p":847},{"i":849,"t":"1.1 编译程序的逻辑结构","u":"/en/docs/Curriculum/编译原理/Note","h":"#11-编译程序的逻辑结构","p":847},{"i":851,"t":"1.2 前端和后端","u":"/en/docs/Curriculum/编译原理/Note","h":"#12-前端和后端","p":847},{"i":853,"t":"1.3 遍的概念","u":"/en/docs/Curriculum/编译原理/Note","h":"#13-遍的概念","p":847},{"i":855,"t":"第二章：文法和语言","u":"/en/docs/Curriculum/编译原理/Note","h":"#第二章文法和语言","p":847},{"i":856,"t":"2.1 句型","u":"/en/docs/Curriculum/编译原理/Note","h":"#21-句型","p":847},{"i":858,"t":"2.2 句子：","u":"/en/docs/Curriculum/编译原理/Note","h":"#22-句子","p":847},{"i":860,"t":"2.3 文法的分类：","u":"/en/docs/Curriculum/编译原理/Note","h":"#23-文法的分类","p":847},{"i":862,"t":"2.4 最左/右推导：","u":"/en/docs/Curriculum/编译原理/Note","h":"#24-最左右推导","p":847},{"i":864,"t":"第三章：词法分析","u":"/en/docs/Curriculum/编译原理/Note","h":"#第三章词法分析","p":847},{"i":865,"t":"3.1 正规文法转换成正规式","u":"/en/docs/Curriculum/编译原理/Note","h":"#31-正规文法转换成正规式","p":847},{"i":866,"t":"3.2 有穷自动机（FA）","u":"/en/docs/Curriculum/编译原理/Note","h":"#32-有穷自动机fa","p":847},{"i":868,"t":"3.3 正规式RE与有穷自动机FA的互相转化","u":"/en/docs/Curriculum/编译原理/Note","h":"#33-正规式re与有穷自动机fa的互相转化","p":847},{"i":869,"t":"3.4 正规文法RM与有穷自动机FA的互相转化","u":"/en/docs/Curriculum/编译原理/Note","h":"#34-正规文法rm与有穷自动机fa的互相转化","p":847},{"i":870,"t":"第四章：自顶向下语法分析方法","u":"/en/docs/Curriculum/编译原理/Note","h":"#第四章自顶向下语法分析方法","p":847},{"i":872,"t":"1. FIRST集的定义","u":"/en/docs/Curriculum/编译原理/Note","h":"#1-first集的定义","p":847},{"i":873,"t":"2. Follow集的定义","u":"/en/docs/Curriculum/编译原理/Note","h":"#2-follow集的定义","p":847},{"i":875,"t":"3. SELECT集的定义","u":"/en/docs/Curriculum/编译原理/Note","h":"#3-select集的定义","p":847},{"i":877,"t":"4. LL(1)文法的定义","u":"/en/docs/Curriculum/编译原理/Note","h":"#4-ll1文法的定义","p":847},{"i":878,"t":"5. LL(1)文法的判别","u":"/en/docs/Curriculum/编译原理/Note","h":"#5-ll1文法的判别","p":847},{"i":880,"t":"6. 预测分析表","u":"/en/docs/Curriculum/编译原理/Note","h":"#6-预测分析表","p":847},{"i":882,"t":"7. 非LL(1)文法到LL(1)文法的等价变换","u":"/en/docs/Curriculum/编译原理/Note","h":"#7-非ll1文法到ll1文法的等价变换","p":847},{"i":884,"t":"第五章：自底向上语法分析方法","u":"/en/docs/Curriculum/编译原理/Note","h":"#第五章自底向上语法分析方法","p":847},{"i":885,"t":"5.1 概念","u":"/en/docs/Curriculum/编译原理/Note","h":"#51-概念","p":847},{"i":887,"t":"5.2 方法","u":"/en/docs/Curriculum/编译原理/Note","h":"#52-方法","p":847},{"i":889,"t":"5.3 工作过程","u":"/en/docs/Curriculum/编译原理/Note","h":"#53-工作过程","p":847},{"i":890,"t":"5.4 移入-归约分析器的4种动作","u":"/en/docs/Curriculum/编译原理/Note","h":"#54-移入-归约分析器的4种动作","p":847},{"i":892,"t":"5.5 重要题型","u":"/en/docs/Curriculum/编译原理/Note","h":"#55-重要题型","p":847},{"i":894,"t":"概念总结","u":"/en/docs/Curriculum/编译原理/Note","h":"#概念总结","p":847},{"i":895,"t":"1 编译程序各阶段功能","u":"/en/docs/Curriculum/编译原理/Note","h":"#1-编译程序各阶段功能","p":847},{"i":897,"t":"2 语法分析方法的概念","u":"/en/docs/Curriculum/编译原理/Note","h":"#2-语法分析方法的概念","p":847},{"i":899,"t":"3 翻译模式","u":"/en/docs/Curriculum/编译原理/Note","h":"#3-翻译模式","p":847},{"i":901,"t":"4 属性文法","u":"/en/docs/Curriculum/编译原理/Note","h":"#4-属性文法","p":847},{"i":903,"t":"5 符号表","u":"/en/docs/Curriculum/编译原理/Note","h":"#5-符号表","p":847},{"i":907,"t":"支持我！","u":"/en/docs/Curriculum/intro","h":"#支持我","p":905},{"i":911,"t":"引言","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#引言","p":909},{"i":912,"t":"将随机分布作为输入","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#将随机分布作为输入","p":909},{"i":914,"t":"为什么要添加分布","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#为什么要添加分布","p":909},{"i":916,"t":"核心思想","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#核心思想","p":909},{"i":918,"t":"具体结构与作用","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#具体结构与作用","p":909},{"i":920,"t":"生成器（Generator）","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#生成器generator","p":909},{"i":922,"t":"判别器（Discriminator）","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#判别器discriminator","p":909},{"i":924,"t":"训练算法","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#训练算法","p":909},{"i":928,"t":"整体结构","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#整体结构","p":926},{"i":930,"t":"Encoder","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#encoder","p":926},{"i":931,"t":"整体结构","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#整体结构-1","p":926},{"i":933,"t":"位置编码（Positional Encoding）","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#位置编码positional-encoding","p":926},{"i":935,"t":"具体结构","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#具体结构","p":926},{"i":937,"t":"Decoder","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#decoder","p":926},{"i":939,"t":"Autoregressive Decoder（AT）","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#autoregressive-decoderat","p":926},{"i":941,"t":"Non-Autoregressive Decoder（NAT）","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#non-autoregressive-decodernat","p":926},{"i":942,"t":"训练（Training）","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#训练training","p":926},{"i":943,"t":"损失函数","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#损失函数","p":926},{"i":945,"t":"Teacher Forcing","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#teacher-forcing","p":926},{"i":947,"t":"Teacher Forcing与Masked Multi-Head Self-Attention","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#teacher-forcing与masked-multi-head-self-attention","p":926},{"i":951,"t":"第2章 数字图像处理基础","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#第2章-数字图像处理基础","p":949},{"i":952,"t":"图像的采样与量化","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#图像的采样与量化","p":949},{"i":954,"t":"距离度量","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#距离度量","p":949},{"i":956,"t":"图像质量评价","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#图像质量评价","p":949},{"i":958,"t":"灰度直方图","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#灰度直方图","p":949},{"i":959,"t":"第3章 图像变换","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#第3章-图像变换","p":949},{"i":960,"t":"傅里叶变换","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#傅里叶变换","p":949},{"i":962,"t":"第4章 图像处理的基本运算","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#第4章-图像处理的基本运算","p":949},{"i":963,"t":"点运算","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#点运算","p":949},{"i":965,"t":"比例缩放","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#比例缩放","p":949},{"i":967,"t":"灰度级插值","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#灰度级插值","p":949},{"i":969,"t":"第5章 图像空域增强","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#第5章-图像空域增强","p":949},{"i":970,"t":"直接灰度变换","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#直接灰度变换","p":949},{"i":972,"t":"直方图灰度变换","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#直方图灰度变换","p":949},{"i":974,"t":"空域滤波增强","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#空域滤波增强","p":949},{"i":976,"t":"第6章 图像频域增强","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#第6章-图像频域增强","p":949},{"i":978,"t":"低通滤波","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#低通滤波","p":949},{"i":980,"t":"高通滤波","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#高通滤波","p":949},{"i":982,"t":"带通和带阻滤波","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#带通和带阻滤波","p":949},{"i":984,"t":"第7章 彩色图像处理","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#第7章-彩色图像处理","p":949},{"i":985,"t":"伪彩色图像的处理","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#伪彩色图像的处理","p":949},{"i":987,"t":"全彩色图像的处理","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#全彩色图像的处理","p":949},{"i":988,"t":"第8章 图像复原","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#第8章-图像复原","p":949},{"i":989,"t":"图像退化机理","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#图像退化机理","p":949},{"i":991,"t":"图像退化模型","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#图像退化模型","p":949},{"i":993,"t":"第11章 图像分割","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#第11章-图像分割","p":949},{"i":994,"t":"阈值分割法","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#阈值分割法","p":949},{"i":996,"t":"边缘检测的基本原理","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#边缘检测的基本原理","p":949},{"i":998,"t":"边缘检测算子","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#边缘检测算子","p":949},{"i":1000,"t":"课后习题中的问答题","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#课后习题中的问答题","p":949},{"i":1004,"t":"基本概念","u":"/en/docs/Deep-Learning/大模型基础/Diffusion-Model","h":"#基本概念","p":1002},{"i":1006,"t":"训练过程","u":"/en/docs/Deep-Learning/大模型基础/Diffusion-Model","h":"#训练过程","p":1002},{"i":1008,"t":"推理过程","u":"/en/docs/Deep-Learning/大模型基础/Diffusion-Model","h":"#推理过程","p":1002},{"i":1011,"t":"回顾文字生成的两种方法","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#回顾文字生成的两种方法","p":1010},{"i":1013,"t":"自回归方法（AR）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#自回归方法ar","p":1010},{"i":1015,"t":"非自回归方法（NAR）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#非自回归方法nar","p":1010},{"i":1017,"t":"目前图像生成模型的共同点","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#目前图像生成模型的共同点","p":1010},{"i":1019,"t":"生成模型的共同结构","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#生成模型的共同结构","p":1010},{"i":1021,"t":"通用框架概览","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#通用框架概览","p":1010},{"i":1023,"t":"Benchmark","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#benchmark","p":1010},{"i":1025,"t":"通用框架解析","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#通用框架解析","p":1010},{"i":1027,"t":"常见图像生成模型速览","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#常见图像生成模型速览","p":1010},{"i":1029,"t":"变分自编码器（VAE）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#变分自编码器vae","p":1010},{"i":1031,"t":"基于流的生成模型（Flow-Based Generative Model）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#基于流的生成模型flow-based-generative-model","p":1010},{"i":1033,"t":"生成对抗网络（GAN）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#生成对抗网络gan","p":1010},{"i":1035,"t":"扩散模型（Diffusion Model）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#扩散模型diffusion-model","p":1010},{"i":1039,"t":"依赖","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#依赖","p":1037},{"i":1040,"t":"创建 Anaconda 虚拟环境","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#创建-anaconda-虚拟环境","p":1037},{"i":1042,"t":"安装依赖","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#安装依赖","p":1037},{"i":1044,"t":"下载预训练生成器","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#下载预训练生成器","p":1037},{"i":1046,"t":"概述","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#概述","p":1037},{"i":1047,"t":"技术细节","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#技术细节","p":1037},{"i":1048,"t":"prompts 的初始化","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#prompts-的初始化","p":1037},{"i":1050,"t":"prompts 的 tokenize 与 embedding","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#prompts-的-tokenize-与-embedding","p":1037},{"i":1052,"t":"compute_text_features 的实现细节","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#compute_text_features-的实现细节","p":1037},{"i":1054,"t":"训练 stage 1","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#训练-stage-1","p":1037},{"i":1056,"t":"训练 stage 2","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#训练-stage-2","p":1037},{"i":1058,"t":"定量分析指标","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#定量分析指标","p":1037},{"i":1060,"t":"新增功能","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#新增功能","p":1037},{"i":1061,"t":"支持自定义图像的风格迁移","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#支持自定义图像的风格迁移","p":1037},{"i":1063,"t":"Web UI","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#web-ui","p":1037},{"i":1065,"t":"问题提出与改进","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#问题提出与改进","p":1037},{"i":1066,"t":"改进：Mapper 结构的设计","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#改进mapper-结构的设计","p":1037},{"i":1068,"t":"问题：训练阶段人工 prompts 的作用是什么？","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#问题训练阶段人工-prompts-的作用是什么","p":1037},{"i":1070,"t":"改进：使学习到的 prompts 向用户自主设计的 prompts 模板对齐","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#改进使学习到的-prompts-向用户自主设计的-prompts-模板对齐","p":1037},{"i":1074,"t":"介绍","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#介绍","p":1072},{"i":1076,"t":"BERT（Bidirectional Encoder Representation from Transformers）","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#bertbidirectional-encoder-representation-from-transformers","p":1072},{"i":1078,"t":"结构","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#结构","p":1072},{"i":1080,"t":"Self-Supervised Pretraining","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#self-supervised-pretraining","p":1072},{"i":1082,"t":"Fine-tuning","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#fine-tuning","p":1072},{"i":1084,"t":"Why does BERT work?","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#why-does-bert-work","p":1072},{"i":1086,"t":"GPT: Generative Pre-trained Transformer","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#gpt-generative-pre-trained-transformer","p":1072},{"i":1090,"t":"引言","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#引言","p":1088},{"i":1092,"t":"为什么需要采样？","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#为什么需要采样","p":1088},{"i":1094,"t":"长尾效应","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#长尾效应","p":1088},{"i":1096,"t":"采样技巧","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#采样技巧","p":1088},{"i":1098,"t":"温度（Temperature）","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#温度temperature","p":1088},{"i":1100,"t":"Top-k 采样","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#top-k-采样","p":1088},{"i":1102,"t":"Top-p 采样（Nucleus Sampling）","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#top-p-采样nucleus-sampling","p":1088},{"i":1104,"t":"结论","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#结论","p":1088},{"i":1108,"t":"安装Visdom","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","h":"#安装visdom","p":1106},{"i":1110,"t":"Visdom的使用","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","h":"#visdom的使用","p":1106},{"i":1111,"t":"Visdom的启动","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","h":"#visdom的启动","p":1106},{"i":1113,"t":"单窗口单曲线的可视化","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","h":"#单窗口单曲线的可视化","p":1106},{"i":1115,"t":"单窗口多曲线的可视化","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","h":"#单窗口多曲线的可视化","p":1106},{"i":1119,"t":"Overview","u":"/en/docs/Deep-Learning/代码实现/Speaker-Classification","h":"#overview","p":1117},{"i":1121,"t":"Dataset","u":"/en/docs/Deep-Learning/代码实现/Speaker-Classification","h":"#dataset","p":1117},{"i":1123,"t":"Related","u":"/en/docs/Deep-Learning/代码实现/Speaker-Classification","h":"#related","p":1117},{"i":1127,"t":"认识CNN的局限性","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#认识cnn的局限性","p":1125},{"i":1128,"t":"输入与输出的局限性","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#输入与输出的局限性","p":1125},{"i":1130,"t":"关联上下文信息的局限性","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#关联上下文信息的局限性","p":1125},{"i":1132,"t":"Self-Attention的原理","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#self-attention的原理","p":1125},{"i":1133,"t":"什么是Self-Attention","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#什么是self-attention","p":1125},{"i":1135,"t":"Self-Attention的核心思想","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#self-attention的核心思想","p":1125},{"i":1137,"t":"Self-Attention的实现","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#self-attention的实现","p":1125},{"i":1139,"t":"Multi-Head Self-Attention","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#multi-head-self-attention","p":1125},{"i":1141,"t":"Self-Attention与CNN的对比","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#self-attention与cnn的对比","p":1125},{"i":1143,"t":"Self Attention 的计算","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#self-attention-的计算","p":1125},{"i":1145,"t":"输入形状","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#输入形状","p":1125},{"i":1147,"t":"自注意力机制的计算步骤","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#自注意力机制的计算步骤","p":1125},{"i":1149,"t":"多头自注意力","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#多头自注意力","p":1125},{"i":1151,"t":"示例","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#示例","p":1125},{"i":1153,"t":"自注意力中的掩码 Mask","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#自注意力中的掩码-mask","p":1125},{"i":1157,"t":"常规卷积","u":"/en/docs/Deep-Learning/基础知识/深度可分离卷积","h":"#常规卷积","p":1155},{"i":1159,"t":"逐通道卷积-Depthwise Convolution","u":"/en/docs/Deep-Learning/基础知识/深度可分离卷积","h":"#逐通道卷积-depthwise-convolution","p":1155},{"i":1161,"t":"逐点卷积-Pointwise Convolution","u":"/en/docs/Deep-Learning/基础知识/深度可分离卷积","h":"#逐点卷积-pointwise-convolution","p":1155},{"i":1165,"t":"依赖","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#依赖","p":1163},{"i":1166,"t":"创建 Anaconda 虚拟环境","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#创建-anaconda-虚拟环境","p":1163},{"i":1168,"t":"安装依赖","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#安装依赖","p":1163},{"i":1170,"t":"下载预训练生成器","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#下载预训练生成器","p":1163},{"i":1172,"t":"概述","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#概述","p":1163},{"i":1173,"t":"技术细节","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#技术细节","p":1163},{"i":1174,"t":"prompts 的初始化","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#prompts-的初始化","p":1163},{"i":1176,"t":"prompts 的 tokenize 与 embedding","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#prompts-的-tokenize-与-embedding","p":1163},{"i":1178,"t":"compute_text_features 的实现细节","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#compute_text_features-的实现细节","p":1163},{"i":1180,"t":"训练 stage 1","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#训练-stage-1","p":1163},{"i":1182,"t":"训练 stage 2","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#训练-stage-2","p":1163},{"i":1184,"t":"定量分析指标","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#定量分析指标","p":1163},{"i":1186,"t":"新增功能","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#新增功能","p":1163},{"i":1187,"t":"支持自定义图像的风格迁移","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#支持自定义图像的风格迁移","p":1163},{"i":1189,"t":"Web UI","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#web-ui","p":1163},{"i":1191,"t":"问题提出与改进","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#问题提出与改进","p":1163},{"i":1192,"t":"改进：Mapper 结构的设计","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#改进mapper-结构的设计","p":1163},{"i":1194,"t":"问题：训练阶段人工 prompts 的作用是什么？","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#问题训练阶段人工-prompts-的作用是什么","p":1163},{"i":1196,"t":"改进：使学习到的 prompts 向用户自主设计的 prompts 模板对齐","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#改进使学习到的-prompts-向用户自主设计的-prompts-模板对齐","p":1163},{"i":1199,"t":"激活函数","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","h":"#激活函数","p":1198},{"i":1200,"t":"Sigmoid函数 / Logistic函数","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","h":"#sigmoid函数--logistic函数","p":1198},{"i":1202,"t":"线性整流单元（Rectified Linear Unit, ReLU）","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","h":"#线性整流单元rectified-linear-unit-relu","p":1198},{"i":1204,"t":"损失函数","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","h":"#损失函数","p":1198},{"i":1205,"t":"Mean Squared Error 均方误差","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","h":"#mean-squared-error-均方误差","p":1198},{"i":1207,"t":"Cross Entropy Loss 交叉熵损失","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","h":"#cross-entropy-loss-交叉熵损失","p":1198},{"i":1211,"t":"什么是正则化","u":"/en/docs/Deep-Learning/基础知识/正则化与权重衰退","h":"#什么是正则化","p":1209},{"i":1213,"t":"L1正则化","u":"/en/docs/Deep-Learning/基础知识/正则化与权重衰退","h":"#l1正则化","p":1209},{"i":1215,"t":"L2正则化与权重衰退","u":"/en/docs/Deep-Learning/基础知识/正则化与权重衰退","h":"#l2正则化与权重衰退","p":1209},{"i":1218,"t":"背景","u":"/en/docs/Deep-Learning/基础知识/AlexNet","h":"#背景","p":1217},{"i":1220,"t":"新的概念和技术","u":"/en/docs/Deep-Learning/基础知识/AlexNet","h":"#新的概念和技术","p":1217},{"i":1222,"t":"与LeNet比较","u":"/en/docs/Deep-Learning/基础知识/AlexNet","h":"#与lenet比较","p":1217},{"i":1225,"t":"What is k-fold cross-validation?","u":"/en/docs/Deep-Learning/基础知识/K-foldCross-validation","h":"#what-is-k-fold-cross-validation","p":1224},{"i":1227,"t":"How does k-fold cross-validation work?","u":"/en/docs/Deep-Learning/基础知识/K-foldCross-validation","h":"#how-does-k-fold-cross-validation-work","p":1224},{"i":1229,"t":"Summary","u":"/en/docs/Deep-Learning/基础知识/K-foldCross-validation","h":"#summary","p":1224},{"i":1232,"t":"1x1 卷积","u":"/en/docs/Deep-Learning/基础知识/ConvolutionalLayer","h":"#1x1-卷积","p":1231},{"i":1234,"t":"二维卷积层","u":"/en/docs/Deep-Learning/基础知识/ConvolutionalLayer","h":"#二维卷积层","p":1231},{"i":1237,"t":"卷积的诞生&核心特征","u":"/en/docs/Deep-Learning/基础知识/FromFullyConnectedLayerToConvolutionalLayer","h":"#卷积的诞生核心特征","p":1236},{"i":1239,"t":"重新考察全连接层","u":"/en/docs/Deep-Learning/基础知识/FromFullyConnectedLayerToConvolutionalLayer","h":"#重新考察全连接层","p":1236},{"i":1241,"t":"总结","u":"/en/docs/Deep-Learning/基础知识/FromFullyConnectedLayerToConvolutionalLayer","h":"#总结","p":1236},{"i":1244,"t":"什么是Logistic Regression","u":"/en/docs/Deep-Learning/基础知识/LogisticRegression","h":"#�什么是logistic-regression","p":1243},{"i":1246,"t":"逻辑回归（Logistic Regression）和线性回归（Linear Regression）","u":"/en/docs/Deep-Learning/基础知识/LogisticRegression","h":"#逻辑回归logistic-regression和线性回归linear-regression","p":1243},{"i":1248,"t":"逻辑回归到底是回归任务（Regression）还是分类任务（Classification）？","u":"/en/docs/Deep-Learning/基础知识/LogisticRegression","h":"#逻辑回归到底是回归任务regression还是分类任务classification","p":1243},{"i":1250,"t":"为什么逻辑回归或其他分类任务不使用分类准确率作为损失函数？","u":"/en/docs/Deep-Learning/基础知识/LogisticRegression","h":"#为什么逻辑回归或其他分类任务不使用分类准确率作为损失函数","p":1243},{"i":1253,"t":"卷积对像素位置信息是敏感的","u":"/en/docs/Deep-Learning/基础知识/PoolingLayer","h":"#卷积对��像素位置信息是敏感的","p":1252},{"i":1255,"t":"池化层的作用","u":"/en/docs/Deep-Learning/基础知识/PoolingLayer","h":"#池化层的作用","p":1252},{"i":1257,"t":"池化的实现","u":"/en/docs/Deep-Learning/基础知识/PoolingLayer","h":"#池化的实现","p":1252},{"i":1260,"t":"什么是感知机","u":"/en/docs/Deep-Learning/基础知识/Perceptron","h":"#什么是感知机","p":1259},{"i":1262,"t":"详细原理","u":"/en/docs/Deep-Learning/基础知识/Perceptron","h":"#详细原理","p":1259},{"i":1264,"t":"总结","u":"/en/docs/Deep-Learning/基础知识/Perceptron","h":"#总结","p":1259},{"i":1267,"t":"背景","u":"/en/docs/Deep-Learning/基础知识/LeNet","h":"#背景","p":1266},{"i":1269,"t":"代码实现","u":"/en/docs/Deep-Learning/基础知识/LeNet","h":"#代码实现","p":1266},{"i":1271,"t":"问题","u":"/en/docs/Deep-Learning/基础知识/LeNet","h":"#问题","p":1266},{"i":1274,"t":"常用函数部分","u":"/en/docs/Deep-Learning/基础知识/PytorchBasics","h":"#常用函数部分","p":1273},{"i":1278,"t":"摘要","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#摘要","p":1276},{"i":1280,"t":"研究动机","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#研究动机","p":1276},{"i":1282,"t":"主要方法与实现","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#主要方法与实现","p":1276},{"i":1284,"t":"Diffusion Loss","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#diffusion-loss","p":1276},{"i":1286,"t":"采样","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#采样","p":1276},{"i":1288,"t":"双向注意力机制","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#双向注意力机制","p":1276},{"i":1289,"t":"结合掩码生成模型的思想 MAR","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#结合掩码生成模型的思想-mar","p":1276},{"i":1291,"t":"实验与结论","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#实验与结论","p":1276},{"i":1292,"t":"对比离散与连续、单向与双向注意力、逐 token 或逐 patch 预测","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#对比离散与连续单向与双向注意力逐-token-或逐-patch-预测","p":1276},{"i":1293,"t":"扩散过程 MLP 参数对性能的影响","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#扩散过程-mlp-参数对性能的影响","p":1276},{"i":1294,"t":"扩散过程采样步数对性能的影响","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#扩散过程采样步数对性能的影响","p":1276},{"i":1295,"t":"温度对多样性和准确性的控制","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#温度对多样性和准确性的控制","p":1276},{"i":1296,"t":"MAR 模型速度与效果的 trade-off","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#mar-模型速度与效果的-trade-off","p":1276},{"i":1300,"t":"整体结构","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#整体结构","p":1298},{"i":1302,"t":"Encoder","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#encoder","p":1298},{"i":1303,"t":"整体结构","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#整体结构-1","p":1298},{"i":1305,"t":"位置编码（Positional Encoding）","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#位置编码positional-encoding","p":1298},{"i":1307,"t":"具体结构","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#具体结构","p":1298},{"i":1309,"t":"Decoder","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#decoder","p":1298},{"i":1311,"t":"Autoregressive Decoder（AT）","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#autoregressive-decoderat","p":1298},{"i":1313,"t":"Non-Autoregressive Decoder（NAT）","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#non-autoregressive-decodernat","p":1298},{"i":1314,"t":"训练（Training）","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#训练training","p":1298},{"i":1315,"t":"损失函数","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#损失函数","p":1298},{"i":1317,"t":"Teacher Forcing","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#teacher-forcing","p":1298},{"i":1319,"t":"Teacher Forcing与Masked Multi-Head Self-Attention","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#teacher-forcing与masked-multi-head-self-attention","p":1298},{"i":1323,"t":"摘要","u":"/en/docs/Deep-Learning/论文笔记/Latent-Diffusion-Models","h":"#摘要","p":1321},{"i":1325,"t":"主要方法与模型结构","u":"/en/docs/Deep-Learning/论文笔记/Latent-Diffusion-Models","h":"#主要方法与模型结构","p":1321},{"i":1328,"t":"数学原理（Mathematical Preliminary）","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","h":"#数学原理mathematical-preliminary","p":1326},{"i":1329,"t":"先验概率与后验概率","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","h":"#先验概率与后验概率","p":1326},{"i":1331,"t":"条件概率与高斯分布的KL散度","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","h":"#条件概率与高斯分布的kl散度","p":1326},{"i":1333,"t":"马尔科夫链条件概率形式","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","h":"#马尔科夫链条件概率形式","p":1326},{"i":1335,"t":"参数重整化技巧","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","h":"#参数重整化技巧","p":1326},{"i":1339,"t":"摘要","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#摘要","p":1337},{"i":1341,"t":"Motivation 与主要贡献","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#motivation-与主要贡献","p":1337},{"i":1343,"t":"模型结构","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#模型结构","p":1337},{"i":1344,"t":"Image Tokenizer","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#image-tokenizer","p":1337},{"i":1346,"t":"Image Generation Autoregressive Model","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#image-generation-autoregressive-model","p":1337},{"i":1350,"t":"摘要","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#摘要","p":1348},{"i":1352,"t":"相关工作","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#相关工作","p":1348},{"i":1353,"t":"Generative Model Adaption","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#generative-model-adaption","p":1348},{"i":1355,"t":"Prompt Learning","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#prompt-learning","p":1348},{"i":1357,"t":"主要方法","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#主要方法","p":1348},{"i":1358,"t":"概述","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#概述","p":1348},{"i":1360,"t":"Image-specific Prompt Learning","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#image-specific-prompt-learning","p":1348},{"i":1363,"t":"摘要","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#摘要","p":1361},{"i":1365,"t":"研究动机","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#研究动机","p":1361},{"i":1367,"t":"主要方法与实现","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#主要方法与实现","p":1361},{"i":1369,"t":"Diffusion Loss","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#diffusion-loss","p":1361},{"i":1371,"t":"采样","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#采样","p":1361},{"i":1373,"t":"双向注意力机制","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#双向注意力机制","p":1361},{"i":1374,"t":"结合掩码生成模型的思想 MAR","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#结合掩码生成模型的思想-mar","p":1361},{"i":1376,"t":"实验与结论","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#实验与结论","p":1361},{"i":1377,"t":"对比离散与连续、单向与双向注意力、逐 token 或逐 patch 预测","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#对比离散与连续单向与双向注意力逐-token-或逐-patch-预测","p":1361},{"i":1378,"t":"扩散过程 MLP 参数对性能的影响","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#扩散过程-mlp-参数对性能的影响","p":1361},{"i":1379,"t":"扩散过程采样步数对性能的影响","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#扩散过程采样步数对性能的影响","p":1361},{"i":1380,"t":"温度对多样性和准确性的控制","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#温度对多样性和准确性的控制","p":1361},{"i":1381,"t":"MAR 模型速度与效果的 trade-off","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#mar-模型速度与效果的-trade-off","p":1361},{"i":1385,"t":"自回归模型的数学定义","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#自回归模型的数学定义","p":1383},{"i":1387,"t":"Autoregressive 模型时间线","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#autoregressive-模型时间线","p":1383},{"i":1388,"t":"PixelRNN（2016）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#pixelrnn2016","p":1383},{"i":1390,"t":"PixelCNN（2016）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#pixelcnn2016","p":1383},{"i":1392,"t":"VQ-VAE（2017）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#vq-vae2017","p":1383},{"i":1394,"t":"VQVAE-2（2019）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#vqvae-22019","p":1383},{"i":1396,"t":"VQGAN（2021）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#vqgan2021","p":1383},{"i":1398,"t":"RQTransformer（2021）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#rqtransformer2021","p":1383},{"i":1400,"t":"DALL-E（2021）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#dall-e2021","p":1383},{"i":1402,"t":"Parti（2022）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#parti2022","p":1383},{"i":1405,"t":"摘要","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#摘要","p":1404},{"i":1407,"t":"研究背景与动机","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#研究背景与动机","p":1404},{"i":1409,"t":"问题发现与提出","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#问题发现与提出","p":1404},{"i":1411,"t":"主要方法与贡献","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#主要方法与贡献","p":1404},{"i":1412,"t":"主要方法","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#主要方法","p":1404},{"i":1414,"t":"模型主要结构","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#模型主要结构","p":1404},{"i":1416,"t":"总结","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#总结","p":1404},{"i":1420,"t":"摘要","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#摘要","p":1418},{"i":1422,"t":"Motivation 与主要贡献","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#motivation-与主要贡献","p":1418},{"i":1424,"t":"模型结构","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#模型结构","p":1418},{"i":1425,"t":"Image Tokenizer","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#image-tokenizer","p":1418},{"i":1427,"t":"Image Generation Autoregressive Model","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#image-generation-autoregressive-model","p":1418},{"i":1430,"t":"图像生成基座模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Image-and-Video-Generative-Foundation-Model","h":"#图像生成基座模型","p":1429},{"i":1432,"t":"四种生成范式","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Image-and-Video-Generative-Foundation-Model","h":"#四种生成范式","p":1429},{"i":1434,"t":"如何训练优秀的生成基座模型？","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Image-and-Video-Generative-Foundation-Model","h":"#如何训练优秀的生成基座模型","p":1429},{"i":1436,"t":"视频生成基座模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Image-and-Video-Generative-Foundation-Model","h":"#视频生成基座模型","p":1429},{"i":1440,"t":"Python的广播机制","u":"/en/docs/Deep-Learning/Fill-The-Gaps","h":"#python的广播机制","p":1438},{"i":1442,"t":"点积（dot product）与矩阵乘法（matmul product）","u":"/en/docs/Deep-Learning/Fill-The-Gaps","h":"#点积dot-product与矩阵乘法matmul-product","p":1438},{"i":1444,"t":"zip函数与解压操作*","u":"/en/docs/Deep-Learning/Fill-The-Gaps","h":"#zip函数与解压操作","p":1438},{"i":1446,"t":"对batch_first参数的理解","u":"/en/docs/Deep-Learning/Fill-The-Gaps","h":"#对batch_first参数的理解","p":1438},{"i":1450,"t":"Denoising Diffusion Models 在图像中的应用","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"#denoising-diffusion-models-在图像中的应用","p":1448},{"i":1451,"t":"Diffusion Model 的结构","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"#diffusion-model-的结构","p":1448},{"i":1453,"t":"使用 Diffusion Model 对图像进行编辑和定制","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"#使用-diffusion-model-对图像进行编辑和定制","p":1448},{"i":1455,"t":"Diffusion Models 在视频中的应用","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"#diffusion-models-在视频中的应用","p":1448},{"i":1456,"t":"视频生成模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"#视频生成模型","p":1448},{"i":1458,"t":"视频的风格转换（Style Transfer）和编辑（editing）方法","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"#视频的风格转换style-transfer和编辑editing方法","p":1448},{"i":1462,"t":"支持我！","u":"/en/docs/Deep-Learning/intro","h":"#支持我","p":1460},{"i":1466,"t":"MDX 升级后的大量渲染报错","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","h":"#mdx-升级后的大量渲染报错","p":1464},{"i":1468,"t":"升级后的数学公式渲染问题","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","h":"#升级后的数学公式渲染问题","p":1464},{"i":1470,"t":"Admonitions 无法正常渲染","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","h":"#admonitions-无�法正常渲染","p":1464},{"i":1472,"t":"更新方式","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","h":"#更新方式","p":1464},{"i":1478,"t":"摘要","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#摘要","p":1476},{"i":1480,"t":"研究背景与动机","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#研究背景与动机","p":1476},{"i":1482,"t":"问题发现与提出","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#问题发现与提出","p":1476},{"i":1484,"t":"主要方法与贡献","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#主要方法与贡献","p":1476},{"i":1485,"t":"主要方法","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#主要方法","p":1476},{"i":1487,"t":"模型主要结构","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#模型主要结构","p":1476},{"i":1489,"t":"局限性与展望","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#局限性与展望","p":1476},{"i":1491,"t":"总结","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#总结","p":1476},{"i":1495,"t":"支持我！","u":"/en/docs/Others/intro","h":"#支持我","p":1493},{"i":1499,"t":"一、编写脚本","u":"/en/docs/Others/Linux/实用工具/终端代理","h":"#一编写脚本","p":1497},{"i":1501,"t":"二、关联终端配置文件","u":"/en/docs/Others/Linux/实用工具/终端代理","h":"#二关联终端配置文件","p":1497},{"i":1503,"t":"三、使用","u":"/en/docs/Others/Linux/实用工具/终端代理","h":"#三使用","p":1497},{"i":1506,"t":"一、发生原因","u":"/en/docs/Others/Linux/问题解决/双系统挂载Windows磁盘为只读文件","h":"#一发生原因","p":1505},{"i":1508,"t":"二、解决方案","u":"/en/docs/Others/Linux/问题解决/双系统挂载Windows磁盘为只读文件","h":"#二解决方案","p":1505},{"i":1511,"t":"一、latte-dock","u":"/en/docs/Others/Linux/客制化/如何让你的KDE看起来更像macOS","h":"#一latte-dock","p":1510},{"i":1513,"t":"二、Kde Plasmoids","u":"/en/docs/Others/Linux/客制化/如何让你的KDE看起来更像macOS","h":"#二kde-plasmoids","p":1510},{"i":1516,"t":"树","u":"/en/docs/Tui-Mian/计算机基础综合/数据结构","h":"#树","p":1515},{"i":1518,"t":"图","u":"/en/docs/Tui-Mian/计算机基础综合/数据结构","h":"#图","p":1515},{"i":1522,"t":"思路","u":"/en/docs/Tui-Mian/机试/大数除法","h":"#思路","p":1520},{"i":1524,"t":"参考代码","u":"/en/docs/Tui-Mian/机试/大数除法","h":"#参考代码","p":1520},{"i":1526,"t":"扩展","u":"/en/docs/Tui-Mian/机试/大数除法","h":"#扩展","p":1520},{"i":1529,"t":"面试常考问题","u":"/en/docs/Tui-Mian/数学/概率论","h":"#面试常考问题","p":1528},{"i":1533,"t":"一、基础知识","u":"/en/docs/Tui-Mian/数学/线性代数","h":"#一基础知识","p":1531},{"i":1535,"t":"二、面试常考问题","u":"/en/docs/Tui-Mian/数学/线性代数","h":"#二面试常考问题","p":1531},{"i":1538,"t":"一、线性代数","u":"/en/docs/Tui-Mian/数学/夏令营面试数学部分复习","h":"#一线性代数","p":1537},{"i":1540,"t":"二、概率论","u":"/en/docs/Tui-Mian/数学/夏令营面试数学部分复习","h":"#二概率论","p":1537},{"i":1543,"t":"一、U-2-Net","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#一u-2-net","p":1542},{"i":1544,"t":"（一）SOD任务","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#一sod任务","p":1542},{"i":1546,"t":"（二）网络结构","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二网络结构","p":1542},{"i":1548,"t":"（三）损失函数","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#三损失函数","p":1542},{"i":1550,"t":"（四）深度可分离卷积","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#四深度可分离卷积","p":1542},{"i":1552,"t":"二、YOLO","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二yolo","p":1542},{"i":1553,"t":"（一）mAP","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#一map","p":1542},{"i":1555,"t":"（二）YOLOv1","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二yolov1","p":1542},{"i":1557,"t":"（二）YOLOv2","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二yolov2","p":1542},{"i":1559,"t":"（三）YOLOv5","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#三yolov5","p":1542},{"i":1561,"t":"三、CBAM","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#三cbam","p":1542},{"i":1563,"t":"（一）总体结构","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#一总体结构","p":1542},{"i":1565,"t":"（二）通道注意力","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二通道注意力","p":1542},{"i":1567,"t":"（三）空间注意力","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#三空间注意力","p":1542},{"i":1569,"t":"（四）其他注意事项","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#四其他注意事项","p":1542},{"i":1571,"t":"四、Focal Loss","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#四focal-loss","p":1542},{"i":1573,"t":"五、SENet","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#五senet","p":1542},{"i":1575,"t":"六、自注意力机制","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#六自注意力机制","p":1542},{"i":1577,"t":"七、自我介绍","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#七自我介绍","p":1542},{"i":1578,"t":"（一）英文自我介绍","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#一英文自我介绍","p":1542},{"i":1580,"t":"（二）西电广研院自我介绍","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二西电广研院自我介绍","p":1542},{"i":1582,"t":"（三）电子科技大学自我介绍","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#三电子科技大学自我介绍","p":1542},{"i":1586,"t":"支持我！","u":"/en/docs/Tui-Mian/intro","h":"#支持我","p":1584},{"i":1589,"t":"前言","u":"/en/docs/Tui-Mian/Summary","h":"#前言","p":1588},{"i":1591,"t":"扫盲","u":"/en/docs/Tui-Mian/Summary","h":"#扫盲","p":1588},{"i":1593,"t":"个人情况","u":"/en/docs/Tui-Mian/Summary","h":"#个人情况","p":1588},{"i":1595,"t":"前期准备","u":"/en/docs/Tui-Mian/Summary","h":"#前期准备","p":1588},{"i":1596,"t":"夏令营","u":"/en/docs/Tui-Mian/Summary","h":"#夏令营","p":1588},{"i":1597,"t":"预推免","u":"/en/docs/Tui-Mian/Summary","h":"#预推免","p":1588},{"i":1598,"t":"后序","u":"/en/docs/Tui-Mian/Summary","h":"#后序","p":1588}],"index":{"version":"2.3.9","fields":["t"],"fieldVectors":[["t/802",[0,0.381]],["t/804",[0,0.381]],["t/807",[0,0.381]],["t/809",[1,6.505]],["t/813",[0,0.381]],["t/815",[0,0.381]],["t/819",[0,0.381]],["t/823",[0,0.381]],["t/825",[0,0.381]],["t/828",[0,0.381]],["t/829",[2,6.505]],["t/831",[3,6.505]],["t/833",[0,0.381]],["t/835",[0,0.381]],["t/837",[0,0.381]],["t/839",[0,0.381]],["t/843",[4,6.505]],["t/845",[5,6.505]],["t/848",[0,0.381]],["t/849",[0,0.291,6,4.974]],["t/851",[0,0.291,7,4.974]],["t/853",[0,0.291,8,4.974]],["t/855",[0,0.381]],["t/856",[0,0.291,9,4.974]],["t/858",[0,0.291,10,4.974]],["t/860",[0,0.291,11,4.974]],["t/862",[0,0.291,12,4.974]],["t/864",[0,0.381]],["t/865",[0,0.291,13,4.974]],["t/866",[14,4.974,15,4.974]],["t/868",[16,4.974,17,4.974]],["t/869",[18,4.974,19,4.974]],["t/870",[0,0.381]],["t/872",[20,3.99,21,4.974]],["t/873",[22,3.661,23,4.974]],["t/875",[24,4.215,25,4.974]],["t/877",[26,3.99,27,4.516]],["t/878",[27,4.516,28,4.215]],["t/880",[0,0.291,29,4.516]],["t/882",[30,4.516,31,4.974]],["t/884",[0,0.381]],["t/885",[0,0.291,32,4.974]],["t/887",[0,0.291,33,4.974]],["t/889",[0,0.291,34,4.974]],["t/890",[0,0.236,26,3.23,35,4.026]],["t/892",[0,0.291,36,4.974]],["t/894",[0,0.381]],["t/895",[0,0.291,20,3.99]],["t/897",[0,0.291,22,3.661]],["t/899",[0,0.291,24,4.215]],["t/901",[0,0.291,26,3.99]],["t/903",[0,0.291,28,4.215]],["t/907",[0,0.381]],["t/911",[0,0.381]],["t/912",[0,0.381]],["t/914",[0,0.381]],["t/916",[0,0.381]],["t/918",[0,0.381]],["t/920",[37,4.787]],["t/922",[38,6.505]],["t/924",[0,0.381]],["t/928",[0,0.381]],["t/930",[39,4.983]],["t/931",[0,0.381]],["t/933",[39,3.81,40,4.516]],["t/935",[0,0.381]],["t/937",[41,5.907]],["t/939",[42,3.532,43,4.516]],["t/941",[42,2.859,44,3.656,45,3.656]],["t/942",[46,5.512]],["t/943",[0,0.381]],["t/945",[47,3.99,48,4.516]],["t/947",[47,2.055,49,2.326,50,2.171,51,2.171,52,1.664,53,1.762]],["t/951",[0,0.291,22,3.661]],["t/952",[0,0.381]],["t/954",[0,0.381]],["t/956",[0,0.381]],["t/958",[0,0.381]],["t/959",[0,0.291,24,4.215]],["t/960",[0,0.381]],["t/962",[0,0.291,26,3.99]],["t/963",[0,0.381]],["t/965",[0,0.381]],["t/967",[0,0.381]],["t/969",[0,0.291,28,4.215]],["t/970",[0,0.381]],["t/972",[0,0.381]],["t/974",[0,0.381]],["t/976",[0,0.291,29,4.516]],["t/978",[0,0.381]],["t/980",[0,0.381]],["t/982",[0,0.381]],["t/984",[0,0.291,30,4.516]],["t/985",[0,0.381]],["t/987",[0,0.381]],["t/988",[0,0.291,54,4.974]],["t/989",[0,0.381]],["t/991",[0,0.381]],["t/993",[0,0.291,55,4.974]],["t/994",[0,0.381]],["t/996",[0,0.381]],["t/998",[0,0.381]],["t/1000",[0,0.381]],["t/1004",[0,0.381]],["t/1006",[0,0.381]],["t/1008",[0,0.381]],["t/1011",[0,0.381]],["t/1013",[56,6.505]],["t/1015",[57,6.505]],["t/1017",[0,0.381]],["t/1019",[0,0.381]],["t/1021",[0,0.381]],["t/1023",[58,6.505]],["t/1025",[0,0.381]],["t/1027",[0,0.381]],["t/1029",[59,6.505]],["t/1031",[37,2.489,60,3.382,61,3.382,62,2.258]],["t/1033",[63,6.505]],["t/1035",[62,3.321,64,3.532]],["t/1039",[0,0.381]],["t/1040",[0,0.355,65,3.656]],["t/1042",[0,0.381]],["t/1044",[0,0.381]],["t/1046",[0,0.381]],["t/1047",[0,0.381]],["t/1048",[0,0.291,66,3.231]],["t/1050",[0,0.276,66,1.894,67,2.146,68,2.647]],["t/1052",[0,0.291,69,4.516]],["t/1054",[0,0.236,20,3.23,70,3.23]],["t/1056",[0,0.236,22,2.963,70,3.23]],["t/1058",[0,0.381]],["t/1060",[0,0.381]],["t/1061",[0,0.381]],["t/1063",[71,4.516,72,4.516]],["t/1065",[0,0.381]],["t/1066",[0,0.291,73,4.516]],["t/1068",[0,0.355,66,2.615]],["t/1070",[0,0.347,66,3.058]],["t/1074",[0,0.381]],["t/1076",[39,2.591,74,3.382,75,3.382,76,3.071]],["t/1078",[0,0.381]],["t/1080",[52,2.615,77,4.026,78,4.026]],["t/1082",[79,4.974,80,4.974]],["t/1084",[81,4.974,82,4.516]],["t/1086",[37,2.146,46,2.471,76,2.647,83,2.916,84,2.916]],["t/1090",[0,0.381]],["t/1092",[0,0.381]],["t/1094",[0,0.381]],["t/1096",[0,0.381]],["t/1098",[85,6.505]],["t/1100",[0,0.236,86,3.656,87,3.412]],["t/1102",[86,3.071,88,3.382,89,3.382,90,3.382]],["t/1104",[0,0.381]],["t/1108",[91,5.512]],["t/1110",[91,5.512]],["t/1111",[91,5.512]],["t/1113",[0,0.381]],["t/1115",[0,0.381]],["t/1119",[92,6.505]],["t/1121",[93,6.505]],["t/1123",[94,6.505]],["t/1127",[95,6.505]],["t/1128",[0,0.381]],["t/1130",[0,0.381]],["t/1132",[52,3.231,53,3.42]],["t/1133",[52,3.231,53,3.42]],["t/1135",[52,3.231,53,3.42]],["t/1137",[52,3.231,53,3.42]],["t/1139",[50,2.866,51,2.866,52,2.197,53,2.326]],["t/1141",[52,3.231,96,4.974]],["t/1143",[0,0.236,52,2.615,53,2.769]],["t/1145",[0,0.381]],["t/1147",[0,0.381]],["t/1149",[0,0.381]],["t/1151",[0,0.381]],["t/1153",[0,0.291,97,4.974]],["t/1157",[0,0.381]],["t/1159",[0,0.236,98,4.026,99,3.656]],["t/1161",[0,0.236,99,3.656,100,4.026]],["t/1165",[0,0.381]],["t/1166",[0,0.355,65,3.656]],["t/1168",[0,0.381]],["t/1170",[0,0.381]],["t/1172",[0,0.381]],["t/1173",[0,0.381]],["t/1174",[0,0.291,66,3.231]],["t/1176",[0,0.276,66,1.894,67,2.146,68,2.647]],["t/1178",[0,0.291,69,4.516]],["t/1180",[0,0.236,20,3.23,70,3.23]],["t/1182",[0,0.236,22,2.963,70,3.23]],["t/1184",[0,0.381]],["t/1186",[0,0.381]],["t/1187",[0,0.381]],["t/1189",[71,4.516,72,4.516]],["t/1191",[0,0.381]],["t/1192",[0,0.291,73,4.516]],["t/1194",[0,0.355,66,2.615]],["t/1196",[0,0.347,66,3.058]],["t/1199",[0,0.381]],["t/1200",[0,0.236,101,4.026,102,3.412]],["t/1202",[103,3.382,104,3.382,105,3.382,106,3.382]],["t/1204",[0,0.381]],["t/1205",[0,0.198,107,3.382,108,3.382,109,3.382]],["t/1207",[0,0.198,110,2.866,111,3.382,112,2.713]],["t/1211",[0,0.381]],["t/1213",[113,6.505]],["t/1215",[114,6.505]],["t/1218",[0,0.381]],["t/1220",[0,0.381]],["t/1222",[115,6.505]],["t/1225",[87,2.866,110,2.866,116,3.071,117,3.071]],["t/1227",[82,2.647,87,2.471,110,2.471,116,2.647,117,2.647]],["t/1229",[118,6.505]],["t/1232",[0,0.291,119,4.974]],["t/1234",[0,0.381]],["t/1237",[0,0.381]],["t/1239",[0,0.381]],["t/1241",[0,0.381]],["t/1244",[102,4.215,120,4.516]],["t/1246",[102,3.412,120,3.656,121,4.026]],["t/1248",[122,6.505]],["t/1250",[0,0.381]],["t/1253",[0,0.381]],["t/1255",[0,0.381]],["t/1257",[0,0.381]],["t/1260",[0,0.381]],["t/1262",[0,0.381]],["t/1264",[0,0.381]],["t/1267",[0,0.381]],["t/1269",[0,0.381]],["t/1271",[0,0.381]],["t/1274",[0,0.381]],["t/1278",[0,0.381]],["t/1280",[0,0.381]],["t/1282",[0,0.381]],["t/1284",[64,3.532,112,3.99]],["t/1286",[0,0.381]],["t/1288",[0,0.381]],["t/1289",[0,0.291,123,3.99]],["t/1291",[0,0.381]],["t/1292",[0,0.347,67,2.146,124,2.647]],["t/1293",[0,0.355,125,3.656]],["t/1294",[0,0.381]],["t/1295",[0,0.381]],["t/1296",[0,0.236,123,3.23,126,3.656]],["t/1300",[0,0.381]],["t/1302",[39,4.983]],["t/1303",[0,0.381]],["t/1305",[39,3.81,40,4.516]],["t/1307",[0,0.381]],["t/1309",[41,5.907]],["t/1311",[42,3.532,43,4.516]],["t/1313",[42,2.859,44,3.656,45,3.656]],["t/1314",[46,5.512]],["t/1315",[0,0.381]],["t/1317",[47,3.99,48,4.516]],["t/1319",[47,2.055,49,2.326,50,2.171,51,2.171,52,1.664,53,1.762]],["t/1323",[0,0.381]],["t/1325",[0,0.381]],["t/1328",[127,4.974,128,4.974]],["t/1329",[0,0.381]],["t/1331",[129,6.505]],["t/1333",[0,0.381]],["t/1335",[0,0.381]],["t/1339",[0,0.381]],["t/1341",[0,0.291,130,4.516]],["t/1343",[0,0.381]],["t/1344",[67,3.661,131,3.81]],["t/1346",[37,2.489,42,2.402,62,2.258,131,2.591]],["t/1350",[0,0.381]],["t/1352",[0,0.381]],["t/1353",[37,2.963,62,2.688,132,4.026]],["t/1355",[66,3.231,133,4.516]],["t/1357",[0,0.381]],["t/1358",[0,0.381]],["t/1360",[66,2.197,131,2.591,133,3.071,134,3.382]],["t/1363",[0,0.381]],["t/1365",[0,0.381]],["t/1367",[0,0.381]],["t/1369",[64,3.532,112,3.99]],["t/1371",[0,0.381]],["t/1373",[0,0.381]],["t/1374",[0,0.291,123,3.99]],["t/1376",[0,0.381]],["t/1377",[0,0.347,67,2.146,124,2.647]],["t/1378",[0,0.355,125,3.656]],["t/1379",[0,0.381]],["t/1380",[0,0.381]],["t/1381",[0,0.236,123,3.23,126,3.656]],["t/1385",[0,0.381]],["t/1387",[0,0.291,42,3.532]],["t/1388",[135,6.505]],["t/1390",[136,6.505]],["t/1392",[137,4.974,138,4.974]],["t/1394",[139,4.974,140,4.974]],["t/1396",[141,6.505]],["t/1398",[142,6.505]],["t/1400",[143,4.974,144,4.974]],["t/1402",[145,6.505]],["t/1405",[0,0.381]],["t/1407",[0,0.381]],["t/1409",[0,0.381]],["t/1411",[0,0.381]],["t/1412",[0,0.381]],["t/1414",[0,0.381]],["t/1416",[0,0.381]],["t/1420",[0,0.381]],["t/1422",[0,0.291,130,4.516]],["t/1424",[0,0.381]],["t/1425",[67,3.661,131,3.81]],["t/1427",[37,2.489,42,2.402,62,2.258,131,2.591]],["t/1430",[0,0.381]],["t/1432",[0,0.381]],["t/1434",[0,0.381]],["t/1436",[0,0.381]],["t/1440",[146,6.505]],["t/1442",[147,4.026,148,4.026,149,4.026]],["t/1444",[150,6.505]],["t/1446",[151,6.505]],["t/1450",[0,0.198,62,2.258,64,2.402,152,3.382]],["t/1451",[0,0.236,62,2.688,64,2.859]],["t/1453",[0,0.31,62,2.258,64,2.402]],["t/1455",[0,0.236,62,2.688,64,2.859]],["t/1456",[0,0.381]],["t/1458",[153,4.974,154,4.974]],["t/1462",[0,0.381]],["t/1466",[0,0.291,155,4.974]],["t/1468",[0,0.381]],["t/1470",[0,0.291,156,4.974]],["t/1472",[0,0.381]],["t/1478",[0,0.381]],["t/1480",[0,0.381]],["t/1482",[0,0.381]],["t/1484",[0,0.381]],["t/1485",[0,0.381]],["t/1487",[0,0.381]],["t/1489",[0,0.381]],["t/1491",[0,0.381]],["t/1495",[0,0.381]],["t/1499",[0,0.381]],["t/1501",[0,0.381]],["t/1503",[0,0.381]],["t/1506",[0,0.381]],["t/1508",[0,0.381]],["t/1511",[157,4.974,158,4.974]],["t/1513",[159,4.974,160,4.974]],["t/1516",[0,0.381]],["t/1518",[0,0.381]],["t/1522",[0,0.381]],["t/1524",[0,0.381]],["t/1526",[0,0.381]],["t/1529",[0,0.381]],["t/1533",[0,0.381]],["t/1535",[0,0.381]],["t/1538",[0,0.381]],["t/1540",[0,0.381]],["t/1543",[22,2.963,161,4.026,162,4.026]],["t/1544",[163,6.505]],["t/1546",[0,0.381]],["t/1548",[0,0.381]],["t/1550",[0,0.381]],["t/1552",[164,6.505]],["t/1553",[165,6.505]],["t/1555",[166,6.505]],["t/1557",[167,6.505]],["t/1559",[168,6.505]],["t/1561",[169,6.505]],["t/1563",[0,0.381]],["t/1565",[0,0.381]],["t/1567",[0,0.381]],["t/1569",[0,0.381]],["t/1571",[112,3.99,170,4.974]],["t/1573",[171,6.505]],["t/1575",[0,0.381]],["t/1577",[0,0.381]],["t/1578",[0,0.381]],["t/1580",[0,0.381]],["t/1582",[0,0.381]],["t/1586",[0,0.381]],["t/1589",[0,0.381]],["t/1591",[0,0.381]],["t/1593",[0,0.381]],["t/1595",[0,0.381]],["t/1596",[0,0.381]],["t/1597",[0,0.381]],["t/1598",[0,0.381]]],"invertedIndex":[["",{"_index":0,"t":{"802":{"position":[[0,4]]},"804":{"position":[[0,2]]},"807":{"position":[[0,4]]},"813":{"position":[[0,4]]},"815":{"position":[[0,2]]},"819":{"position":[[0,4]]},"823":{"position":[[0,4]]},"825":{"position":[[0,2]]},"828":{"position":[[0,8]]},"833":{"position":[[0,7]]},"835":{"position":[[0,5]]},"837":{"position":[[0,2]]},"839":{"position":[[0,2]]},"848":{"position":[[0,6]]},"849":{"position":[[4,9]]},"851":{"position":[[4,5]]},"853":{"position":[[4,4]]},"855":{"position":[[0,9]]},"856":{"position":[[4,2]]},"858":{"position":[[4,3]]},"860":{"position":[[4,6]]},"862":{"position":[[4,7]]},"864":{"position":[[0,8]]},"865":{"position":[[4,10]]},"870":{"position":[[0,14]]},"880":{"position":[[3,5]]},"884":{"position":[[0,14]]},"885":{"position":[[4,2]]},"887":{"position":[[4,2]]},"889":{"position":[[4,4]]},"890":{"position":[[4,2]]},"892":{"position":[[4,4]]},"894":{"position":[[0,4]]},"895":{"position":[[2,9]]},"897":{"position":[[2,9]]},"899":{"position":[[2,4]]},"901":{"position":[[2,4]]},"903":{"position":[[2,3]]},"907":{"position":[[0,4]]},"911":{"position":[[0,2]]},"912":{"position":[[0,9]]},"914":{"position":[[0,8]]},"916":{"position":[[0,4]]},"918":{"position":[[0,7]]},"924":{"position":[[0,4]]},"928":{"position":[[0,4]]},"931":{"position":[[0,4]]},"935":{"position":[[0,4]]},"943":{"position":[[0,4]]},"951":{"position":[[4,8]]},"952":{"position":[[0,8]]},"954":{"position":[[0,4]]},"956":{"position":[[0,6]]},"958":{"position":[[0,5]]},"959":{"position":[[4,4]]},"960":{"position":[[0,5]]},"962":{"position":[[4,9]]},"963":{"position":[[0,3]]},"965":{"position":[[0,4]]},"967":{"position":[[0,5]]},"969":{"position":[[4,6]]},"970":{"position":[[0,6]]},"972":{"position":[[0,7]]},"974":{"position":[[0,6]]},"976":{"position":[[4,6]]},"978":{"position":[[0,4]]},"980":{"position":[[0,4]]},"982":{"position":[[0,7]]},"984":{"position":[[4,6]]},"985":{"position":[[0,8]]},"987":{"position":[[0,8]]},"988":{"position":[[4,4]]},"989":{"position":[[0,6]]},"991":{"position":[[0,6]]},"993":{"position":[[5,4]]},"994":{"position":[[0,5]]},"996":{"position":[[0,9]]},"998":{"position":[[0,6]]},"1000":{"position":[[0,9]]},"1004":{"position":[[0,4]]},"1006":{"position":[[0,4]]},"1008":{"position":[[0,4]]},"1011":{"position":[[0,11]]},"1017":{"position":[[0,12]]},"1019":{"position":[[0,9]]},"1021":{"position":[[0,6]]},"1025":{"position":[[0,6]]},"1027":{"position":[[0,10]]},"1039":{"position":[[0,2]]},"1040":{"position":[[0,2],[12,4]]},"1042":{"position":[[0,4]]},"1044":{"position":[[0,8]]},"1046":{"position":[[0,2]]},"1047":{"position":[[0,4]]},"1048":{"position":[[8,4]]},"1050":{"position":[[8,1],[19,1]]},"1052":{"position":[[22,5]]},"1054":{"position":[[0,2]]},"1056":{"position":[[0,2]]},"1058":{"position":[[0,6]]},"1060":{"position":[[0,4]]},"1061":{"position":[[0,12]]},"1065":{"position":[[0,7]]},"1066":{"position":[[10,5]]},"1068":{"position":[[0,9],[18,7]]},"1070":{"position":[[0,8],[17,8],[34,4]]},"1074":{"position":[[0,2]]},"1078":{"position":[[0,2]]},"1090":{"position":[[0,2]]},"1092":{"position":[[0,8]]},"1094":{"position":[[0,4]]},"1096":{"position":[[0,4]]},"1100":{"position":[[6,2]]},"1104":{"position":[[0,2]]},"1113":{"position":[[0,10]]},"1115":{"position":[[0,10]]},"1128":{"position":[[0,9]]},"1130":{"position":[[0,11]]},"1143":{"position":[[15,3]]},"1145":{"position":[[0,4]]},"1147":{"position":[[0,11]]},"1149":{"position":[[0,6]]},"1151":{"position":[[0,2]]},"1153":{"position":[[0,8]]},"1157":{"position":[[0,4]]},"1159":{"position":[[0,5]]},"1161":{"position":[[0,4]]},"1165":{"position":[[0,2]]},"1166":{"position":[[0,2],[12,4]]},"1168":{"position":[[0,4]]},"1170":{"position":[[0,8]]},"1172":{"position":[[0,2]]},"1173":{"position":[[0,4]]},"1174":{"position":[[8,4]]},"1176":{"position":[[8,1],[19,1]]},"1178":{"position":[[22,5]]},"1180":{"position":[[0,2]]},"1182":{"position":[[0,2]]},"1184":{"position":[[0,6]]},"1186":{"position":[[0,4]]},"1187":{"position":[[0,12]]},"1191":{"position":[[0,7]]},"1192":{"position":[[10,5]]},"1194":{"position":[[0,9],[18,7]]},"1196":{"position":[[0,8],[17,8],[34,4]]},"1199":{"position":[[0,4]]},"1200":{"position":[[10,1]]},"1204":{"position":[[0,4]]},"1205":{"position":[[19,4]]},"1207":{"position":[[19,5]]},"1211":{"position":[[0,6]]},"1218":{"position":[[0,2]]},"1220":{"position":[[0,7]]},"1232":{"position":[[4,2]]},"1234":{"position":[[0,5]]},"1237":{"position":[[0,10]]},"1239":{"position":[[0,8]]},"1241":{"position":[[0,2]]},"1250":{"position":[[0,29]]},"1253":{"position":[[0,13]]},"1255":{"position":[[0,6]]},"1257":{"position":[[0,5]]},"1260":{"position":[[0,6]]},"1262":{"position":[[0,4]]},"1264":{"position":[[0,2]]},"1267":{"position":[[0,2]]},"1269":{"position":[[0,4]]},"1271":{"position":[[0,2]]},"1274":{"position":[[0,6]]},"1278":{"position":[[0,2]]},"1280":{"position":[[0,4]]},"1282":{"position":[[0,7]]},"1286":{"position":[[0,2]]},"1288":{"position":[[0,7]]},"1289":{"position":[[0,11]]},"1291":{"position":[[0,5]]},"1292":{"position":[[0,18],[25,2],[34,2]]},"1293":{"position":[[0,4],[9,8]]},"1294":{"position":[[0,14]]},"1295":{"position":[[0,13]]},"1296":{"position":[[4,8]]},"1300":{"position":[[0,4]]},"1303":{"position":[[0,4]]},"1307":{"position":[[0,4]]},"1315":{"position":[[0,4]]},"1323":{"position":[[0,2]]},"1325":{"position":[[0,9]]},"1329":{"position":[[0,9]]},"1333":{"position":[[0,11]]},"1335":{"position":[[0,7]]},"1339":{"position":[[0,2]]},"1341":{"position":[[11,5]]},"1343":{"position":[[0,4]]},"1350":{"position":[[0,2]]},"1352":{"position":[[0,4]]},"1357":{"position":[[0,4]]},"1358":{"position":[[0,2]]},"1363":{"position":[[0,2]]},"1365":{"position":[[0,4]]},"1367":{"position":[[0,7]]},"1371":{"position":[[0,2]]},"1373":{"position":[[0,7]]},"1374":{"position":[[0,11]]},"1376":{"position":[[0,5]]},"1377":{"position":[[0,18],[25,2],[34,2]]},"1378":{"position":[[0,4],[9,8]]},"1379":{"position":[[0,14]]},"1380":{"position":[[0,13]]},"1381":{"position":[[4,8]]},"1385":{"position":[[0,10]]},"1387":{"position":[[15,5]]},"1405":{"position":[[0,2]]},"1407":{"position":[[0,7]]},"1409":{"position":[[0,7]]},"1411":{"position":[[0,7]]},"1412":{"position":[[0,4]]},"1414":{"position":[[0,6]]},"1416":{"position":[[0,2]]},"1420":{"position":[[0,2]]},"1422":{"position":[[11,5]]},"1424":{"position":[[0,4]]},"1430":{"position":[[0,8]]},"1432":{"position":[[0,6]]},"1434":{"position":[[0,14]]},"1436":{"position":[[0,8]]},"1450":{"position":[[27,7]]},"1451":{"position":[[16,3]]},"1453":{"position":[[0,2],[19,10]]},"1455":{"position":[[17,7]]},"1456":{"position":[[0,6]]},"1462":{"position":[[0,4]]},"1466":{"position":[[4,10]]},"1468":{"position":[[0,12]]},"1470":{"position":[[12,6]]},"1472":{"position":[[0,4]]},"1478":{"position":[[0,2]]},"1480":{"position":[[0,7]]},"1482":{"position":[[0,7]]},"1484":{"position":[[0,7]]},"1485":{"position":[[0,4]]},"1487":{"position":[[0,6]]},"1489":{"position":[[0,6]]},"1491":{"position":[[0,2]]},"1495":{"position":[[0,4]]},"1499":{"position":[[0,6]]},"1501":{"position":[[0,10]]},"1503":{"position":[[0,4]]},"1506":{"position":[[0,6]]},"1508":{"position":[[0,6]]},"1516":{"position":[[0,1]]},"1518":{"position":[[0,1]]},"1522":{"position":[[0,2]]},"1524":{"position":[[0,4]]},"1526":{"position":[[0,2]]},"1529":{"position":[[0,6]]},"1533":{"position":[[0,6]]},"1535":{"position":[[0,8]]},"1538":{"position":[[0,6]]},"1540":{"position":[[0,5]]},"1546":{"position":[[0,7]]},"1548":{"position":[[0,7]]},"1550":{"position":[[0,10]]},"1563":{"position":[[0,7]]},"1565":{"position":[[0,8]]},"1567":{"position":[[0,8]]},"1569":{"position":[[0,9]]},"1575":{"position":[[0,8]]},"1577":{"position":[[0,6]]},"1578":{"position":[[0,9]]},"1580":{"position":[[0,12]]},"1582":{"position":[[0,13]]},"1586":{"position":[[0,4]]},"1589":{"position":[[0,2]]},"1591":{"position":[[0,2]]},"1593":{"position":[[0,4]]},"1595":{"position":[[0,4]]},"1596":{"position":[[0,3]]},"1597":{"position":[[0,3]]},"1598":{"position":[[0,2]]}}}],["1",{"_index":20,"t":{"872":{"position":[[0,2]]},"895":{"position":[[0,1]]},"1054":{"position":[[9,1]]},"1180":{"position":[[9,1]]}}}],["1.1",{"_index":6,"t":{"849":{"position":[[0,3]]}}}],["1.2",{"_index":7,"t":{"851":{"position":[[0,3]]}}}],["1.3",{"_index":8,"t":{"853":{"position":[[0,3]]}}}],["11",{"_index":55,"t":{"993":{"position":[[0,4]]}}}],["1x1",{"_index":119,"t":{"1232":{"position":[[0,3]]}}}],["2",{"_index":22,"t":{"873":{"position":[[0,2]]},"897":{"position":[[0,1]]},"951":{"position":[[0,3]]},"1056":{"position":[[9,1]]},"1182":{"position":[[9,1]]},"1543":{"position":[[4,1]]}}}],["2.1",{"_index":9,"t":{"856":{"position":[[0,3]]}}}],["2.2",{"_index":10,"t":{"858":{"position":[[0,3]]}}}],["2.3",{"_index":11,"t":{"860":{"position":[[0,3]]}}}],["2.4",{"_index":12,"t":{"862":{"position":[[0,3]]}}}],["2（2019",{"_index":140,"t":{"1394":{"position":[[6,7]]}}}],["3",{"_index":24,"t":{"875":{"position":[[0,2]]},"899":{"position":[[0,1]]},"959":{"position":[[0,3]]}}}],["3.1",{"_index":13,"t":{"865":{"position":[[0,3]]}}}],["3.2",{"_index":14,"t":{"866":{"position":[[0,3]]}}}],["3.3",{"_index":16,"t":{"868":{"position":[[0,3]]}}}],["3.4",{"_index":18,"t":{"869":{"position":[[0,3]]}}}],["4",{"_index":26,"t":{"877":{"position":[[0,2]]},"890":{"position":[[7,10]]},"901":{"position":[[0,1]]},"962":{"position":[[0,3]]}}}],["5",{"_index":28,"t":{"878":{"position":[[0,2]]},"903":{"position":[[0,1]]},"969":{"position":[[0,3]]}}}],["5.1",{"_index":32,"t":{"885":{"position":[[0,3]]}}}],["5.2",{"_index":33,"t":{"887":{"position":[[0,3]]}}}],["5.3",{"_index":34,"t":{"889":{"position":[[0,3]]}}}],["5.4",{"_index":35,"t":{"890":{"position":[[0,3]]}}}],["5.5",{"_index":36,"t":{"892":{"position":[[0,3]]}}}],["6",{"_index":29,"t":{"880":{"position":[[0,2]]},"976":{"position":[[0,3]]}}}],["7",{"_index":30,"t":{"882":{"position":[[0,2]]},"984":{"position":[[0,3]]}}}],["8",{"_index":54,"t":{"988":{"position":[[0,3]]}}}],["adapt",{"_index":132,"t":{"1353":{"position":[[17,8]]}}}],["admonit",{"_index":156,"t":{"1470":{"position":[[0,11]]}}}],["anaconda",{"_index":65,"t":{"1040":{"position":[[3,8]]},"1166":{"position":[[3,8]]}}}],["ar",{"_index":56,"t":{"1013":{"position":[[0,9]]}}}],["attent",{"_index":53,"t":{"947":{"position":[[39,9]]},"1132":{"position":[[5,12]]},"1133":{"position":[[8,9]]},"1135":{"position":[[5,14]]},"1137":{"position":[[5,12]]},"1139":{"position":[[16,9]]},"1143":{"position":[[5,9]]},"1319":{"position":[[39,9]]}}}],["attention与cnn",{"_index":96,"t":{"1141":{"position":[[5,16]]}}}],["autoregress",{"_index":42,"t":{"939":{"position":[[0,14]]},"941":{"position":[[4,14]]},"1311":{"position":[[0,14]]},"1313":{"position":[[4,14]]},"1346":{"position":[[17,14]]},"1387":{"position":[[0,14]]},"1427":{"position":[[17,14]]}}}],["base",{"_index":61,"t":{"1031":{"position":[[14,5]]}}}],["batch_first",{"_index":151,"t":{"1446":{"position":[[0,17]]}}}],["benchmark",{"_index":58,"t":{"1023":{"position":[[0,9]]}}}],["bert",{"_index":81,"t":{"1084":{"position":[[9,4]]}}}],["bert（bidirect",{"_index":74,"t":{"1076":{"position":[[0,18]]}}}],["boch",{"_index":3,"t":{"831":{"position":[[0,6]]}}}],["c",{"_index":4,"t":{"843":{"position":[[0,7]]}}}],["cbam",{"_index":169,"t":{"1561":{"position":[[0,6]]}}}],["cnn",{"_index":95,"t":{"1127":{"position":[[0,9]]}}}],["compute_text_featur",{"_index":69,"t":{"1052":{"position":[[0,21]]},"1178":{"position":[[0,21]]}}}],["convolut",{"_index":99,"t":{"1159":{"position":[[16,11]]},"1161":{"position":[[15,11]]}}}],["cross",{"_index":110,"t":{"1207":{"position":[[0,5]]},"1225":{"position":[[15,5]]},"1227":{"position":[[16,5]]}}}],["dall",{"_index":143,"t":{"1400":{"position":[[0,4]]}}}],["dataset",{"_index":93,"t":{"1121":{"position":[[0,7]]}}}],["decod",{"_index":41,"t":{"937":{"position":[[0,7]]},"1309":{"position":[[0,7]]}}}],["decoder（at",{"_index":43,"t":{"939":{"position":[[15,11]]},"1311":{"position":[[15,11]]}}}],["decoder（nat",{"_index":45,"t":{"941":{"position":[[19,12]]},"1313":{"position":[[19,12]]}}}],["denois",{"_index":152,"t":{"1450":{"position":[[0,9]]}}}],["depthwis",{"_index":98,"t":{"1159":{"position":[[6,9]]}}}],["diffus",{"_index":64,"t":{"1035":{"position":[[0,14]]},"1284":{"position":[[0,9]]},"1369":{"position":[[0,9]]},"1450":{"position":[[10,9]]},"1451":{"position":[[0,9]]},"1453":{"position":[[3,9]]},"1455":{"position":[[0,9]]}}}],["disclaim",{"_index":1,"t":{"809":{"position":[[0,10]]}}}],["discrimin",{"_index":38,"t":{"922":{"position":[[0,18]]}}}],["dock",{"_index":158,"t":{"1511":{"position":[[8,4]]}}}],["dot",{"_index":147,"t":{"1442":{"position":[[0,6]]}}}],["embed",{"_index":68,"t":{"1050":{"position":[[21,9]]},"1176":{"position":[[21,9]]}}}],["encod",{"_index":39,"t":{"930":{"position":[[0,7]]},"933":{"position":[[16,9]]},"1076":{"position":[[19,7]]},"1302":{"position":[[0,7]]},"1305":{"position":[[16,9]]}}}],["entropi",{"_index":111,"t":{"1207":{"position":[[6,7]]}}}],["error",{"_index":109,"t":{"1205":{"position":[[13,5]]}}}],["e（2021",{"_index":144,"t":{"1400":{"position":[[5,7]]}}}],["fa",{"_index":15,"t":{"866":{"position":[[4,9]]}}}],["fine",{"_index":79,"t":{"1082":{"position":[[0,4]]}}}],["first",{"_index":21,"t":{"872":{"position":[[3,9]]}}}],["flow",{"_index":60,"t":{"1031":{"position":[[0,13]]}}}],["focal",{"_index":170,"t":{"1571":{"position":[[0,7]]}}}],["fold",{"_index":116,"t":{"1225":{"position":[[10,4]]},"1227":{"position":[[11,4]]}}}],["follow",{"_index":23,"t":{"873":{"position":[[3,10]]}}}],["forc",{"_index":48,"t":{"945":{"position":[[8,7]]},"1317":{"position":[[8,7]]}}}],["forcing与mask",{"_index":49,"t":{"947":{"position":[[8,14]]},"1319":{"position":[[8,14]]}}}],["gan",{"_index":63,"t":{"1033":{"position":[[0,11]]}}}],["geeko",{"_index":2,"t":{"829":{"position":[[0,7]]}}}],["gener",{"_index":37,"t":{"920":{"position":[[0,14]]},"1031":{"position":[[20,10]]},"1086":{"position":[[5,10]]},"1346":{"position":[[6,10]]},"1353":{"position":[[0,10]]},"1427":{"position":[[6,10]]}}}],["gpt",{"_index":83,"t":{"1086":{"position":[[0,4]]}}}],["head",{"_index":51,"t":{"947":{"position":[[29,4]]},"1139":{"position":[[6,4]]},"1319":{"position":[[29,4]]}}}],["imag",{"_index":131,"t":{"1344":{"position":[[0,5]]},"1346":{"position":[[0,5]]},"1360":{"position":[[0,5]]},"1425":{"position":[[0,5]]},"1427":{"position":[[0,5]]}}}],["k",{"_index":87,"t":{"1100":{"position":[[4,1]]},"1225":{"position":[[8,1]]},"1227":{"position":[[9,1]]}}}],["kde",{"_index":159,"t":{"1513":{"position":[[0,5]]}}}],["kl",{"_index":129,"t":{"1331":{"position":[[0,14]]}}}],["l1",{"_index":113,"t":{"1213":{"position":[[0,5]]}}}],["l2",{"_index":114,"t":{"1215":{"position":[[0,10]]}}}],["latt",{"_index":157,"t":{"1511":{"position":[[0,7]]}}}],["learn",{"_index":133,"t":{"1355":{"position":[[7,8]]},"1360":{"position":[[22,8]]}}}],["lenet",{"_index":115,"t":{"1222":{"position":[[0,8]]}}}],["linear",{"_index":104,"t":{"1202":{"position":[[17,6]]}}}],["linux的编译系统对c",{"_index":5,"t":{"845":{"position":[[0,22]]}}}],["ll(1",{"_index":27,"t":{"877":{"position":[[3,10]]},"878":{"position":[[3,10]]}}}],["ll(1)文法到ll(1",{"_index":31,"t":{"882":{"position":[[3,21]]}}}],["logist",{"_index":102,"t":{"1200":{"position":[[12,10]]},"1244":{"position":[[0,11]]},"1246":{"position":[[0,13]]}}}],["loss",{"_index":112,"t":{"1207":{"position":[[14,4]]},"1284":{"position":[[10,4]]},"1369":{"position":[[10,4]]},"1571":{"position":[[8,4]]}}}],["map",{"_index":165,"t":{"1553":{"position":[[0,6]]}}}],["mapper",{"_index":73,"t":{"1066":{"position":[[0,9]]},"1192":{"position":[[0,9]]}}}],["mar",{"_index":123,"t":{"1289":{"position":[[12,3]]},"1296":{"position":[[0,3]]},"1374":{"position":[[12,3]]},"1381":{"position":[[0,3]]}}}],["mask",{"_index":97,"t":{"1153":{"position":[[9,4]]}}}],["mathemat",{"_index":127,"t":{"1328":{"position":[[0,17]]}}}],["mdx",{"_index":155,"t":{"1466":{"position":[[0,3]]}}}],["mean",{"_index":107,"t":{"1205":{"position":[[0,4]]}}}],["mlp",{"_index":125,"t":{"1293":{"position":[[5,3]]},"1378":{"position":[[5,3]]}}}],["model",{"_index":62,"t":{"1031":{"position":[[31,6]]},"1035":{"position":[[15,6]]},"1346":{"position":[[32,5]]},"1353":{"position":[[11,5]]},"1427":{"position":[[32,5]]},"1450":{"position":[[20,6]]},"1451":{"position":[[10,5]]},"1453":{"position":[[13,5]]},"1455":{"position":[[10,6]]}}}],["motiv",{"_index":130,"t":{"1341":{"position":[[0,10]]},"1422":{"position":[[0,10]]}}}],["multi",{"_index":50,"t":{"947":{"position":[[23,5]]},"1139":{"position":[[0,5]]},"1319":{"position":[[23,5]]}}}],["nar",{"_index":57,"t":{"1015":{"position":[[0,11]]}}}],["net",{"_index":162,"t":{"1543":{"position":[[6,3]]}}}],["non",{"_index":44,"t":{"941":{"position":[[0,3]]},"1313":{"position":[[0,3]]}}}],["nucleu",{"_index":89,"t":{"1102":{"position":[[6,10]]}}}],["overview",{"_index":92,"t":{"1119":{"position":[[0,8]]}}}],["p",{"_index":88,"t":{"1102":{"position":[[4,1]]}}}],["parti（2022",{"_index":145,"t":{"1402":{"position":[[0,11]]}}}],["patch",{"_index":124,"t":{"1292":{"position":[[28,5]]},"1377":{"position":[[28,5]]}}}],["pixelcnn（2016",{"_index":136,"t":{"1390":{"position":[[0,14]]}}}],["pixelrnn（2016",{"_index":135,"t":{"1388":{"position":[[0,14]]}}}],["plasmoid",{"_index":160,"t":{"1513":{"position":[[6,9]]}}}],["pointwis",{"_index":100,"t":{"1161":{"position":[[5,9]]}}}],["posit",{"_index":40,"t":{"933":{"position":[[0,15]]},"1305":{"position":[[0,15]]}}}],["pre",{"_index":84,"t":{"1086":{"position":[[16,3]]}}}],["preliminari",{"_index":128,"t":{"1328":{"position":[[18,12]]}}}],["pretrain",{"_index":78,"t":{"1080":{"position":[[16,11]]}}}],["product",{"_index":149,"t":{"1442":{"position":[[28,8]]}}}],["product）与矩阵乘法（matmul",{"_index":148,"t":{"1442":{"position":[[7,20]]}}}],["prompt",{"_index":66,"t":{"1048":{"position":[[0,7]]},"1050":{"position":[[0,7]]},"1068":{"position":[[10,7]]},"1070":{"position":[[9,7],[26,7]]},"1174":{"position":[[0,7]]},"1176":{"position":[[0,7]]},"1194":{"position":[[10,7]]},"1196":{"position":[[9,7],[26,7]]},"1355":{"position":[[0,6]]},"1360":{"position":[[15,6]]}}}],["python",{"_index":146,"t":{"1440":{"position":[[0,11]]}}}],["rectifi",{"_index":103,"t":{"1202":{"position":[[0,16]]}}}],["regress",{"_index":120,"t":{"1244":{"position":[[12,10]]},"1246":{"position":[[38,11]]}}}],["regression）和线性回归（linear",{"_index":121,"t":{"1246":{"position":[[14,23]]}}}],["regression）还是分类任务（classif",{"_index":122,"t":{"1248":{"position":[[0,46]]}}}],["relat",{"_index":94,"t":{"1123":{"position":[[0,7]]}}}],["relu",{"_index":106,"t":{"1202":{"position":[[30,5]]}}}],["represent",{"_index":75,"t":{"1076":{"position":[[27,14]]}}}],["re与有穷自动机fa",{"_index":17,"t":{"868":{"position":[[4,18]]}}}],["rm与有穷自动机fa",{"_index":19,"t":{"869":{"position":[[4,19]]}}}],["rqtransformer（2021",{"_index":142,"t":{"1398":{"position":[[0,19]]}}}],["sampl",{"_index":90,"t":{"1102":{"position":[[17,9]]}}}],["select",{"_index":25,"t":{"875":{"position":[[3,10]]}}}],["self",{"_index":52,"t":{"947":{"position":[[34,4]]},"1080":{"position":[[0,4]]},"1132":{"position":[[0,4]]},"1133":{"position":[[0,7]]},"1135":{"position":[[0,4]]},"1137":{"position":[[0,4]]},"1139":{"position":[[11,4]]},"1141":{"position":[[0,4]]},"1143":{"position":[[0,4]]},"1319":{"position":[[34,4]]}}}],["senet",{"_index":171,"t":{"1573":{"position":[[0,7]]}}}],["sigmoid",{"_index":101,"t":{"1200":{"position":[[0,9]]}}}],["sod",{"_index":163,"t":{"1544":{"position":[[0,8]]}}}],["specif",{"_index":134,"t":{"1360":{"position":[[6,8]]}}}],["squar",{"_index":108,"t":{"1205":{"position":[[5,7]]}}}],["stage",{"_index":70,"t":{"1054":{"position":[[3,5]]},"1056":{"position":[[3,5]]},"1180":{"position":[[3,5]]},"1182":{"position":[[3,5]]}}}],["style",{"_index":153,"t":{"1458":{"position":[[0,13]]}}}],["summari",{"_index":118,"t":{"1229":{"position":[[0,7]]}}}],["supervis",{"_index":77,"t":{"1080":{"position":[[5,10]]}}}],["teacher",{"_index":47,"t":{"945":{"position":[[0,7]]},"947":{"position":[[0,7]]},"1317":{"position":[[0,7]]},"1319":{"position":[[0,7]]}}}],["temperatur",{"_index":85,"t":{"1098":{"position":[[0,15]]}}}],["token",{"_index":67,"t":{"1050":{"position":[[10,8]]},"1176":{"position":[[10,8]]},"1292":{"position":[[19,5]]},"1344":{"position":[[6,9]]},"1377":{"position":[[19,5]]},"1425":{"position":[[6,9]]}}}],["top",{"_index":86,"t":{"1100":{"position":[[0,3]]},"1102":{"position":[[0,3]]}}}],["trade",{"_index":126,"t":{"1296":{"position":[[13,5]]},"1381":{"position":[[13,5]]}}}],["train",{"_index":46,"t":{"942":{"position":[[0,12]]},"1086":{"position":[[20,7]]},"1314":{"position":[[0,12]]}}}],["transfer）和编辑（edit",{"_index":154,"t":{"1458":{"position":[[14,23]]}}}],["transform",{"_index":76,"t":{"1076":{"position":[[47,13]]},"1086":{"position":[[28,11]]}}}],["tune",{"_index":80,"t":{"1082":{"position":[[5,6]]}}}],["u",{"_index":161,"t":{"1543":{"position":[[0,3]]}}}],["ui",{"_index":72,"t":{"1063":{"position":[[4,2]]},"1189":{"position":[[4,2]]}}}],["unit",{"_index":105,"t":{"1202":{"position":[[24,5]]}}}],["vae",{"_index":59,"t":{"1029":{"position":[[0,11]]}}}],["vae（2017",{"_index":138,"t":{"1392":{"position":[[3,9]]}}}],["valid",{"_index":117,"t":{"1225":{"position":[[21,11]]},"1227":{"position":[[22,10]]}}}],["visdom",{"_index":91,"t":{"1108":{"position":[[0,8]]},"1110":{"position":[[0,9]]},"1111":{"position":[[0,9]]}}}],["vq",{"_index":137,"t":{"1392":{"position":[[0,2]]}}}],["vqgan（2021",{"_index":141,"t":{"1396":{"position":[[0,11]]}}}],["vqvae",{"_index":139,"t":{"1394":{"position":[[0,5]]}}}],["web",{"_index":71,"t":{"1063":{"position":[[0,3]]},"1189":{"position":[[0,3]]}}}],["work",{"_index":82,"t":{"1084":{"position":[[14,5]]},"1227":{"position":[[33,5]]}}}],["yolo",{"_index":164,"t":{"1552":{"position":[[0,6]]}}}],["yolov1",{"_index":166,"t":{"1555":{"position":[[0,9]]}}}],["yolov2",{"_index":167,"t":{"1557":{"position":[[0,9]]}}}],["yolov5",{"_index":168,"t":{"1559":{"position":[[0,9]]}}}],["zip",{"_index":150,"t":{"1444":{"position":[[0,11]]}}}]],"pipeline":["stemmer"]}},{"documents":[{"i":800,"t":"设N是一个四位数，它的9倍恰好是其反序数（例如：1234的反序数是4321），求N的值","s":"反序输出","u":"/en/docs/Algorithms/题解/反序输出","p":800},{"i":806,"t":"饮水思源","s":"鸣谢","u":"/en/docs/Acknowledgement/intro","p":806},{"i":811,"t":"输入一个数，比如201，让数字随意组合，是否能组合出30的倍数，如果能够组合成30的倍数，就输出最大的倍数，不能就输出-1","s":"排列组合（求30的倍数）","u":"/en/docs/Algorithms/题解/排列组合（求30的倍数）","p":811},{"i":817,"t":"欢迎来到笔记本的算法部分","s":"Welcome","u":"/en/docs/Algorithms/intro","p":817},{"i":821,"t":"在一面很长的墙壁上，工人们用不同的油漆去刷墙，然而可能有些地方刷过以后觉得不好看，他们会重新刷一下。有些部分因为重复刷了很多次覆盖了很多层油漆，小诺很好奇那些地方被刷过多少种颜色的油漆。","s":"一维前缀和（刷出一道墙）","u":"/en/docs/Algorithms/题解/一维前缀和（刷出一道墙）","p":821},{"i":827,"t":"必须要知道的原理","s":"Linux 系统下 GeekOS 的环境配置","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","p":827},{"i":841,"t":"在上一篇博客中我们完成了GeekOS环境的配置，下面我们来验证环境配置的成功与否以及project 0的实现。","s":"GeekOS project 0的实现","u":"/en/docs/Curriculum/操作系统课设/GeekOS-project-0","p":841},{"i":847,"t":"第一章：前言","s":"编译原理笔记","u":"/en/docs/Curriculum/编译原理/Note","p":847},{"i":905,"t":"欢迎来到笔记本的课程学习部分","s":"Welcome","u":"/en/docs/Curriculum/intro","p":905},{"i":909,"t":"How to pronounce Adversarial?","s":"生成式对抗网络（GAN）","u":"/en/docs/Deep-Learning/大模型基础/GAN","p":909},{"i":926,"t":"论文：arXiv","s":"NeurIPS 2017: Attention Is All You Need","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","p":926},{"i":949,"t":"本笔记使用的教材是陈天华所著、清华大学出版社的《数字图像处理及应用：使用MATLAB分析与实现》。","s":"数字图像处理复习笔记","u":"/en/docs/Curriculum/数字图像处理/Note","p":949},{"i":1002,"t":"在以前的文章图像生成模型中已经大概介绍了目前SOTA的图像生成模型的共同点，并初步了解了Diffusion Model，在这篇文章中将详细讲解扩散模型的数学原理等。","s":"扩散模型（Diffusion Model）","u":"/en/docs/Deep-Learning/大模型基础/Diffusion-Model","p":1002},{"i":1010,"t":"回顾文字生成的两种方法","s":"图像生成模型","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","p":1010},{"i":1037,"t":"本篇论文主要基于 IPL 的思想实现。本仓库大部分从 IPL-Zero-Shot-Generative-Model-Adaptation fork 而来并做出了一定修改。","s":"本科毕业论文：基于 Prompt Learning 的视觉-语言大模型在图像生成中的应用与研究","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","p":1037},{"i":1072,"t":"在自监督学习的模型中，出现了很多以芝麻街任务命名的经典模型和论文。","s":"自监督学习（Self-Supervised Learning）","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","p":1072},{"i":1088,"t":"参考资料：Sampling for Text Generation","s":"生成模型中的采样技巧","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","p":1088},{"i":1106,"t":"对于TensorFlow框架，可以使用TensorBoard实现可视化。","s":"Visdom可视化","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","p":1106},{"i":1117,"t":"Transformer实战练习，代码见Github仓库。","s":"Speaker Classification","u":"/en/docs/Deep-Learning/代码实现/Speaker-Classification","p":1117},{"i":1125,"t":"参考链接:","s":"自注意力（Self-Attention）","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","p":1125},{"i":1155,"t":"输入：shape为$[5, 5, 3]$的图像","s":"深度可分离卷积","u":"/en/docs/Deep-Learning/基础知识/深度可分离卷积","p":1155},{"i":1163,"t":"本篇论文主要基于 IPL 的思想实现。本仓库大部分从 IPL-Zero-Shot-Generative-Model-Adaptation fork 而来并做出了一定修改。","s":"本科毕业论文：基于 Prompt Learning 的视觉-语言大模型在图像生成中的应用与研究","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","p":1163},{"i":1198,"t":"激活函数","s":"激活函数与Loss的梯度","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","p":1198},{"i":1209,"t":"正则化与权重衰退","s":"正则化与权重衰退","u":"/en/docs/Deep-Learning/基础知识/正则化与权重衰退","p":1209},{"i":1217,"t":"背景","s":"AlexNet","u":"/en/docs/Deep-Learning/基础知识/AlexNet","p":1217},{"i":1224,"t":"What is k-fold cross-validation?","s":"K-fold cross-validation","u":"/en/docs/Deep-Learning/基础知识/K-foldCross-validation","p":1224},{"i":1231,"t":"1x1 卷积","s":"卷积层","u":"/en/docs/Deep-Learning/基础知识/ConvolutionalLayer","p":1231},{"i":1236,"t":"卷积的诞生&核心特征","s":"从全连接到卷积","u":"/en/docs/Deep-Learning/基础知识/FromFullyConnectedLayerToConvolutionalLayer","p":1236},{"i":1243,"t":"什么是Logistic Regression","s":"关于Logistic Regression","u":"/en/docs/Deep-Learning/基础知识/LogisticRegression","p":1243},{"i":1252,"t":"卷积对像素位置信息是敏感的","s":"池化层","u":"/en/docs/Deep-Learning/基础知识/PoolingLayer","p":1252},{"i":1259,"t":"什么是感知机","s":"Perceptron","u":"/en/docs/Deep-Learning/基础知识/Perceptron","p":1259},{"i":1266,"t":"背景","s":"LeNet","u":"/en/docs/Deep-Learning/基础知识/LeNet","p":1266},{"i":1273,"t":"常用函数部分","s":"PyTorch基础","u":"/en/docs/Deep-Learning/基础知识/PytorchBasics","p":1273},{"i":1276,"t":"原文链接：https://arxiv.org/pdf/2406.11838","s":"Autoregressive Image Generation without Vector Quantization","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","p":1276},{"i":1298,"t":"论文：arXiv","s":"NeurIPS 2017: Attention Is All You Need","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","p":1298},{"i":1321,"t":"原文链接：https://arxiv.org/pdf/2112.10752","s":"CVPR 2022: High-Resolution Image Synthesis with Latent Diffusion Models","u":"/en/docs/Deep-Learning/论文笔记/Latent-Diffusion-Models","p":1321},{"i":1326,"t":"论文：arXiv","s":"NeurIPS 2020: Denoising Diffusion Probabilistic Models","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","p":1326},{"i":1337,"t":"原文链接：https://arxiv.org/pdf/2406.06525","s":"Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","p":1337},{"i":1348,"t":"论文：CVPR 2023 open access","s":"CVPR 2023: Zero-shot Generative Model Adaptation via Image-specific Prompt Learning","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","p":1348},{"i":1361,"t":"原文链接：https://arxiv.org/pdf/2406.11838","s":"自回归模型：MAR","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","p":1361},{"i":1383,"t":"自回归模型（Autoregressive Models）在图像生成领域扮演着重要角色，它们基于一个核心假设：当前像素值依赖于之前的像素值。这种依赖关系可以通过条件概率来表达，其中每一个像素的生成都是基于之前已经生成的像素的条件分布。在传统的视觉自回归图像生成任务中，这通常意味着从左到右和从上到下的顺序生成每个像素。","s":"图像生成：自回归模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","p":1383},{"i":1404,"t":"摘要","s":"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","p":1404},{"i":1418,"t":"原文链接：https://arxiv.org/pdf/2406.06525","s":"自回归模型：LlamaGen","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","p":1418},{"i":1429,"t":"图像生成基座模型","s":"图像生成和视频生成基座模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Image-and-Video-Generative-Foundation-Model","p":1429},{"i":1438,"t":"这里记录着在学习过程中发现的理解或操作方面出现的错误，温故知新。","s":"查漏补缺","u":"/en/docs/Deep-Learning/Fill-The-Gaps","p":1438},{"i":1448,"t":"参考资料：","s":"图像生成：扩散模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","p":1448},{"i":1460,"t":"欢迎来到笔记本的深度学习部分","s":"Welcome","u":"/en/docs/Deep-Learning/intro","p":1460},{"i":1464,"t":"按照官方文档将 Docusaurus 从 V2.4.3 升级至 V3.5.2，记录以下主要问题。","s":"更新至 Docusaurus V3","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","p":1464},{"i":1474,"t":"1. 告示栏的启用","s":"告示栏","u":"/en/docs/Others/博客搭建/announcement_bar","p":1474},{"i":1476,"t":"原文链接：https://arxiv.org/pdf/2404.02905","s":"自回归模型：VAR","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","p":1476},{"i":1493,"t":"欢迎来到笔记本的其他部分","s":"Welcome","u":"/en/docs/Others/intro","p":1493},{"i":1497,"t":"通过编写脚本函数的方式，手动开启代理","s":"终端代理","u":"/en/docs/Others/Linux/实用工具/终端代理","p":1497},{"i":1505,"t":"一、发生原因","s":"挂载Windows磁盘为只读文件","u":"/en/docs/Others/Linux/问题解决/双系统挂载Windows磁盘为只读文件","p":1505},{"i":1510,"t":"一、latte-dock","s":"如何让你的Kde Plasma看起来更像macOS","u":"/en/docs/Others/Linux/客制化/如何让你的KDE看起来更像macOS","p":1510},{"i":1515,"t":"树","s":"数据结构","u":"/en/docs/Tui-Mian/计算机基础综合/数据结构","p":1515},{"i":1520,"t":"大数除法是指被除数大小超出long long范围，而导致必须使用字符串存储的除法，属于简单模拟的范畴","s":"大数除法","u":"/en/docs/Tui-Mian/机试/大数除法","p":1520},{"i":1528,"t":"面试常考问题","s":"概率论","u":"/en/docs/Tui-Mian/数学/概率论","p":1528},{"i":1531,"t":"参考链接：","s":"线性代数","u":"/en/docs/Tui-Mian/数学/线性代数","p":1531},{"i":1537,"t":"一、线性代数","s":"夏令营面试数学部分复习","u":"/en/docs/Tui-Mian/数学/夏令营面试数学部分复习","p":1537},{"i":1542,"t":"一、U-2-Net","s":"简历面试准备","u":"/en/docs/Tui-Mian/简历/简历面试准备","p":1542},{"i":1584,"t":"欢迎来到笔记本的推免复习部分","s":"Welcome","u":"/en/docs/Tui-Mian/intro","p":1584},{"i":1588,"t":"前言","s":"经验贴：2023年双非计算机保研经历","u":"/en/docs/Tui-Mian/Summary","p":1588}],"index":{"version":"2.3.9","fields":["t"],"fieldVectors":[["t/800",[0,4.538]],["t/806",[1,0.663]],["t/811",[2,3.538,3,3.055]],["t/817",[1,0.663]],["t/821",[1,0.663]],["t/827",[1,0.663]],["t/841",[4,3.538,5,3.538]],["t/847",[1,0.663]],["t/905",[1,0.663]],["t/909",[6,3.538,7,3.538]],["t/926",[8,3.509]],["t/949",[9,4.538]],["t/1002",[10,3.538,11,2.498]],["t/1010",[1,0.663]],["t/1037",[1,0.403,11,0.838,12,1.79,13,1.024,14,1.024,15,0.917,16,1.024,17,1.024]],["t/1072",[1,0.663]],["t/1088",[15,2.242,18,2.9,19,2.9]],["t/1106",[20,4.538]],["t/1117",[21,4.538]],["t/1125",[1,0.663]],["t/1155",[22,2.9,23,2.9,24,2.9]],["t/1163",[1,0.403,11,0.838,12,1.79,13,1.024,14,1.024,15,0.917,16,1.024,17,1.024]],["t/1198",[1,0.663]],["t/1209",[1,0.663]],["t/1217",[1,0.663]],["t/1224",[25,2.456,26,2.456,27,2.456,28,2.456]],["t/1231",[1,0.517,29,3.538]],["t/1236",[1,0.663]],["t/1243",[30,3.538,31,3.538]],["t/1252",[1,0.663]],["t/1259",[1,0.663]],["t/1266",[1,0.663]],["t/1273",[1,0.663]],["t/1276",[32,3.918]],["t/1298",[8,3.509]],["t/1321",[33,4.538]],["t/1326",[8,3.509]],["t/1337",[34,3.918]],["t/1348",[35,2.456,36,2.456,37,2.456,38,2.456]],["t/1361",[32,3.918]],["t/1383",[11,2.498,39,3.538]],["t/1404",[1,0.663]],["t/1418",[34,3.918]],["t/1429",[1,0.663]],["t/1438",[1,0.663]],["t/1448",[1,0.663]],["t/1460",[1,0.663]],["t/1464",[1,0.565,40,1.881,41,1.881,42,1.881]],["t/1474",[1,0.517,3,3.055]],["t/1476",[43,4.538]],["t/1493",[1,0.663]],["t/1497",[1,0.663]],["t/1505",[1,0.663]],["t/1510",[44,3.538,45,3.538]],["t/1515",[1,0.663]],["t/1520",[46,4.948]],["t/1528",[1,0.663]],["t/1531",[1,0.663]],["t/1537",[1,0.663]],["t/1542",[47,2.9,48,2.9,49,2.9]],["t/1584",[1,0.663]],["t/1588",[1,0.663]]],"invertedIndex":[["",{"_index":1,"t":{"806":{"position":[[0,4]]},"817":{"position":[[0,12]]},"821":{"position":[[0,93]]},"827":{"position":[[0,8]]},"847":{"position":[[0,6]]},"905":{"position":[[0,14]]},"1010":{"position":[[0,11]]},"1037":{"position":[[0,8],[13,13],[74,11]]},"1072":{"position":[[0,33]]},"1125":{"position":[[0,5]]},"1163":{"position":[[0,8],[13,13],[74,11]]},"1198":{"position":[[0,4]]},"1209":{"position":[[0,8]]},"1217":{"position":[[0,2]]},"1231":{"position":[[4,2]]},"1236":{"position":[[0,10]]},"1252":{"position":[[0,13]]},"1259":{"position":[[0,6]]},"1266":{"position":[[0,2]]},"1273":{"position":[[0,6]]},"1404":{"position":[[0,2]]},"1429":{"position":[[0,8]]},"1438":{"position":[[0,32]]},"1448":{"position":[[0,5]]},"1460":{"position":[[0,14]]},"1464":{"position":[[0,7],[19,1],[28,3]]},"1474":{"position":[[3,6]]},"1493":{"position":[[0,12]]},"1497":{"position":[[0,18]]},"1505":{"position":[[0,6]]},"1515":{"position":[[0,1]]},"1528":{"position":[[0,6]]},"1531":{"position":[[0,5]]},"1537":{"position":[[0,6]]},"1584":{"position":[[0,14]]},"1588":{"position":[[0,2]]}}}],["0",{"_index":5,"t":{"841":{"position":[[50,5]]}}}],["1",{"_index":3,"t":{"811":{"position":[[60,1]]},"1474":{"position":[[0,2]]}}}],["1x1",{"_index":29,"t":{"1231":{"position":[[0,3]]}}}],["2",{"_index":48,"t":{"1542":{"position":[[4,1]]}}}],["201，让数字随意组合，是否能组合出30的倍数，如果能够组合成30",{"_index":2,"t":{"811":{"position":[[0,59]]}}}],["2023",{"_index":36,"t":{"1348":{"position":[[8,4]]}}}],["3",{"_index":24,"t":{"1155":{"position":[[17,6]]}}}],["5",{"_index":23,"t":{"1155":{"position":[[14,2]]}}}],["access",{"_index":38,"t":{"1348":{"position":[[18,6]]}}}],["adapt",{"_index":16,"t":{"1037":{"position":[[58,10]]},"1163":{"position":[[58,10]]}}}],["adversari",{"_index":7,"t":{"909":{"position":[[17,12]]}}}],["arxiv",{"_index":8,"t":{"926":{"position":[[0,8]]},"1298":{"position":[[0,8]]},"1326":{"position":[[0,8]]}}}],["autoregress",{"_index":39,"t":{"1383":{"position":[[0,20]]}}}],["cross",{"_index":27,"t":{"1224":{"position":[[15,5]]}}}],["cvpr",{"_index":35,"t":{"1348":{"position":[[0,7]]}}}],["dock",{"_index":45,"t":{"1510":{"position":[[8,4]]}}}],["docusauru",{"_index":40,"t":{"1464":{"position":[[8,10]]}}}],["fold",{"_index":26,"t":{"1224":{"position":[[10,4]]}}}],["fork",{"_index":17,"t":{"1037":{"position":[[69,4]]},"1163":{"position":[[69,4]]}}}],["geekos环境的配置，下面我们来验证环境配置的成功与否以及project",{"_index":4,"t":{"841":{"position":[[0,49]]}}}],["gener",{"_index":15,"t":{"1037":{"position":[[41,10]]},"1088":{"position":[[23,10]]},"1163":{"position":[[41,10]]}}}],["https://arxiv.org/pdf/2112.10752",{"_index":33,"t":{"1321":{"position":[[0,37]]}}}],["https://arxiv.org/pdf/2404.02905",{"_index":43,"t":{"1476":{"position":[[0,37]]}}}],["https://arxiv.org/pdf/2406.06525",{"_index":34,"t":{"1337":{"position":[[0,37]]},"1418":{"position":[[0,37]]}}}],["https://arxiv.org/pdf/2406.11838",{"_index":32,"t":{"1276":{"position":[[0,37]]},"1361":{"position":[[0,37]]}}}],["ipl",{"_index":12,"t":{"1037":{"position":[[9,3],[27,3]]},"1163":{"position":[[9,3],[27,3]]}}}],["k",{"_index":25,"t":{"1224":{"position":[[8,1]]}}}],["latt",{"_index":44,"t":{"1510":{"position":[[0,7]]}}}],["logist",{"_index":30,"t":{"1243":{"position":[[0,11]]}}}],["long",{"_index":46,"t":{"1520":{"position":[[0,17],[18,32]]}}}],["matlab",{"_index":9,"t":{"949":{"position":[[0,49]]}}}],["model",{"_index":11,"t":{"1002":{"position":[[54,28]]},"1037":{"position":[[52,5]]},"1163":{"position":[[52,5]]},"1383":{"position":[[21,137]]}}}],["net",{"_index":49,"t":{"1542":{"position":[[6,3]]}}}],["n是一个四位数，它的9倍恰好是其反序数（例如：1234的反序数是4321），求n",{"_index":0,"t":{"800":{"position":[[0,43]]}}}],["open",{"_index":37,"t":{"1348":{"position":[[13,4]]}}}],["pronounc",{"_index":6,"t":{"909":{"position":[[7,9]]}}}],["regress",{"_index":31,"t":{"1243":{"position":[[12,10]]}}}],["sampl",{"_index":18,"t":{"1088":{"position":[[0,13]]}}}],["shape为$[5",{"_index":22,"t":{"1155":{"position":[[0,13]]}}}],["shot",{"_index":14,"t":{"1037":{"position":[[36,4]]},"1163":{"position":[[36,4]]}}}],["sota的图像生成模型的共同点，并初步了解了diffus",{"_index":10,"t":{"1002":{"position":[[0,53]]}}}],["tensorflow框架，可以使用tensorboard",{"_index":20,"t":{"1106":{"position":[[0,36]]}}}],["text",{"_index":19,"t":{"1088":{"position":[[18,4]]}}}],["transformer实战练习，代码见github",{"_index":21,"t":{"1117":{"position":[[0,28]]}}}],["u",{"_index":47,"t":{"1542":{"position":[[0,3]]}}}],["v2.4.3",{"_index":41,"t":{"1464":{"position":[[21,6]]}}}],["v3.5.2",{"_index":42,"t":{"1464":{"position":[[32,16]]}}}],["valid",{"_index":28,"t":{"1224":{"position":[[21,11]]}}}],["zero",{"_index":13,"t":{"1037":{"position":[[31,4]]},"1163":{"position":[[31,4]]}}}]],"pipeline":["stemmer"]}},{"documents":[],"index":{"version":"2.3.9","fields":["t"],"fieldVectors":[],"invertedIndex":[],"pipeline":["stemmer"]}},{"documents":[{"i":801,"t":"tip 设N是一个四位数，它的9倍恰好是其反序数（例如：1234的反序数是4321），求N的值","s":"反序输出","u":"/en/docs/Algorithms/题解/反序输出","h":"","p":800},{"i":803,"t":"#include <bits/stdc++.h> using namespace std; int main() { for (int i = 1000; i <= 9999; i++) { int x = i * 9, y = 0; while (x > 0) { y = y * 10 + x % 10; x /= 10; } if (i == y) { cout << i << endl; } } return 0; }","s":"参考代码","u":"/en/docs/Algorithms/题解/反序输出","h":"#参考代码","p":800},{"i":805,"t":"反序输出可以分为两部分：拆分以及反序拼接 拆分：n位整数求余10可以得到最后一位，再除以10可以得到除去上述最后一位之后的n-1位整数，循环得到每一个最后一位，完成拆分 while (x > 0) { y = y * 10 + x % 10; // 拼接与拆分 x /= 10; } 拼接：将s中的数字拼接成整数 int sum = 0; for (int i = 0; i < s.size(); i++) { sum = sum * 10 + s[i]; }","s":"题解","u":"/en/docs/Algorithms/题解/反序输出","h":"#题解","p":800},{"i":808,"t":"该网站的搭建离不开 Docusaurus 的支持以及对 Sonder的宝藏笔记本 的参考。","s":"饮水思源","u":"/en/docs/Acknowledgement/intro","h":"#饮水思源","p":806},{"i":810,"t":"本网站展示的所有标识和链接仅属于个人喜好，不代表国家的立场或企业、组织的行为。 本网站的所有信息仅供参考，不构成法律或商业建议。","s":"Disclaimer","u":"/en/docs/Acknowledgement/intro","h":"#disclaimer","p":806},{"i":812,"t":"tip 输入一个数，比如201，让数字随意组合，是否能组合出30的倍数，如果能够组合成30的倍数，就输出最大的倍数，不能就输出-1 例如输入201可以随意组合成 201，210，012，021，102，120等数字 其中120，210都是30的倍数，由于要找最大的，所以答案是210 输入样例：201 输出样例：210","s":"排列组合（求30的倍数）","u":"/en/docs/Algorithms/题解/排列组合（求30的倍数）","h":"","p":811},{"i":814,"t":"#include <bits/stdc++.h> using namespace std; int main() { string s; cin >> s; int maxx = 0, flag = 0; sort(s.begin(), s.end()); do { int now = 0; for (int i = 0; i < s.size(); i++) { now = now * 10 + s[i] - '0'; } if (now % 30 == 0) { flag = 1; maxx = max(maxx, now); } } while (next_permutation(s.begin(), s.end())); if (flag == 1) { cout << maxx << endl; return 0; } else { cout << -1 << endl; } }","s":"参考代码","u":"/en/docs/Algorithms/题解/排列组合（求30的倍数）","h":"#参考代码","p":811},{"i":816,"t":"使用C++ STL提供的排列组合模版 首先将代排列组合的字符串或数组进行排序 sort(list.begin(), list.end()); 使用排列组合模版 do { something(); } while (next_permutation(list.begin(), list.end())); 此时，在每一个do循环中，list按从小到大的顺序进行排列组合遍历","s":"题解","u":"/en/docs/Algorithms/题解/排列组合（求30的倍数）","h":"#题解","p":811},{"i":818,"t":"tip 欢迎来到笔记本的算法部分","s":"Welcome","u":"/en/docs/Algorithms/intro","h":"","p":817},{"i":820,"t":"如果可以帮到你的话就给个免费的 Star 吧！","s":"支持我！","u":"/en/docs/Algorithms/intro","h":"#支持我","p":817},{"i":822,"t":"tip 在一面很长的墙壁上，工人们用不同的油漆去刷墙，然而可能有些地方刷过以后觉得不好看，他们会重新刷一下。有些部分因为重复刷了很多次覆盖了很多层油漆，小诺很好奇那些地方被刷过多少种颜色的油漆。 输入描述： 若干行输入，每行两个数字B[i],E[i](0<=B[i]<=E[i]<=200000)表示这次刷的墙壁是哪一段 （假设每次刷的时候油漆颜色都和之前的不同），以0 0结束 又若干行输入，每行两个数字begin[i],end[i]（0<=begin[i]<=end[i]<=200000）表示小诺询问的段， 以0 0结束 输出描述: 对于每个小诺的询问输出(end[i]-begin[i]+1)行,表示对应询问段的每个点被多少种颜色的油漆覆盖过。","s":"一维前缀和（刷出一道墙）","u":"/en/docs/Algorithms/题解/一维前缀和（刷出一道墙）","h":"","p":821},{"i":824,"t":"#include <bits/stdc++.h> using namespace std; int main() { vector<int> colors(200001, 0); int B, E; while (scanf(\"%d %d\", &B, &E)) { if (B == 0 && E == 0) { break; } colors[B]++; // 刷墙起点标记 colors[E + 1]--; // 刷墙终点标记 } // 计算前缀和 for (int i = 1; i < colors.size(); i++) { colors[i] += colors[i - 1]; } int begin, end; while (scanf(\"%d %d\", &begin, &end)) { if (begin == 0 && end == 0) { break; } for (int i = begin; i <= end; i++) { printf(\"%d\\n\", colors[i]); } } return 0; }","s":"参考代码","u":"/en/docs/Algorithms/题解/一维前缀和（刷出一道墙）","h":"#参考代码","p":821},{"i":826,"t":"使用前缀和思想简化时间复杂度，设计前缀和数组，使输出的数组中元素的值代表其对应节点被刷的次数。 首先初始化前缀和数组，使每一个元素等于为0。 该题的巧妙之处就在于：对于每一个输入的索引B与E，B作为开始刷的节点索引令前缀和数组中对应元素的值+1+1+1，E+1作为刷墙结束的下一个节点的索引令对应的值−1-1−1。这样在所有输入结束后的计算前缀和阶段，在每一个值为[1,−1)[1, -1)[1,−1)的索引区间中的元素值都会加1，而对于某次刷漆终点E的下一个索引为E+1的元素值由于−1-1−1而抵消影响（自身值为−1-1−1加上之前元素所累积的1而归零），此时数组中元素的值才代表其对应节点被刷的次数。 关于超时，可以在函数中加入以下代码消除流操作的缓冲区，并使用\"\\n\"代替endl。 ios::sync_with_stdio(false);","s":"题解","u":"/en/docs/Algorithms/题解/一维前缀和（刷出一道墙）","h":"#题解","p":821},{"i":830,"t":"GeekOS是一个基于x86体系结构的微操作系统内核. 由美国马理兰大学的教师开发, 主要用于操作系统课程设计的教育. 出于教学目的, 这个系统内核设计简单, 却又兼备实用性, 它可以运行在真正的X86 PC硬件平台. 在下载好GeekOS后, 在geekos-version/src/目录下会存在project0-project6这7个文件夹, 分别代表GeekOS设计的7个学习任务. 在环境搭建完成之后, 我们进行的每一个项目的代码编写几乎都在geekos-version/src/projecti/src/geekos/文件夹下, 每一个项目的编译都在geekos-version/src/projecti/build文件夹下进行, 即要在终端中通过cd进入该目录, 再执行make depend和make命令.","s":"GeekOS:","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#geekos","p":827},{"i":832,"t":"bochs是一个x86硬件平台的模拟器. GeekOS运行依托于bochs. 在安装好Linux操作系统后需要安装bochs以及nasm, 以完成GeekOS环境的搭建.","s":"bochs:","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#bochs","p":827},{"i":834,"t":"GeekOS的开发环境可分为两部分, 一部分是编译环境, 一部分是运行环境. 在编译过程中, 使用Linux自带的编译环境以及编译命令对特定的GeekOS project进行编译即可. 首先在终端中通过cd命令进入geekos-version/src/projecti/build目录, 再执行make depend和make命令. 编译后生成bochs的镜像文件fd.img, 这是bochs运行所必须的文件,也是GeekOS运行环境的前置配置.","s":"二者之间的关系","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#二者之间的关系","p":827},{"i":836,"t":"安装其实非常简单, 这里主要花篇幅介绍安装后解决报错的配置.","s":"安装与配置","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#安装与配置","p":827},{"i":838,"t":"需要下载GeekOS Files, 安装bochs, nasm等. GeekOS直接下载压缩包, 解压即可. arch系用户通过以下命令即可完成bochs和nasm的安装. yay -S bochs nasm 其他发行版的安装方法这里不再赘述, 可选择从群文件里下载源文件并编译安装, 师兄师姐也在群文件里给了一些教程指导.","s":"安装","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#安装","p":827},{"i":840,"t":"完成安装后, 我们就可以开始对project0中的代码进行完善了, 并在geekos-version/src/project0/build目录下执行make depend以及make命令, 目的是编译project0的代码, 生成bochs的镜像文件fd.img以构建GeekOS的运行环境. 但很多报错就是在make这一步产生的, 因此在安装完成后还需要进行配置. 配置分为两部分, 一个是对GeekOS中makefile的修改, 另一部分是对bochs的配置文件的修改. GeekOS中makefile的配置​ 综合网上很多师兄师姐的博客，这三个错误应该是每个人都会遇到的，所以当你不确定自己能不能运行时，请全部完成这三个步骤. 问题: warnings being treated as errors 解决方案: 修改geekos-version/src/projecti/build目录下的makefie文件(由于每个project下都存在一个对应的makefile文件, 所以在每个项目编译前都要修改一次) // 修改第149行： CC_GENERAL_OPTS := $(GENERAL_OPTS) -Werror // 修改后： CC_GENERAL_OPTS := $(GENERAL_OPTS) 问题: X86_64与i386输出不兼容 解决方案: 修改geekos-version/src/projecti/build目录下的makefie文件 # Target C compiler. gcc 2.95.2 or later should work. 100行 TARGET_CC := $(TARGET_CC_PREFIX)gcc -m32 # Host C compiler. This is used to compile programs to execute on # the host platform, not the target (x86) platform. On x86/ELF # systems, such as Linux and FreeBSD, it can generally be the same # as the target C compiler. 106行 HOST_CC := gcc -m32 # Target linker. GNU ld is probably to only one that will work.109行 TARGET_LD := $(TARGET_CC_PREFIX)ld -m elf_i386 问题: undefined reference to '__stack_chk_fail' 解决方案: 修改geekos-version/src/projecti/build目录下的makefie文件 # Flags used for all C source files // 修改前：148行 GENERAL_OPTS := -O -Wall $(EXTRA_C_OPTS) // 修改后： GENERAL_OPTS := -O -Wall -fno-stack-protector $(EXTRA_C_OPTS) bochs配置文件的修改​ 在geekos-version/src/projecti/build目录下创建.bochsrc文件 # An example .bochsrc file. # You will need to edit these lines to reflect your system. vgaromimage: file=/usr/local/share/bochs/VGABIOS-lgpl-latest # 请根据自己的实际安装路径更改 romimage: file=/usr/local/share/bochs/BIOS-bochs-latest # 请根据自己的实际安装路径更改 megs: 8 boot: a floppya: 1_44=fd.img, status=inserted #floppya: 1_44=fd_aug.img, status=inserted log: ./bochs.out # keyboard_serial_delay: 200 # vga_update_interval: 300000 mouse: enabled=0 private_colormap: enabled=0 # i440fxsupport: enabled=0 # Uncomment this to write all bochs debugging messages to # bochs.out. This produces a lot of output, but can be very # useful for debugging the kernel. #debug: action=report 到此为止, 所有的配置工作已经完成, 可以正常的进行下一步的代码完善. 如果需要验证自己是否配置成功, 可以参照下一篇博客GeekOS project 0的实现, 在本篇博客中会有完整的C语言代码编写以及编译、使用bochs执行的过程.","s":"配置","u":"/en/docs/Curriculum/操作系统课设/Environment-Configuration","h":"#配置","p":827},{"i":842,"t":"在上一篇博客中我们完成了GeekOS环境的配置，下面我们来验证环境配置的成功与否以及project 0的实现。","s":"GeekOS project 0的实现","u":"/en/docs/Curriculum/操作系统课设/GeekOS-project-0","h":"","p":841},{"i":844,"t":"编写geekos-version/src/projecti/src/geekos/main.c文件 编写函数project0实现检测键盘输入Ctrl+d结束线程。 void project0(){ Print(\"To Exit hit Ctrl + d.\\n\"); Keycode keycode; while(1) { if(Read_Key(&keycode)) { if(!((keycode & KEY_SPECIAL_FLAG) || (keycode & KEY_RELEASE_FLAG)))// 不是特殊键或者弹起 { int asciiCode = keycode & 0xff;//d if((keycode & KEY_CTRL_FLAG)==KEY_CTRL_FLAG && asciiCode=='d')//ctrl+d { Print(\"\\n---------Adios!---------\\n\"); # 这里需要注意素质 Exit(1); }else { Print(\"%c\",(asciiCode=='\\r') ? '\\n' : asciiCode); } } } } } 在main函数中添加以下代码，实现自定义函数的调用，创建线程。 struct Kernel_Thread *thread; thread = Start_Kernel_Thread(&project0,0,PRIORITY_NORMAL,false); 总体代码 /* * GeekOS C code entry point * Copyright (c) 2001,2003,2004 David H. Hovemeyer <daveho@cs.umd.edu> * Copyright (c) 2003, Jeffrey K. Hollingsworth <hollings@cs.umd.edu> * Copyright (c) 2004, Iulian Neamtiu <neamtiu@cs.umd.edu> * $Revision: 1.51 $ * * This is free software. You are permitted to use, * redistribute, and modify it as specified in the file \"COPYING\". */ #include <geekos/bootinfo.h> #include <geekos/string.h> #include <geekos/screen.h> #include <geekos/mem.h> #include <geekos/crc32.h> #include <geekos/tss.h> #include <geekos/int.h> #include <geekos/kthread.h> #include <geekos/trap.h> #include <geekos/timer.h> #include <geekos/keyboard.h> void project0(){ Print(\"To Exit hit Ctrl + d.\\n\"); Keycode keycode; while(1) { if(Read_Key(&keycode)) { if(!((keycode & KEY_SPECIAL_FLAG) || (keycode & KEY_RELEASE_FLAG)))// 不是特殊键或者弹起 { int asciiCode = keycode & 0xff;//d if((keycode & KEY_CTRL_FLAG)==KEY_CTRL_FLAG && asciiCode=='d')//ctrl+d { Print(\"\\n---------Adios! Motherfucker!---------\\n\"); Exit(1); }else { Print(\"%c\",(asciiCode=='\\r') ? '\\n' : asciiCode); } } } } } /* * Kernel C code entry point. * Initializes kernel subsystems, mounts filesystems, * and spawns init process. */ void Main(struct Boot_Info* bootInfo) { Init_BSS(); Init_Screen(); Init_Mem(bootInfo); Init_CRC32(); Init_TSS(); Init_Interrupts(); Init_Scheduler(); Init_Traps(); Init_Timer(); Init_Keyboard(); Set_Current_Attr(ATTRIB(BLACK, GREEN|BRIGHT)); Print(\"Welcome to GeekOS!\\n\"); Set_Current_Attr(ATTRIB(BLACK, GRAY)); // TODO(\"Start a kernel thread to echo pressed keys and print counts\"); struct Kernel_Thread *thread; thread = Start_Kernel_Thread(&project0,0,PRIORITY_NORMAL,false); /* Now this thread is done. */ Exit(0); }","s":"编写C语言代码","u":"/en/docs/Curriculum/操作系统课设/GeekOS-project-0","h":"#编写c语言代码","p":841},{"i":846,"t":"每一个项目的编译都在geekos-version/src/projecti/build文件夹下进行，即要在终端中通过cd进入该目录。 执行 make depend make 此时，该目录下会生成bochs.out、depend.mak以及fd.img文件，bochs.out文件是日志输出文件，depend.mak是编译中间生成的文件，最终生成的fd.img是最重要的GeekOS映像文件，有了它才能使用bochs运行GeekOS操作系统。感恩它！ 目录下的文件应该是这样的结构： 下面就可以使用bochs运行GeekOS系统了，可以说bochs的运行依赖两个文件，一个是配置文件.bochsrc，一个是映像文件fd.img，映像文件的加载路径需要在.bochsrc文件中定义，在环境配置的博客中已经介绍过了。这里再贴一下内容。 # An example .bochsrc file. # You will need to edit these lines to reflect your system. vgaromimage: file=/usr/local/share/bochs/VGABIOS-lgpl-latest # 请根据自己的实际安装路径更改 romimage: file=/usr/local/share/bochs/BIOS-bochs-latest # 请根据自己的实际安装路径更改 megs: 8 boot: a floppya: 1_44=fd.img, status=inserted #floppya: 1_44=fd_aug.img, status=inserted log: ./bochs.out # keyboard_serial_delay: 200 # vga_update_interval: 300000 mouse: enabled=0 private_colormap: enabled=0 # i440fxsupport: enabled=0 # Uncomment this to write all bochs debugging messages to # bochs.out. This produces a lot of output, but can be very # useful for debugging the kernel. #debug: action=report 在这个目录下打开终端，执行 bochs 选择6，按下回车 可能会出现黑屏情况，这是因为进入了调试模式，终端正在等待命令，在终端输入 c 即可完成bochs的正式启动，最终的效果","s":"使用Linux的编译系统对C语言代码进行编译","u":"/en/docs/Curriculum/操作系统课设/GeekOS-project-0","h":"#使用linux的编译系统对c语言代码进行编译","p":841},{"i":850,"t":"词法分析：分析输入串如何构成句子，得到单词序列 语法分析：分析单词序列如何构成程序，构造语法分析树 语义分析：审查语义错误，为代码生成收集类型信息 中间代码生成 代码优化 目标代码生成 表管理、错误检查和处理贯穿整个过程","s":"1.1 编译程序的逻辑结构","u":"/en/docs/Curriculum/编译原理/Note","h":"#11-编译程序的逻辑结构","p":847},{"i":852,"t":"前端是指与源语言有关、与目标机无关的部分 如词法分析、语法分析、语义分析、中间代码生成、代码优化中与机器无关的部分 后端是指与目标机有关的部分 如代码优化中与机器有关的部分、目标代码的生成","s":"1.2 前端和后端","u":"/en/docs/Curriculum/编译原理/Note","h":"#12-前端和后端","p":847},{"i":854,"t":"遍是指从头到尾扫描一遍源程序","s":"1.3 遍的概念","u":"/en/docs/Curriculum/编译原理/Note","h":"#13-遍的概念","p":847},{"i":857,"t":"若从文法的开始符号开始存在以下推导，则称α\\alphaα为该文法的一个句型，句型中既可以包含终结符，也可以包含非终结符，也可以是空串 S⇒∗α, α∈(VT∪VN)∗(1) S \\Rightarrow^* \\alpha,\\space \\alpha \\in (V_T \\cup V_N)^* \\tag{1}S⇒∗α, α∈(VT​∪VN​)∗(1)","s":"2.1 句型","u":"/en/docs/Curriculum/编译原理/Note","h":"#21-句型","p":847},{"i":859,"t":"S⇒∗β, β∈VT∗(2)S \\Rightarrow^* \\beta,\\space \\beta \\in V_T^* \\tag{2}S⇒∗β, β∈VT∗​(2) 则称β\\betaβ是该文法的句子","s":"2.2 句子：","u":"/en/docs/Curriculum/编译原理/Note","h":"#22-句子","p":847},{"i":861,"t":"0型文法，又称无限制文法、短语文法 1型文法，又称文有关文法 2型文法，又称上下文无关文法（Context-Free Grammar，CFG） 可用来构建语法树，语法树是上下文无关文法推导和规约的图形化表示 A→β, A∈VN, β∈(VT∪VN)∗(3)\\Alpha \\rightarrow \\beta,\\space \\Alpha \\in V_N, \\space \\beta \\in (V_T \\cup V_N)^* \\tag{3}A→β, A∈VN​, β∈(VT​∪VN​)∗(3) 3型文法，又称正规文法（Regular Grammar，RG） 左线性文法 右线性文法","s":"2.3 文法的分类：","u":"/en/docs/Curriculum/编译原理/Note","h":"#23-文法的分类","p":847},{"i":863,"t":"如果在推导的任何一步都是对产生式左部中的最左/右非终结符进行替换，则称为最左/右推导，其中最右推导也被成为规范推导","s":"2.4 最左/右推导：","u":"/en/docs/Curriculum/编译原理/Note","h":"#24-最左右推导","p":847},{"i":867,"t":"确定的有穷自动机（DFA） DFA的定义及组成 确定的含义：在状态转换的每一步，FA根据当前的状态及扫描的输入字符，便能唯一地知道FA的下一状态。 tip 在状态转换图中的直观体现就是，在确定行表示的当前状态以及列确定的路径后，得到的目的状态不会是元素个数大于1的集合。 DFA的可接受以及接受集的定义：从开始状态开始，经过该符号串表示的路径，若能到达终态则称该符号串可被改DFA接受。 不确定的有穷自动机（NFA） NFA的确定化，即将NFA转换为DFA（子集法） 步骤： 画出DFA转换表 tip 转换表中在状态一列中，状态包含原NFA终态的集合要标*，代表其为等价DFA的终态 计算move(T,a)move(T, a)move(T,a) 计算ϵ−closure(T)\\epsilon -closure(T)ϵ−closure(T) 为转换表中的状态重命名 确定初态和终态 DFA的最小化（分割法） 步骤如下： tip 考试时注意过程怎么写，下面使用需要三轮分割的列子演示步骤 在分割完成后，对可以化简的集合选出一个状态作为代表，删除其他多余状态，重新画图","s":"3.2 有穷自动机（FA）","u":"/en/docs/Curriculum/编译原理/Note","h":"#32-有穷自动机fa","p":847},{"i":871,"t":"描述程序语法结构的规则可以使用2型文法（上下文无关语法，CFG） 语法分析方法包含确定的和不确定的分析方法，确定的语法分析方法根据输入符号，唯一选择产生式 确定的自顶向下分析方法：根据当前的输入符号唯一地确定选用哪个产生式替换相应的非终结符以往下推导","s":"第四章：自顶向下语法分析方法","u":"/en/docs/Curriculum/编译原理/Note","h":"#第四章自顶向下语法分析方法","p":847},{"i":874,"t":"tip FOLLOW集的求法可以按照下图技巧进行 若要求的非终结符是开始符号，则直接将#插入FOLLOW集中 在所有产生式的右部中找到要求的非终结符 看非终结符的右侧是什么元素 若无元素，则直接将该产生式左部的FOLLOW集加入到该非终结符的FOLLOW集中 若为终结符，直接将该终结符加入到FOLLOW集中 若为非终结符，将FIRST(该非终结符)减去ϵ\\epsilonϵ的所有终结符元素都加入至FOLLOW集中","s":"2. Follow集的定义","u":"/en/docs/Curriculum/编译原理/Note","h":"#2-follow集的定义","p":847},{"i":876,"t":"tip 需要注意的是FIRST集、FOLLOW集是针对于符号串而言的，而SELECT集是针对于产生式而言的","s":"3. SELECT集的定义","u":"/en/docs/Curriculum/编译原理/Note","h":"#3-select集的定义","p":847},{"i":879,"t":"tip 考试时注意书写过程，需要画出以下两张表","s":"5. LL(1)文法的判别","u":"/en/docs/Curriculum/编译原理/Note","h":"#5-ll1文法的判别","p":847},{"i":881,"t":"预测分析表通过计算SELECT集得到，形如下表 行标为各非终结符，列标为输入符号，若从某一非终结符开始的产生式的SELECT集包含某一输入符号，则对应产生式就是行列确定的元素值。","s":"6. 预测分析表","u":"/en/docs/Curriculum/编译原理/Note","h":"#6-预测分析表","p":847},{"i":883,"t":"消除左公因子（回溯） caution 同一非终结符的多个产生式存在共同前缀，会导致回溯现象，需要消除 消除左递归 caution 左递归文法会使递归下降分析器陷入无限循环 消除直接左递归 消除间接左递归 通过代入法变成直接左递归再消除","s":"7. 非LL(1)文法到LL(1)文法的等价变换","u":"/en/docs/Curriculum/编译原理/Note","h":"#7-非ll1文法到ll1文法的等价变换","p":847},{"i":886,"t":"从的底部向顶部的方向构造语法分析树，采用最左归约的方式，即最右推导的逆过程 tip 注意辨别：自顶向下的语法分析采用最左推导的方式 最右推导是规范推导，最左归约是最右推导的逆过程，又称规范归约","s":"5.1 概念","u":"/en/docs/Curriculum/编译原理/Note","h":"#51-概念","p":847},{"i":888,"t":"算符优先分析法 按照算符的优先关系和结合性质进行语法分析 LR分析法（重点） 规范规约：句柄作为可归约串","s":"5.2 方法","u":"/en/docs/Curriculum/编译原理/Note","h":"#52-方法","p":847},{"i":891,"t":"移入：将下一个输入符号移到栈顶 归约：被归约的符号串的右端处于栈顶，语法分析器在栈中确定这个串的左端非终结符来替换该串 接受：宣布语法分析过程成功完成 报错：发现一个语法错误，并调用错误恢复子程序","s":"5.4 移入-归约分析器的4种动作","u":"/en/docs/Curriculum/编译原理/Note","h":"#54-移入-归约分析器的4种动作","p":847},{"i":893,"t":"前导知识：4种项目状态 归约项目：·在最后 接受项目：拓广文法的开始符号的产生式，且·在最后 移进项目：·后面是终结符VTV_TVT​ 待约项目：·后面是非终结符VNV_NVN​ 移入-归约分析 LR(0)分析表 / 构造其识别活前缀DFA https://www.bilibili.com/video/BV1pL4y1E7RE/?spm_id_from=333.788&vd_source=24d8fcf68bc0e2b0003defe0995cf533 在写预测分析表的reduce项时，action的每一列都要写 SLR(1)分析表 / 构造其识别活前缀DFA https://www.bilibili.com/video/BV12u411S7Us/?spm_id_from=333.788&vd_source=24d8fcf68bc0e2b0003defe0995cf533 在写预测分析表的reduce项时，只写产生式左部的FOLLOW集对应的action列 LR(1)分析表 / 构造其识别活前缀DFA https://www.bilibili.com/video/BV1Vm4y1Q7XB/?spm_id_from=333.788&vd_source=24d8fcf68bc0e2b0003defe0995cf533 在构造项目集时，要加入前向搜索符；并且，在写预测分析表的reduce项时只写前向搜索符对应的action列 LALR(1)分析表 / 构造其识别活前缀DFA 在构造项目集时，要加入前向搜索符，但是要合并同心集，把相同表达式但是不同前向搜索符的前向搜索符合并，并且在写预测分析表的reduce项时只写前向搜索符集对应的action列 https://www.bilibili.com/video/BV13r4y1m7sQ/?spm_id_from=333.788&vd_source=24d8fcf68bc0e2b0003defe0995cf533","s":"5.5 重要题型","u":"/en/docs/Curriculum/编译原理/Note","h":"#55-重要题型","p":847},{"i":896,"t":"词法分析：从左到右扫描源程序，识别出各个单词，确定单词类型并形成单词序列，进行词法错误检查，对标识符进行登记，即符号表管理 语法分析：从词法分析输出的单词序列识别出各类短语，构造语法分析树，并进行语法错误检查 语义分析：审查程序是否具有语义错误，为代码生成阶段收集类型信息，不符合规范时报错（符号表是语义正确性检查的依据） 中间代码生成：生成中间代码，如三地址指令、四元式、波兰式、逆波兰式、树形结构等 代码优化：对代码进行等价变换以求提高执行效率，提高速度或节省空间 目标代码生成：将中间代码转化成目标机上的机器指令代码或汇编代码（符号表是对符号分配地址的依据）","s":"1 编译程序各阶段功能","u":"/en/docs/Curriculum/编译原理/Note","h":"#1-编译程序各阶段功能","p":847},{"i":898,"t":"就产生语法树的方向而言，可大致分为自顶向下的语法分析和自底向上的语法分析两大类。 自顶向下的语法分析方法：主流方法为递归下降分析法。根据当前的输入符号唯一地确定选用哪个产生式替换相应的非终结符以往下推导。 自底向上的语法分析方法：将输入串w归约为文法开始符号S的过程。 tip LR(0), SLR(1), LR(1) LR(0)文法可能存在移进-归约冲突、归约-归约冲突 SLR(1)文法在构造的过程中不存在归约-归约冲突，但有可能出现移进-归约冲突，可以由FOLLOW集解决的话则是SLR(1)文法 tip 3 翻译模式​ 翻译模式是适合语法制导语义计算的另一种描述形式，可以体现一种合理调用语义动作的算法。 S-翻译模式： 仅涉及综合属性的翻译模式，通常将语义动作集合置于产生式右端末尾。 L-翻译模式： 既可以包含综合属性，也可以包含继承属性。 4 属性文法​ 在文法基础上，为文法符号关联有特定意义的属性，并为产生式关联相应的语义动作，称之为属性文法。 S-属性文法： 只包含综合属性的属性文法成为S-属性文法 L-属性文法： 可以包含综合属性，也可以包含继承属性，但要求产生式右部的文法符号的继承属性的计算只取决于该符号左边符号的属性 5 符号表​ 符号表是编译程序中用于收集标识符的属性信息的数据结构。 各阶段作用： 语义分析阶段：语义合法性检查的依据 目标代码生成阶段：对符号名进行地址分配的依据","s":"2 语法分析方法的概念","u":"/en/docs/Curriculum/编译原理/Note","h":"#2-语法分析方法的概念","p":847},{"i":900,"t":"翻译模式是适合语法制导语义计算的另一种描述形式，可以体现一种合理调用语义动作的算法。 S-翻译模式： 仅涉及综合属性的翻译模式，通常将语义动作集合置于产生式右端末尾。 L-翻译模式： 既可以包含综合属性，也可以包含继承属性。","s":"3 翻译模式","u":"/en/docs/Curriculum/编译原理/Note","h":"#3-翻译模式","p":847},{"i":902,"t":"在文法基础上，为文法符号关联有特定意义的属性，并为产生式关联相应的语义动作，称之为属性文法。 S-属性文法： 只包含综合属性的属性文法成为S-属性文法 L-属性文法： 可以包含综合属性，也可以包含继承属性，但要求产生式右部的文法符号的继承属性的计算只取决于该符号左边符号的属性","s":"4 属性文法","u":"/en/docs/Curriculum/编译原理/Note","h":"#4-属性文法","p":847},{"i":904,"t":"符号表是编译程序中用于收集标识符的属性信息的数据结构。 各阶段作用： 语义分析阶段：语义合法性检查的依据 目标代码生成阶段：对符号名进行地址分配的依据","s":"5 符号表","u":"/en/docs/Curriculum/编译原理/Note","h":"#5-符号表","p":847},{"i":906,"t":"tip 欢迎来到笔记本的课程学习部分","s":"Welcome","u":"/en/docs/Curriculum/intro","h":"","p":905},{"i":908,"t":"如果可以帮到你的话就给个免费的 Star 吧！","s":"支持我！","u":"/en/docs/Curriculum/intro","h":"#支持我","p":905},{"i":910,"t":"正确发音 How to pronounce Adversarial? /ˌædvərˈseriəl/","s":"生成式对抗网络（GAN）","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"","p":909},{"i":913,"t":"生成器（Generator）通常接收一个来自潜在空间（latent space）的随机向量作为输入。这个潜在空间通常是一个随机分布，比如均匀分布或正态分布。生成器的任务是将这个随机向量映射成与训练数据相似的样本。","s":"将随机分布作为输入","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#将随机分布作为输入","p":909},{"i":915,"t":"当需要解决的任务需要富有“创造力”时，即根据不同的输入，可以产生多个不一样且正确的输出时。这样的设计使得生成器能够生成多样性的样本，因为每个不同的随机向量都可能导致生成器输出不同的样本。在训练过程中，通过不断调整生成器的参数，使得生成器的输出在数据分布中更难以被判别器区分。","s":"为什么要添加分布","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#为什么要添加分布","p":909},{"i":917,"t":"GAN的工作原理： 生成器生成数据： 生成器从潜在空间中采样并生成一些数据。 真实数据与生成数据进入判别器： 真实数据和生成器生成的数据一起输入判别器。 判别器训练： 判别器被训练来正确分类真实数据和生成数据。 生成器训练： 生成器被训练来生成能够欺骗判别器的数据。生成器的目标是生成足够逼真的数据，以至于判别器无法准确区分真假。 迭代： 生成器和判别器交替训练，迭代进行，直到生成器生成的数据足够逼真。 训练的目标： 生成器目标： 生成更逼真的数据，以欺骗判别器。 判别器目标： 区分真实数据和生成数据，提高对真实数据的分类准确性。 GAN的训练是一个博弈过程，生成器和判别器相互竞争，最终达到平衡，生成器生成的数据足够逼真，判别器也无法准确判别真伪。这种模型在图像生成、风格转换等任务中取得了显著的成功。","s":"核心思想","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#核心思想","p":909},{"i":919,"t":"GAN（Generative Adversarial Network，生成对抗网络）包括两个主要的组件：生成器（Generator）和判别器（Discriminator）。这两个组件通过对抗训练的方式一起学习。 生成器（Generator）： 它负责生成与训练数据相似的新样本。生成器接收来自潜在空间（latent space）的随机向量作为输入，并输出一个与训练数据类似的样本。生成器的目标是欺骗判别器，使其无法区分生成的样本和真实的训练数据。 判别器（Discriminator）： 它负责判别输入的样本是真实的训练数据还是生成器生成的假样本。判别器的目标是尽可能准确地分类输入的样本。 GAN的核心思想是通过对抗过程训练生成器和判别器，不断提高它们的性能。生成器试图生成逼真的样本，而判别器试图正确地区分真实样本和生成样本。这个对抗的训练过程可以被视为在两个分布之间进行的最优控制。","s":"具体结构与作用","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#具体结构与作用","p":909},{"i":921,"t":"结构： 生成器是一个神经网络，通常是一个反卷积神经网络（Generator Network），其输入通常是一个随机噪声（潜在空间中的点），输出是与训练数据相似的图像或数据。 作用： 生成器的目标是学习生成与真实数据相似的数据。通过迭代训练，生成器的参数被调整，使其生成的数据能够愈发逼真。 Unconditional Generation​ Unconditional generation（无条件生成）指的是在生成模型中生成样本时，不受任何条件的约束。在这种情况下，生成器仅根据其学到的分布生成数据，而无需关注特定的输入条件或上下文。 对于生成对抗网络（GAN）或变分自动编码器（VAE）等生成模型，unconditional generation通常表现为从潜在空间中采样，然后将这些样本输入生成器，以生成新的、与训练数据相似的样本。这种生成方式是随机的，因为每次从潜在空间中采样都会导致生成不同的样本。 Conditional Generation​","s":"生成器（Generator）","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#生成器generator","p":909},{"i":923,"t":"结构： 判别器是一个二元分类器，通常是一个卷积神经网络（Discriminator Network）。它的输入可以是真实数据或生成器生成的数据，输出是一个概率，表示输入数据是真实数据的概率。 作用： 判别器的目标是学习区分真实数据和生成器生成的数据。它被训练成对真实数据给出高概率，对生成的数据给出低概率。","s":"判别器（Discriminator）","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#判别器discriminator","p":909},{"i":925,"t":"首先随机初始化生成器和判别器，接下来在每一轮训练中重复以下策略。 第一步，在生成器通过随机噪声神生成样本后，固定生成器的参数，将生成器产生的输出与训练资料中的标签作为判别器的输入，判别器为每个输入样本打分，代表其为真实样本的概率。 为了最小化损失函数使得判别器为真实样本赋分更高，为生成样本赋分更低，设计以下损失函数： Lossreal=−Ex∼pdata(x)[log⁡D(x)](1)Loss_{real}=-E_{x\\sim p_\\text{data}{(x)}}[\\log D(x)]\\tag{1}Lossreal​=−Ex∼pdata​(x)​[logD(x)](1) 其中，D(x)D(x)D(x)是判别器的输出，xxx是真实样本，EEE是数学期望。 Lossfake=−Ez∼pz(z)[log(1−D(G(z)))](2)Loss_{fake}=-E_{z\\sim p_z(z)}[log(1-D(G(z)))]\\tag{2}Lossfake​=−Ez∼pz​(z)​[log(1−D(G(z)))](2) 其中，G(z)G(z)G(z)是生成器的输出，zzz是随机噪声，EEE是数学期望。 将真实样本和生成样本的损失相加，形成判别器的总体损失。 Lossdiscriminator=Lossreal+Lossfake(3)Loss_{discriminator}=Loss_{real}+Loss_{fake}\\tag{3}Lossdiscriminator​=Lossreal​+Lossfake​(3) 最小化损失函数，更新判别器的参数。 第二步，在判别器参数更新后，固定判别器的参数，随机分布的向量再次输入至生成器中，得到生成样本，此时生成样本被送入参数固定的判别器中得到生成样本属于真实样本的概率。在生成器的训练过程中，我们的目的是让生成器生成的样本尽可能的接近真实样本。 LG=−Ez∼pz(z)[log⁡D(G(z))](4)L_G=-E_{z\\sim p_z(z)}[\\log D(G(z))]\\tag{4}LG​=−Ez∼pz​(z)​[logD(G(z))](4) 其中，G(z)G(z)G(z)是生成器的输出，D(G(z))D(G(z))D(G(z))是生成样本输入到判别器后的输出，zzz是随机噪声，EEE是数学期望。","s":"训练算法","u":"/en/docs/Deep-Learning/大模型基础/GAN","h":"#训练算法","p":909},{"i":927,"t":"相关链接 论文：arXiv 参考资料： Transformer模型详解（图解最完整版） 【機器學習2021】Transformer (下) Transformer是Sequence-to-Sequence (Seq2Seq) 模型，模型的输入是向量序列，输出同样是向量序列，且输出的长度由模型经过学习决定。","s":"NeurIPS 2017: Attention Is All You Need","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"","p":926},{"i":929,"t":"Transformer由Encoder和Decoder组成，编码器和解码器都包含6个Block，整体结构如下图所示。","s":"整体结构","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#整体结构","p":926},{"i":932,"t":"Transformer Encoder结构如下图所示。其中，Add指的是残差连接Residual Connection，Norm指的是Layer Normalization。","s":"整体结构","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#整体结构-1","p":926},{"i":934,"t":"对于输入的句子，对一个词汇的嵌入向量的奇数维度使用sine函数进行编码，对偶数维度使用cosine函数计算编码。 公式如下所示，其中pospospos指的是该词汇在整个输入句子中的位置，2i2i2i以及2i+12i+12i+1指的是该词汇的嵌入向量中的维度，dmodeld_{model}dmodel​指的是在嵌入层之后嵌入向量的总维度。即对于每个输入词汇，都要计算dmodeld_{model}dmodel​次位置编码。 PE(pos,2i)=sin(pos100002i/dmodel)(1)PE_{(pos,2i)}=sin(\\frac{pos}{10000^{2i/d_{\\mathrm{model}}}}) \\tag{1}PE(pos,2i)​=sin(100002i/dmodel​pos​)(1)PE(pos,2i+1)=cos(pos100002i/dmodel)(2)PE_{(pos,2i+1)}=cos(\\frac{pos}{10000^{2i/d_{\\mathrm{model}}}}) \\tag{2}PE(pos,2i+1)​=cos(100002i/dmodel​pos​)(2) 根据三角函数的性质，对于pos+kpos+kpos+k位置的嵌入向量的某一维度（2i2i2i或2i+12i+12i+1）而言，可以表示为pospospos位置与kkk位置的嵌入向量的2i2i2i与2i+12i+12i+1维度的线性组合，使得位置向量中蕴含了相对位置的信息。 PE(pos+k,2i)=PE(pos,2i)×PE(k,2i+1)+PE(pos,2i+1)×PE(k,2i)PE(pos+k,2i+1)=PE(pos,2i+1)×PE(k,2i+1)−PE(pos,2i)×PE(k,2i)(3)\\begin{array}{l}PE(pos+k,2i)=PE(pos,2i)\\times PE(k,2i+1)+PE(pos,2i+1)\\times PE(k,2i)\\\\PE(pos+k,2i+1)=PE(pos,2i+1)\\times PE(k,2i+1)-PE(pos,2i)\\times PE(k,2i)\\end{array} \\tag{3}PE(pos+k,2i)=PE(pos,2i)×PE(k,2i+1)+PE(pos,2i+1)×PE(k,2i)PE(pos+k,2i+1)=PE(pos,2i+1)×PE(k,2i+1)−PE(pos,2i)×PE(k,2i)​(3) 最终，位置编码向量的维度与词汇的嵌入维度相同，进行element-wise的相加操作。 InputEmbedding(pos,i)=WordEmbedding(pos,i)+PositionEncoding(pos,i)(4)InputEmbedding(pos,i)=WordEmbedding(pos,i)+PositionEncoding(pos,i) \\tag{4}InputEmbedding(pos,i)=WordEmbedding(pos,i)+PositionEncoding(pos,i)(4)","s":"位置编码（Positional Encoding）","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#位置编码positional-encoding","p":926},{"i":936,"t":"输入向量由Word Embedding和Positional Embedding相加得到。输入序列经过Mutil-Head Self-Attention之后，通过Residual Connection加上自身的输入向量，再经过Layer Normalization，之后送入FCN并进行Residual Connection加上送入FCN的输入自身，最终再进行Layer Normalization，以上构成了一个Encoder Block。每一个Block输出的向量序列长度等于输入的向量序列长度。","s":"具体结构","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#具体结构","p":926},{"i":938,"t":"Decoder的任务是生成输出，可以根据是否一次性生成输出分为Autoregressive（自回归，abbr. AT）以及Non-Autoregressive（非自回归，abbr. NAT）两种模式。 自回归类型的Decoder需要逐步生成输出，并将之前自身输出的所有词汇经过嵌入层后生成token作为下一次的输入，通常每次生成一个词或一个符号。这种方式的缺点是需要保存和更新词表中的所有可能选项，因此在大词汇表上可能会变得非常慢。然而，它的优点是能够利用上下文信息来生成输出，这有助于提高翻译的质量。 非自回归类型的Decoder试图在一次操作中生成整个输出序列。这通常通过使用诸如注意力机制等策略来实现，这些策略允许解码器关注输入序列的不同部分，同时生成输出序列的不同部分。NAT的优点在于其高效性，因为它不需要保存和更新大量的可能选项。然而，由于它不能利用上下文信息来生成输出，因此其生成的输出质量普遍会低于AT。","s":"Decoder","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#decoder","p":926},{"i":940,"t":"整体结构​ 词汇表（Vocabulary）​ 词汇表（Vocabulary）是一个包含了在特定语言或任务中所有可能出现的所有单词或标记的集合。在自然语言处理（NLP）中，词汇表是训练模型时所使用的唯一单词的集合，由具体的生成任务而确定。 Decoder每一步的输出是一个经过Softmax的Probability Distribution（概率分布），代表着词汇表中每一个词汇当前生成的概率，取最大概率值的词汇便是模型当前时间步输出的词汇。 Begin符号​ 解码器（Decoder）在每个时间步（或每个解码步骤）的输入都来自于前一个时间步自身的输出以及编码器（Encoder）的输出。特别地，首个时间步的输入是Begin符号以及编码器（Encoder）的输出，在每个后续的时间步，解码器的输入会是前一个时间步自身的输出以及编码器（Encoder）的输出，直到生成序列的结束。 特殊符号 Begin符号是在Lexicon中添加的特殊符号，用来表示Decoder生成的开始。Begin符号通常被嵌入到一个低维的连续向量空间中，这个向量空间是通过嵌入层（Embedding Layer）学习得到的，在嵌入层中，离散的符号被映射到一个实数向量。 Begin符号又叫Start符号或SOS符号（Start Of Sentence），都是表示生成的开始。End符号又叫EOS符号（End Of Sentence）。 End符号​ 在Decoder的生成中，每一个时间步的输出是词汇表中每一个单词经过Softmax之后的概率分布。为了保证生成任务可以通过模型自己停止而不是一直重复，我们向Decoder的输出中加入End符号的生成，即每一次输出除了词汇表的所有词汇外还有End符号的概率，当End符号是在所有词汇中概率最大的词汇时，生成停止。 掩码多头自注意力机制（Masked Multi-Head Self-Attention）​ 为什么使用掩码多头自注意力 掩码多头自注意力与Transformer训练时采取的Teacher Forcing策略有很大的关系，具体分析见下文《Teacher Forcing与Masked Multi-Head Self-Attention》的讨论环节：Teacher Forcing与Masked Multi-Head Self-Attention 观察Decoder的整体结构，掩码多头自注意力的输入是添加位置编码之后的Decoder当前时间步之前的所有输出单词经过嵌入后的向量表示。 掩码多头自注意力机制用于确保在生成序列的过程中，每个位置只能关注到该位置及其之前的位置。这是通过在Self-Attention的计算中应用一个掩码（mask）来实现的。这确保了在生成序列时，每个位置只能查看到它之前的信息，而不能查看到未来的信息，从而实现了自回归性质。 具体来说，添加掩码后的自注意力机制在生成注意力分数时不再考虑输入序列的所有向量。如在输入向量aia^iai在计算注意力分数时，只将aia^iai的query向量与a1a^1a1至aia^{i}ai的iii个key向量做dot product，而不考虑aia^iai之后的输入的key。 tip 对于第sss个时间步，Masked Mutil-Head Self-Attention的输入是时间步sss之前Decoder生成的所有输出单词的嵌入表示。 交叉注意力（Cross-Attention）​ 交叉注意力是连接Encoder和Decoder的桥梁，也是Decoder输入的重要组成部分。 交叉注意力接收两个输入序列，一个来自编码器（Encoder）的输出序列（通常是输入序列的表示），另一个来自解码器（Decoder），是经过掩码多头自注意力机制的输出序列（通常是正在生成的序列的中间表示）。 在交叉注意力中，每次计算注意力得分的query来自解码器，key和value来自编码器。解码器每个向量的查询（Query）与编码器位置的键（Key）进行点积得到了注意力分数，通过Softmax操作后转换为注意力权重，再与编码器位置的值（Value）weighted sum得到加权注意力分数，最终将加权注意力分数求和得到每个输入向量的输出。","s":"Autoregressive Decoder（AT）","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#autoregressive-decoderat","p":926},{"i":944,"t":"在 Transformer 中，Encoder 不像 Decoder 需要生成序列，因此它通常不涉及标签的预测。Encoder 的训练通常是在整个模型中的联合训练中进行的，通过优化整个模型的损失函数来进行。 Transformer 的整体训练过程一般分为以下几个步骤： 编码器（Encoder）的正向传播： 输入序列经过编码器的正向传播，产生一组上下文表示。 解码器（Decoder）的正向传播： 解码器接收上下文表示，并生成目标序列。 计算损失： 通过比较生成的目标序列与实际目标序列，计算损失。在 Decoder 中，通常使用交叉熵损失函数。 反向传播： 根据损失，进行反向传播，更新模型参数。这个过程中，梯度通过整个模型传播，包括 Encoder 和 Decoder。 整个模型的参数（包括 Encoder 和 Decoder）都是通过最小化整体损失来进行联合训练的。这是因为整体模型需要协同工作，Encoder 的表示对于 Decoder 的性能至关重要。在训练过程中，梯度从损失函数传播回整个模型，包括 Encoder 和 Decoder，从而更新它们的参数。 需要注意的是，Transformer 模型通常使用的是端到端的训练方式，整个模型的参数是一次性更新的。在某些场景下，你可能会看到对 Encoder 或 Decoder 进行微调（fine-tuning）的情况，但这是在特定应用场景下的调整，不是 Transformer 模型的标准训练方式。","s":"损失函数","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#损失函数","p":926},{"i":946,"t":"在Transformer的推理阶段，自回归类型的Decoder根据分词方式的不同，一个词汇一个词汇的输出，将当前时间步之前生成的所有词汇作为输入load进入Decoder中。但在训练时如果遵从同样的生成范式会大大降低效率，并且面临则一步错步步错的风险（Error Propagation）。 因此使用Teacher Forcing策略，将Ground Truth一次性喂到Decoder中，使模型更快收敛并且避免误差积累的问题。 但是，自回归Decoder在推理时是一个一个词汇产生的，在产生第iii个词汇时其后续的词汇是未知的，更不用说进行注意力分数的就算了，而在训练过程中使用Teacher Forcing时却可以得到第i+1i+1i+1个及其之后词汇的注意力信息，如果不添加其他策略显然会对模型的泛化能力造成很大的影响，而且这并不符合自回归（Autoregression）的特性。为了解决这个问题，掩码多头注意力机制应运而生，在训练阶段将模型在时间发展顺序的右侧的输入masked掉，防止模型学习到不该学习的注意力。","s":"Teacher Forcing","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#teacher-forcing","p":926},{"i":948,"t":"参考文献：MultiHead-Attention和Masked-Attention的机制和原理 与Encoder的多头自注意力不同，在Decoder中，为注意力机制应用了掩码，使模型只能关注到当前位置及其之前的位置，而不能访问未来的信息。这解决了引入Teacher Forcing出现的问题，避免了训练与推理阶段的Mismatch，维护了自回归的特性。 具体来说，模拟推理过程中第一个词汇时的场景。当模型只有voc1voc_1voc1​词汇向量输入时，在Decoder中，voc1voc_1voc1​与自身计算注意力分数，于是有 [o1]=[α1,1′][v1](5)\\begin{bmatrix}o_1\\end{bmatrix}=\\begin{bmatrix}\\alpha_{1,1}^{\\prime}\\end{bmatrix}\\begin{bmatrix}v_1\\end{bmatrix}\\tag{5}[o1​​]=[α1,1′​​][v1​​](5) 我们再模拟训练过程中使用Teacher Forcing，一次性输入为两个词汇voc1voc_1voc1​与voc2voc_2voc2​的情况，于是有 [o1o2]=[α1,1′α2,1′α1,2′α2,2′][v1v2](6)\\begin{bmatrix}o_1\\\\o_2\\end{bmatrix}=\\begin{bmatrix}\\alpha_{1,1}^{\\prime}&\\alpha_{2,1}^{\\prime}\\\\\\alpha_{1,2}^{\\prime}&\\alpha_{2,2}^{\\prime}\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix} \\tag{6}[o1​o2​​]=[α1,1′​α1,2′​​α2,1′​α2,2′​​][v1​v2​​](6) 然而，为了使训练过程中符合推理时自回归的特性，理想的输出应该是 [o1o2]=[α1,1′0α1,2′α2,2′][v1v2](7)\\begin{bmatrix}o_1\\\\o_2\\end{bmatrix}=\\begin{bmatrix}\\alpha_{1,1}^{\\prime}&0\\\\\\alpha_{1,2}^{\\prime}&\\alpha_{2,2}^{\\prime}\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix} \\tag{7}[o1​o2​​]=[α1,1′​α1,2′​​0α2,2′​​][v1​v2​​](7) 继续扩展，当有nnn个输入词汇时，应该有 [o1o2⋮on]=[α1,1′0⋯0α1,2′α2′⋯0⋮⋮⋮α1,n′α2,n′⋯αn,n′][v1v2⋮vn](8)\\begin{bmatrix}o_1\\\\o_2\\\\\\vdots\\\\o_n\\end{bmatrix}=\\begin{bmatrix}\\alpha'_{1,1}&0&\\cdots&0\\\\\\alpha'_{1,2}&\\alpha'_2&\\cdots&0\\\\\\vdots&\\vdots&&\\vdots\\\\\\alpha'_{1,n}&\\alpha'_{2,n}&\\cdots&\\alpha'_{n,n}\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{bmatrix}\\tag{8}​o1​o2​⋮on​​​=​α1,1′​α1,2′​⋮α1,n′​​0α2′​⋮α2,n′​​⋯⋯⋯​00⋮αn,n′​​​​v1​v2​⋮vn​​​(8) 因此，我们需要将当前时间步计算的词汇的时间顺序右侧的输入词汇全部掩码，置为0。 在源码中，有如下片段实现掩码： if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = scores.softmax(dim=-1) 在源码中，将mask置为负无穷是因为这是在经过Softmax之前进行的掩码，在经过Softmax之后负无穷小就变成了0。","s":"Teacher Forcing与Masked Multi-Head Self-Attention","u":"/en/docs/Deep-Learning/大模型基础/Attention-Is-All-You-Need","h":"#teacher-forcing与masked-multi-head-self-attention","p":926},{"i":950,"t":"tip 本笔记使用的教材是陈天华所著、清华大学出版社的《数字图像处理及应用：使用MATLAB分析与实现》。 Take me to church, I'll worship like a dog at the shrine of your lies. I'll tell you my sins, and you can sharpen your knife. Offer me that deathless death, good god, let me give you my life.","s":"数字图像处理复习笔记","u":"/en/docs/Curriculum/数字图像处理/Note","h":"","p":949},{"i":953,"t":"为了从模拟图像产生数字图像，需要进行采样与量化，即对模拟图像在空间(x,y)(x, y)(x,y)方向上以及亮度函数f(x,y)f(x, y)f(x,y)进行离散化处理。 采样： 模拟图像在空间(x,y)(x, y)(x,y)上的离散化称为采样。 若在x和y方向上均进行等间距的采样，则称为均匀采样。 采样点的多少以及采样的间隔直接影响着图像的质量。 量化： 模拟图像经过采样后，在时间和空间上被离散化为像素，但采样所得的像素点的像素值依然是连续量。量化过程就是以离散的灰度值信息代替连续的模拟量灰度信息的过程，是一对多的过程。 量化可以分为线性量化以及非线性量化。 灰度级一般以2的整数次幂表示，如大多图像为彩色RGB图像，256个灰度级，位深度为8（28=2562^8=25628=256），则对于分辨率为256×256的图像来说，需要256×256×3×8位表示，即每一个像素实际上使用24位表示。","s":"图像的采样与量化","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#图像的采样与量化","p":949},{"i":955,"t":"4邻域N4(p)N_4(p)N4​(p)即该像素上下左右的四个点，8邻域N8(p)N_8(p)N8​(p)，对角邻域ND(p)N_D(p)ND​(p)。 像素之间的距离度量必须满足三种关系： 非负性 对称性 三角不等式 三种距离度量： 已知，点ppp的坐标为(x,y)(x, y)(x,y)，点qqq的坐标为(s,t)(s,t)(s,t)。 欧氏距离： De(p,q)=(x−s)2+(y−t)2(1)D_e(p,q)=\\sqrt{(x-s)^2+(y-t)^2}\\tag{1}De​(p,q)=(x−s)2+(y−t)2​(1) 城市距离 D4(p,q)=∣x−s∣+∣y−t∣(2)D_4(p,q)=\\vert x-s\\vert+\\vert y-t\\vert \\tag{2}D4​(p,q)=∣x−s∣+∣y−t∣(2) 棋盘距离 D8(p,q)=max(∣x−s∣,∣y−t∣)(3)D_8(p,q)=max(\\vert x-s\\vert,\\vert y-t\\vert)\\tag{3}D8​(p,q)=max(∣x−s∣,∣y−t∣)(3)","s":"距离度量","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#距离度量","p":949},{"i":957,"t":"MSE越小代表待测图像与参考图像越接近，均方误差公式如下： MSE=1MN∑x=1M∑y=1N[f(x,y)−g(x,y)]2(4)MSE=\\frac{1}{MN}\\sum_{x=1}^{M}\\sum_{y=1}^{N}[f(x,y)-g(x,y)]^2\\tag{4}MSE=MN1​x=1∑M​y=1∑N​[f(x,y)−g(x,y)]2(4) SNR：参考图像像素值的平方均值与均方误差的比值的对数的10倍。越大代表图像质量越好 PSNR：峰值信噪比，图像所允许的最大像素值的平方与均方误差的比值的对数的10倍，这是信噪比的改良版本，消除了图像自身像素值对评价指标的影响。越大代表图像质量越好 SSIM为结构相似系数，越大代表图像质量越好","s":"图像质量评价","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#图像质量评价","p":949},{"i":961,"t":"可以使用傅里叶变换的函数需要满足狄利克莱条件（Dirichlet Condition）： 具有有限个间断点 具有有限个极值点 绝对可积 傅里叶变换的特性： important 傅里叶频谱图的特征： 频率分布：傅里叶频谱图展示了图像在不同频率下的强度分布。高频部分对应图像中的边缘和细节，低频部分对应图像中的整体结构和大致轮廓。 能量分布：图像中不同频率的能量在频谱图中以不同强度的幅度呈现。高幅度的频率分量通常标示着图像中强烈的变化或边缘。 平移不变性：傅里叶变换具有平移不变性，这意味着在频域中图像的平移对应于幅度谱中相位的改变而不影响幅度谱本身。 连续傅里叶变换​ 一维连续傅里叶变换 F(u)=∫−∞+∞f(x)e−j2πuxdx(5)F(u)=\\int_{-\\infty}^{+\\infty}f(x)e^{-j2\\pi ux}dx\\tag{5}F(u)=∫−∞+∞​f(x)e−j2πuxdx(5) 一维连续傅里叶逆变换 f(x)=∫−∞∞F(u)ej2πuxdu(6)f(x)=\\int_{-\\infty}^{\\infty}F(u)e^{j2\\pi ux}du\\tag{6}f(x)=∫−∞∞​F(u)ej2πuxdu(6) 二维连续傅里叶变换 F(u,v)=∫−∞∞∫−∞∞f(x,y)e−j2π(ux+vy)dxdy(7)F(u,v)=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f(x,y)e^{-j2\\pi (ux+vy)}dxdy\\tag{7}F(u,v)=∫−∞∞​∫−∞∞​f(x,y)e−j2π(ux+vy)dxdy(7) 二维连续傅里叶逆变换 f(x,y)=∫−∞∞∫−∞∞F(u,v)ej2π(ux+vy)dudv(8)f(x,y)=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}F(u,v)e^{j2\\pi (ux+vy)}dudv\\tag{8}f(x,y)=∫−∞∞​∫−∞∞​F(u,v)ej2π(ux+vy)dudv(8) 离散傅里叶变换​ 一维离散傅里叶变换 F(u)=∑x=0N−1f(x)e−j2πuxN(9)F(u)=\\sum_{x=0}^{N-1}f(x)e^{-j\\frac{2\\pi ux}{N}}\\tag{9}F(u)=x=0∑N−1​f(x)e−jN2πux​(9) 一维离散傅里叶逆变换 f(x)=1N∑u=0N−1F(u)ej2πuxN(10)f(x)=\\frac{1}{N}\\sum_{u=0}^{N-1}F(u)e^{j\\frac{2\\pi ux}{N}}\\tag{10}f(x)=N1​u=0∑N−1​F(u)ejN2πux​(10) 二维离散傅里叶变换 F(u,v)=∑x=0M−1∑y=0N−1f(x,y)e−j2π(uxM+vyN)(11)F(u,v)=\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(x,y)e^{-j2\\pi (\\frac{ux}{M}+\\frac{vy}{N})}\\tag{11}F(u,v)=x=0∑M−1​y=0∑N−1​f(x,y)e−j2π(Mux​+Nvy​)(11) 二维离散傅里叶逆变换 f(x,y)=1MN∑u=0M−1∑v=0N−1F(u,v)ej2π(uxM+vyN)(12)f(x,y)=\\frac{1}{MN}\\sum_{u=0}^{M-1}\\sum_{v=0}^{N-1}F(u,v)e^{j2\\pi (\\frac{ux}{M}+\\frac{vy}{N})}\\tag{12}f(x,y)=MN1​u=0∑M−1​v=0∑N−1​F(u,v)ej2π(Mux​+Nvy​)(12)","s":"傅里叶变换","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#傅里叶变换","p":949},{"i":964,"t":"下图展示了点运算的主要应用：对比度拉伸 主要应用： 对比度拉伸 光度学标定 显示标定 轮廓线 裁剪","s":"点运算","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#点运算","p":949},{"i":966,"t":"使用齐次坐标对图像进行比例缩放。 全比例缩放是指x方向和y方向使用相同的比例系数。 当对图像进行放大时，会产生之前没有的像素，此时需要使用插值来解决。","s":"比例缩放","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#比例缩放","p":949},{"i":968,"t":"最近邻法插值​ 双线性插值（Bilinear）​ 例题：","s":"灰度级插值","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#灰度级插值","p":949},{"i":971,"t":"线性变换​ 是指将输入图像的灰度值的动态范围按线性关系变换至指定范围或灰度的整个动态范围。 按比例线性变换 均匀线性变换 限幅线性变换 分段线性拉伸 非线性变换​ 对数拉伸可以拉伸低亮度区域，压缩高亮度区域。 指数拉伸可以拉伸高亮度区域，压缩低亮度区域。 注意，上述的拉伸是指新的图像在该灰度范围内分布的更均匀，即出现的灰度级更多；压缩的意义是指新的图像在该灰度范围内分布的更狭窄，即出现的灰度级更少。 例题： 答：该图像存在较亮的问题，灰度直方图分布在较高的区域。从直接灰度变换增强法的角度，我们可以采用指数函数变换，对高灰度区进行扩展。 tip 分析：该图像中没有低灰度的像素，全部集中在高灰度部分，因此使用指数函数变换将原来的集中的高灰度区域进行拉伸，扩展其灰度分布的范围，从而起到增强效果。","s":"直接灰度变换","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#直接灰度变换","p":949},{"i":973,"t":"直方图均衡化​ 目的： 将一非均匀灰度概率密度分布的图像，通过某种灰度变换，将其变成一幅具有均匀灰度概率密度分布的目的图像。 步骤： 根据原始图像计算原始灰度密度分布函数rkr_krk​ 计算变换函数，即累积密度分布函数sks_ksk​ 计算均衡化后的灰度级s(k)s(k)s(k)，即使用以下公式进行均衡化映射： s(k)=ceil(sk×L−1)(13)s(k)=\\mathbf{ceil}(s_k\\times L-1)\\tag{13}s(k)=ceil(sk​×L−1)(13) 其中，LLL是原直方图中灰度级数。 画出最终均衡化后的直方图 例题： 直方图规定化​ 目的： 调整原始图像的直方图使其符合某一规定的直方图的要求。 思想： 将原始图像和规定图像进行均衡化后，二者的灰度概率密度分布相同，进行对应映射即可。 步骤： 对原始图像进行均衡化，得到映射后的s(k)s(k)s(k) 对规定的图像直方图进行均衡化，得到映射后的z(k)z(k)z(k) 由于s(k)s(k)s(k)和z(k)z(k)z(k)都是归一化后的均匀分布，使用二者之间的就近原则将s(k)s(k)s(k)对应到给出的规定直方图的灰度级 画出规定化后的直方图 例题： 此时已经获得了原始图像均衡化后的灰度级与目标规定化输出的图像的灰度级之间的映射关系，最后一步按照rkr_krk​画出目标规定化输出的图像直方图即可。","s":"直方图灰度变换","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#直方图灰度变换","p":949},{"i":975,"t":"空域滤波可以根据作用分为平滑滤波以及锐化滤波。 平滑滤波​ important 噪声的特点： 随机性 叠加性 噪声与图像之间具有相关性 线性平滑滤波​ 邻域平均滤波 加权平均滤波 高斯滤波 非线性平滑滤波​ 中值滤波 最大值滤波 最小值滤波 锐化滤波​ 锐化滤波消除或减弱图像的低频分量从而增强图像中物体的边缘轮廓信息，使得除边缘以外的像素点的灰度值趋向于零。 梯度法 拉普拉斯算子，非线性滤波 定向滤波：检测特定方向边缘的滤波，通常在模板上表现为在该特定方向上模板值较大。 下面的表格展示了水平方向的定向锐化模板 -1 -1 -1 2 2 2 -1 -1 -1 下面的表格展示了对角方向的定向锐化模板 -1 2 -1 -1 2 -1 -1 2 -1","s":"空域滤波增强","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#空域滤波增强","p":949},{"i":977,"t":"图像从空域变换到频域后，低频分量对应图像中灰度值变化缓慢的区域，可能是图像的背景；高频分量表示图像中灰度值变化迅速的区域，可能是图像的噪声或物体的边缘。 在图像频谱（经过中心偏移的）中，中心代表低频信息，四周代表高频信息。","s":"第6章 图像频域增强","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#第6章-图像频域增强","p":949},{"i":979,"t":"抑制、衰减高频分量，保留低频分量。 理想低通滤波器（ILPF）​ tip ILPF means Ideal Low Pass Filter. H(u,v)={1D(u,v)≤D00D(u,v)>D0(14)H(u,v)=\\begin{cases}1&D(u,v)\\leq D_0\\\\ 0&D(u,v)>D_0\\end{cases}\\tag{14}H(u,v)={10​D(u,v)≤D0​D(u,v)>D0​​(14)D(u,v)=u2+v2(15)D(u,v)=\\sqrt{u^2+v^2}\\tag{15}D(u,v)=u2+v2​(15) 在截止频率D0D_0D0​处垂直截止，通过频率和截止频率在D0D_0D0​处具有不连续性，通带和阻带之间没有过渡，会产生无限的振铃效应。 巴特沃斯低通滤波器（BLPF）​ 通带与阻带之间过度平坦，通过频率和截止频率之间没有明显的不连续性，不会出现振铃效应。 H(u,v)=11+(2−1)[D(u,v)D0]2n(16)H(u,v)=\\frac{1}{1+(\\sqrt{2}-1)[\\frac{D(u,v)}{D_0}]^{2n}}\\tag{16}H(u,v)=1+(2​−1)[D0​D(u,v)​]2n1​(16) 其中，取H(u,v)H(u,v)H(u,v)下降到最大值的0.707时的D(u,v)D(u,v)D(u,v)作为截止频率D0D_0D0​。 即当D(u,v)D(u,v)D(u,v)达到截止频率时，D(u,v)D0=1\\frac{D(u,v)}{D_0}=1D0​D(u,v)​=1，此时H(u,v)=0.707H(u,v)=0.707H(u,v)=0.707。 指数低通滤波器（ELPF）​ 一般情况下，取H(u,v)H(u,v)H(u,v)下降到最大值的1/2时的D(u,v)D(u,v)D(u,v)作为截止频率。 H(u,v)=e−[D(u,v)D0]n(17)H(u,v)=e^{-[\\frac{D(u,v)}{D_0}]^n}\\tag{17}H(u,v)=e−[D0​D(u,v)​]n(17) 截止频率和通过频率之间具有更光滑的过渡，没有振铃现象。且指数低通滤波器比巴特沃斯低通滤波器衰减更快，处理后的图像更模糊。 梯形低通滤波器（TLPF）​ 由于在D0D_0D0​尾部包含高频分量D1D_1D1​，处理后图像的清晰度较理想低通滤波器有所改善，但会出现振铃效应， H(u,v)={1D(u,v)<D0D(u,v)−D1D0−D1D0≤D(u,v)≤D10D(u,v)>D1(18)H(u,v)=\\begin{cases}1&D(u,v)<D_0\\\\ \\\\ \\frac{D(u,v)-D_1}{D_0-D_1}&D_0\\leq D(u,v)\\leq D_1\\\\ \\\\ 0&D(u,v)>D_1\\end{cases}\\tag{18}H(u,v)=⎩⎨⎧​1D0​−D1​D(u,v)−D1​​0​D(u,v)<D0​D0​≤D(u,v)≤D1​D(u,v)>D1​​(18)","s":"低通滤波","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#低通滤波","p":949},{"i":981,"t":"抑制低频分量，保留高频分量。 理想高通滤波器（IHPF）​ H(u,v)={1D(u,v)>D00D(u,v)≤D0(19)H(u,v)=\\begin{cases}1&D(u,v)>D_0\\\\ 0&D(u,v)\\leq D_0\\end{cases}\\tag{19}H(u,v)={10​D(u,v)>D0​D(u,v)≤D0​​(19) 性质与理想低通滤波器一样，垂直截断，具有无限振铃效应。 巴特沃斯高通滤波器（BHPF）​ H(u,v)=11+(2−1)[D0D(u,v)]2n(20)H(u,v)=\\frac{1}{1+(\\sqrt{2}-1)[\\frac{D_0}{D(u,v)}]^{2n}}\\tag{20}H(u,v)=1+(2​−1)[D(u,v)D0​​]2n1​(20) 通常采取H(u,v)H(u,v)H(u,v)下降到最大值的0.707时的D(u,v)D(u,v)D(u,v)作为截止频率，没有振铃效应。 指数高通滤波器（EHPF）​ H(u,v)=e−[D0D(u,v)]n(21)H(u,v)=e^{-[\\frac{D_0}{D(u,v)}]^n}\\tag{21}H(u,v)=e−[D(u,v)D0​​]n(21) 梯形高通滤波器（THPF）​ H(u,v)={1D(u,v)>D0D(u,v)−D1D0−D1D1≤D(u,v)≤D00D(u,v)<D1(22)H(u,v)=\\begin{cases}1&D(u,v)>D_0\\\\ \\\\ \\frac{D(u,v)-D_1}{D_0-D_1}&D_1\\leq D(u,v)\\leq D_0\\\\ \\\\ 0&D(u,v)<D_1\\end{cases}\\tag{22}H(u,v)=⎩⎨⎧​1D0​−D1​D(u,v)−D1​​0​D(u,v)>D0​D1​≤D(u,v)≤D0​D(u,v)<D1​​(22)","s":"高通滤波","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#高通滤波","p":949},{"i":983,"t":"带通滤波​ 顾名思义，带通滤波就是允许某一特定频率的信号通过，而衰减频率范围之外的信号。 理想带通滤波的传递函数为： H(u,v)={0D(u,v)<D0−w21D0−w2≤D(u,v)≤D0+w20D(u,v)≥D0+w2(23)H(u,v)=\\begin{cases}0&D(u,v)<D_0-\\frac{w}{2}\\\\1&D_0-\\frac{w}{2}\\leq D(u,v)\\leq D_0+\\frac{w}{2}\\\\0&D(u,v)\\geq D_0+\\frac{w}{2}\\end{cases}\\tag{23}H(u,v)=⎩⎨⎧​010​D(u,v)<D0​−2w​D0​−2w​≤D(u,v)≤D0​+2w​D(u,v)≥D0​+2w​​(23) 其中，D0D_0D0​是通带中心频率，www是通带宽度。 带阻滤波​ 顾名思义，带阻滤波就是衰减某一特定频率范围的信号，而允许频率范围之外的信号通过。 理想带阻滤波的传递函数为： H(u,v)={1D(u,v)<w10w1⩽D(u,v)≤w21D(u,v)>w2(24)H(u,v)=\\begin{cases}1&D(u,v)<w_1\\\\0&w_1\\leqslant D(u,v)\\leq w_2\\\\1&D(u,v)> w_2\\end{cases}\\tag{24}H(u,v)=⎩⎨⎧​101​D(u,v)<w1​w1​⩽D(u,v)≤w2​D(u,v)>w2​​(24)","s":"带通和带阻滤波","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#带通和带阻滤波","p":949},{"i":986,"t":"伪彩色图像的增强一般用于B超、石油开采以及安检方面。","s":"伪彩色图像的处理","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#伪彩色图像的处理","p":949},{"i":990,"t":"退化原因​ 成像系统镜头聚焦不准产生的散焦 相机与景物之间的相对运动 成像系统存在的各种非线性因素以及系统本身的性能 模拟图像在数字化过程中，因数字化的精度和误差而损失图像细节 成像系统中存在的各种随机噪声 复原机理​ 图像复原的过程一般是沿着图像退化的逆向过程进行的。首先根据先验知识分析退化原因，了解图像变质的原理，在此基础上建立图像的退化模型，然后以图像退化的逆过程对图像进行处理。 图像复原与图像增强的区别和联系​ 联系：二者从表面上看都是为了提高图像的质量。 区别：二者在目的和过程上都有明显的区别。 在目的上，图像增强是为了提高图像的视感质量，增强后的图像可能损失一些信息，并与原始图像有一定的差异；而图像复原是为了使待复原的图像与原始图像尽可能的接近。 在过程上。图像增强一般不考虑图像退化的真实过程，而是使用特定技术来突出和强调图像中所关注的特征；而图像复原是直接针对图像产生退化的原因建立相应的数学模型，并沿着退化的逆向进行复原。","s":"图像退化机理","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#图像退化机理","p":949},{"i":992,"t":"图像f(x,y)f(x,y)f(x,y)经过退化系统H(x,y)H(x,y)H(x,y)后再与噪声n(x,y)n(x,y)n(x,y)叠加，得到最后退化的图像g(x,y)g(x,y)g(x,y)。 退化系统的一般特性： 线性特性。 空间位置不变性：经过退化系统后的输出只有输入有关，而与输入在图像中的位置无关。","s":"图像退化模型","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#图像退化模型","p":949},{"i":995,"t":"最佳阈值法​ 假设图像由物体和背景两部分组成，且物体像素的分布和背景像素的分布均符合正态分布，物体像素的正态分布概率密度函数的均值为μ\\muμ，背景像素的正态分布概率密度函数的均值为ν\\nuν，则最佳阈值法确定的阈值为 t=μ+ν2(25)t=\\frac{\\mu +\\nu}{2}\\tag{25}t=2μ+ν​(25) 判别分析法​ 通过计算灰度直方图的0阶矩和1阶矩最大化类间方差从而得到最佳阈值。","s":"阈值分割法","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#阈值分割法","p":949},{"i":997,"t":"什么是图像的边缘？ 图像中结构具有突变的地方，表明一个区域的终结，也是另一个区域的开始，这种不连续性称为边缘。 边缘信号的类型？ 阶跃型，但实际情况中不可能有完全理想的突变。突变处为边缘点。 渐变型，逐渐增大或逐渐减小。渐变的中间位置为边缘点。 台阶型。台阶的中间为边缘点或认为台阶两侧为两个边缘点。","s":"边缘检测的基本原理","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#边缘检测的基本原理","p":949},{"i":999,"t":"Robert算子​ 一阶导数算子。 利用局部差分方法，采用对角线方向相邻两像素之差近似梯度幅值检测边缘。 对噪声敏感，不能抑制噪声。 Sobel算子​ 一阶导数算子。 先进行加权平均，然后进行微分运算。 对噪声具有一定的抑制能力。 Prewitt算子​ 一阶导数算子。 利用局部差分平均方法寻找边缘。两个模板一个检测水平边缘，一个检测竖直边缘。 对噪声具有一定的抑制能力。 Laplace算子​ 二阶导数算子。 使噪声成分得到加强，对噪声更敏感。 与Marr边缘检测算子一样，一般先进行低通滤波平滑后再进行二阶微分运算。 Canny边缘检测​ 使用高斯滤波平滑图像 计算梯度幅值和方向 NMS非极大值抑制，保留每个像素点上梯度强度的极大值，删掉其他值 使用双阈值方法确定强边界和弱边界 滞后边界跟踪","s":"边缘检测算子","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#边缘检测算子","p":949},{"i":1001,"t":"傅里叶频谱图的特征： 频率分布：傅里叶频谱图展示了图像在不同频率下的强度分布。高频部分对应图像中的边缘和细节，低频部分对应图像中的整体结构和大致轮廓。 能量分布：图像中不同频率的能量在频谱图中以不同强度的幅度呈现。高幅度的频率分量通常标示着图像中强烈的变化或边缘。 平移不变性：傅里叶变换具有平移不变性，这意味着在频域中图像的平移对应于幅度谱中相位的改变而不影响幅度谱本身。 如何在频域实现图像平滑 利用傅里叶变换将图像从空域转换为频域； 将频域图像进行中心偏移，使得低频信息在频谱中央； 利用滤波函数生成一个与图像大小相同的二维频域矩阵； 将图像的频域与滤波器的频域相应相乘； 将相乘后的频域再次逆平移到频域的原始位置； 将得到的频域进行逆傅里叶变换，得到滤波后的空域图像。 一阶边缘检测算子与Laplace算子的异同。 相同点：都可以检测边缘并且对噪声敏感 不同点：一阶边缘检测算子检测到的边缘都有明确的方向，而Laplace算子对各种方向的边缘都有较好的响应 已知一幅图像受到加性随机噪声污染，分析并写出三种去除或降低噪声的方法。 空域滤波（均值滤波、中值滤波），频域滤波（巴特沃斯低通滤波），图像恢复技术。 分析采样和量化的过程，以及它们对数字化图像质量的影响。 采样是把空间上连续的图像转换为离散的抽样点，即像素。量化将抽样后所得的连续的像素值离散化为整数值。 对数字化图像质量的影响： 采样间隔越大，所得图像像素数越少，图像空间分辨率越低，质量越差。反之图像质量好，但数据量大。 量化等级越多（灰度级数越多），所得图像层次越丰富，灰度分辨率越高，质量越好，但数据量大。反之，图像质量差，会出现假轮廓现象，但数据量小。 图像噪声的特点。 随机性 叠加性 噪声与图像之间具有相关性 什么是线性灰度拉伸？线性灰度拉伸可以分为几种情况？ 线性灰度拉伸是将输入图像的灰度值的动态范围按线性关系拉伸扩展至指定范围或灰度的整个动态范围。 线性拉伸可分为按比例线性拉伸和分段线性拉伸两种方法。同时，按比例线性拉伸又可以分为均匀线性拉伸以及限幅线性拉伸。 伪彩色图像处理可以应用在哪些方面？ B超 石油开采 安检","s":"课后习题中的问答题","u":"/en/docs/Curriculum/数字图像处理/Note","h":"#课后习题中的问答题","p":949},{"i":1003,"t":"tip 在以前的文章图像生成模型中已经大概介绍了目前SOTA的图像生成模型的共同点，并初步了解了Diffusion Model，在这篇文章中将详细讲解扩散模型的数学原理等。","s":"扩散模型（Diffusion Model）","u":"/en/docs/Deep-Learning/大模型基础/Diffusion-Model","h":"","p":1002},{"i":1005,"t":"首先回顾一下扩散模型的基本概念和生成过程，可以大概分为两步： Forward Process：对训练集中的图片不断加入与图片shape相同的、从某随机分布中sample出的噪声，直至图片可以被认为是从该随机分布中sample出的矩阵。 Forward Process又叫做Diffusion Process，在这一步中产生的噪声-加入噪声的图像对可以用来训练Noise Predictor，即从有噪声的图像中预测出其中的噪声，再从输入中减去噪声得到降噪后的图片。 图像生成的原理 这一步的目的也同样在之前的文章图像生成模型中提到过：由于根据文字prompt期待生成的图像并不是固定的，可以认为生成的图片在目标域（Target Domain）符合某种分布。因此目前的SOTA模型除了将文字prompt作为输入，还从某随机分布中sample出图片shape的随机向量（矩阵）作为输入，期待模型根据prompt将源域（Source Domain）输入的随机向量映射到目标域的分布，生成对应的图片。 Reverse Process：使用Diffusion Process训练的Noise Predictor，根据文字Prompt对从随机分布中sample出的图片大小的噪声图片进行降噪，得到原图。 值得注意的是，变分自编码器（Variational Auto-Encoder, abbr. VAE）与Diffusion Model非常相似：VAE对训练集中的原始图像使用Encoder将其变换为某种Latent Representation，这种Latent Representation的分布也是符合某种随机分布的，VAE再通过Decoder将期待生成的目标域图像还原出来。 在下面的文章中我们也会学习一下VAE的数学原理，从VAE到Diffusion Model的具体数学推导，可以参考胡老师推荐的论文Understanding Diffusion Models: A Unified Perspective。 下面我们以DDPM论文中的原图来分析DDPM的训练与推理过程。","s":"基本概念","u":"/en/docs/Deep-Learning/大模型基础/Diffusion-Model","h":"#基本概念","p":1002},{"i":1007,"t":"循环开始，重复以下步骤； 首先从数据集中sample出原始图像x0\\mathbf{x}_0x0​； ttt是从1,…,T1,\\ldots,T1,…,T范围中sample出的一个integer； ϵ\\epsilonϵ是从Normal Distribution中sample出的与x0\\mathbf{x}_0x0​相同大小的噪声； 根据如下规则进行梯度下降，训练Noise Predictor： ∇θ∥ϵ−ϵθ(αˉtx0+1−αˉtϵ,t)∥2(1)\\nabla_{\\theta}\\left\\|\\boldsymbol{\\epsilon}-\\boldsymbol{\\epsilon}_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\boldsymbol{\\epsilon},t)\\right\\|^{2}\\tag{1}∇θ​​ϵ−ϵθ​(αˉt​​x0​+1−αˉt​​ϵ,t)​2(1) 首先对x0\\mathbf{x}_0x0​和ϵ\\epsilonϵ根据权重αˉ1,αˉ2,...αˉT\\bar{\\alpha}_1,\\bar{\\alpha}_2,...\\bar{\\alpha}_Tαˉ1​,αˉ2​,...αˉT​做weighted sum产生加入噪声后的图像。通常来说，αˉ1\\bar{\\alpha}_1αˉ1​至αˉT\\bar{\\alpha}_TαˉT​是递减的，当在第2步中sample到的ttt越大，则原始图像x0\\mathbf{x}_0x0​对新图像的贡献越大。 ϵθ\\epsilon_{\\theta}ϵθ​是Noise Predictor，其输入是加入噪声的图像以及sample出的ttt，而ϵθ\\epsilon_\\thetaϵθ​训练的Ground Truth就是第3步中sample出的噪声ϵ\\epsilonϵ； 直至噪声预测模型ϵθ\\epsilon_\\thetaϵθ​训练至收敛。","s":"训练过程","u":"/en/docs/Deep-Learning/大模型基础/Diffusion-Model","h":"#训练过程","p":1002},{"i":1009,"t":"从Normal Distribution中sample出图片大小的噪声xT\\mathbf{x}_TxT​； ttt从T,…,1T,\\ldots,1T,…,1范围循环TTT次； 对与每一次以ttt计数的循环，若t>1t>1t>1，则从Normal Distribution中sample出z\\mathbf{z}z，否则z=0\\mathbf{z}=\\mathbf{0}z=0； 根据如下公式得到降噪后的图像： xt−1=1αt(xt−1−αt1−αˉtϵθ(xt,t))+σtz(2)\\mathbf{x}_{t-1}=\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(\\mathbf{x}_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\right)+\\sigma_{t}\\mathbf{z}\\tag{2}xt−1​=αt​​1​(xt​−1−αˉt​​1−αt​​ϵθ​(xt​,t))+σt​z(2) 其中，xt\\mathbf{x}_txt​代表上一步骤中输出的降噪后的图像，xt−1\\mathbf{x}_{t-1}xt−1​代表当前步骤即将输出的降噪后的图像，ϵθ\\epsilon_\\thetaϵθ​代表Noise Predictor预测出的噪声，αˉ1,αˉ2,...αˉT\\bar{\\alpha}_1,\\bar{\\alpha}_2,...\\bar{\\alpha}_Tαˉ1​,αˉ2​,...αˉT​以及α1,α2,...αT\\alpha_1,\\alpha_2,...\\alpha_Tα1​,α2​,...αT​是两组权重序列； 结束本次for循环； 当t=1t=1t=1时，得到x0\\mathbf{x}_0x0​，即最终降噪后的图像。","s":"推理过程","u":"/en/docs/Deep-Learning/大模型基础/Diffusion-Model","h":"#推理过程","p":1002},{"i":1012,"t":"在文字生成模型中根据模型的输入是否与前一时刻的输出有关可以分为自回归AR模型与非自回归NAR模型两种，这两种生成方式的利与弊在图像生成中仍然存在。","s":"回顾文字生成的两种方法","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#回顾文字生成的两种方法","p":1010},{"i":1014,"t":"Transformer-based的文字生成模型有很多，如GPT模型，大多使用自回归（Autoregressive, abbr. AR）的方法逐token生成。 什么是ARM ARM（Autoregressive Model，自回归模型）是一类用于建模时间序列数据的统计模型，其中当前时刻的观测值被认为是过去时刻观测值的线性组合，加上一个随机误差项。这类模型的核心思想是，当前时刻的数据依赖于先前时刻的数据。 若把文字生成的AR方法对应到图像生成中的使用，即一个一个像素生成图像。由于当前对高清图像像素的需求越来越高，自回归的生成方式导致速度非常缓慢，但优点是后面生成的每一个像素都考虑了之前的所有像素，从而使生成的图像更清晰、更细腻、更加符合预期。","s":"自回归方法（AR）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#自回归方法ar","p":1010},{"i":1016,"t":"若使用NAR非自回归的方法一次生成所有像素，各像素在生成时无法考虑之间的语义信息，生成的图像质量普遍低于自回归方法生成的图像。","s":"非自回归方法（NAR）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#非自回归方法nar","p":1010},{"i":1018,"t":"VAE、GAN以及Diffusion Model等生成模型，都不只是单独使用文字作为输入来生成图像，而是使用了从已知的随机分布（e.g. Normal Distribution）中sample出向量作为模型额外输入的方法。 大致的思想如下图所示，由于期待生成的图像并不是固定的，可以将预期输出看作是一个分布，即P(x∣y)P(x|y)P(x∣y)，而图像生成模型需要完成的任务就是将输入的从某一随机分布中sample出的向量对应到图像预期输出分布中的某一个图像。 important 总结：由于根据文字prompt期待生成的图像并不是固定的，可以认为生成的图片在目标域（Target Domain）符合某种分布。因此目前的SOTA模型除了将文字Prompt作为输入，还从某随机分布中sample出图片shape的随机向量（矩阵）作为输入，期待模型根据prompt将源域（Source Domain）输入的随机向量映射到目标域的分布，生成对应的图片。","s":"目前图像生成模型的共同点","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#目前图像生成模型的共同点","p":1010},{"i":1020,"t":"Stable Diffusion是目前图像生成的SOTA模型之一，在本章中我们快速的了解一下Stable Diffusion的大致框架以及原理。","s":"生成模型的共同结构","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#生成模型的共同结构","p":1010},{"i":1022,"t":"目前，如Stable Diffusion等SOTA图像生成模型都具备以下所示的三个模块，通常情况下这三个模块分开训练，最终通过特殊的逻辑和规则组合在一起。 Text Encoder：根据输入的text prompt进行嵌入表示 Generation Model：接受Text Encoder输出的prompt表示以及从随机分布sample出的图像大小的向量，得到“中间产物”，中间产物有以下两种情况： 具有视觉意义但经过压缩比较模糊的图像 不具备视觉特征的矩阵（Latent Representation） Decoder：以上述的“中间产物”作为输入，生成出高清图像 通用框架的三个组成部分如下图所示： 再附上Stable Diffusion、DALL-E系列以及Google的Imagen的结构说明。 其中Imagen将压缩版本的图片作为Generation Model的中间产物，Stable Diffusion以及DALL-E将Latent Representation作为中间产物。 根据Imagen的实验结果，相对于Decoder即Diffusion Model的模型大小，Text Encoder的模型大小对图像生成模型的影响是非常大的。Text Encoder可以帮助模型理解prompt中在训练资料的文字-图像对中没有出现的新的词汇，从而提高图像生成的表现。 Scaling text encoder size is more important than U-Net size. While scaling the size of the diffusion model U-Net improves sample quality, we found scaling the text encoder size to be significantly more impactful than the U-Net size. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding","s":"通用框架概览","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#通用框架概览","p":1010},{"i":1024,"t":"下面介绍两种用于评估图像生成模型的常用Benchmark：FID与CLIP Score。 FID（Fréchet Inception Distance）​ FID提供一个Pre-trained的CNN，该CNN通常使用预训练的Inception v3模型。在计算FID时，生成图像和真实图像分别输入到预训练的CNN中，提取出各自的特征表示向量（Representation）。这两个Representation越接近，代表输出的图像越像预期的“真实”图片。 在FID中，做出了如下重要的假设：将生成的图像真实的图像经过CNN输出的Representation看作是sample自两个高斯分布的随机变量。然后，通过计算两个特征向量的均值和协方差矩阵来得到两个高斯分布的参数。最后，利用两个高斯分布之间的Fréchet距离来衡量生成图像与真实图像之间的差异。 FID=∥μ1−μ2∥22+tr(Σ1+Σ2−2(Σ1Σ2)12)(1)\\mathrm{FID}=\\left\\|\\mu_1-\\mu_2\\right\\|_2^2+\\mathrm{tr}\\left(\\Sigma_1+\\Sigma_2-2\\left(\\Sigma_1\\Sigma_2\\right)^{\\frac12}\\right)\\tag{1}FID=∥μ1​−μ2​∥22​+tr(Σ1​+Σ2​−2(Σ1​Σ2​)21​)(1) 其中，μ1\\mu_1μ1​和μ2\\mu_2μ2​分别是第一个和第二个高斯分布的均值向量；Σ1\\Sigma_1Σ1​和Σ2\\Sigma_2Σ2​则是它们的协方差矩阵；tr(⋅)\\mathrm{tr}(\\cdot)tr(⋅)表示矩阵的迹运算。 高斯分布的均值向量从观测到的数据中计算出来的。对于一个nnn-维高斯分布，其均值向量可以表示为一个长度为nnn的列向量，其中的每一个元素都是一个特定维度的平均数，这可以通过在每个维度上进行简单的算术平均来完成。 值得注意的是，FID指标需要一定数量的生成图像和真实图像来进行统计估计。这是因为FID的计算是基于两个高斯分布之间的距离计算的，因此需要足够多的样本数量才能够获得较为准确的概率分布估计。 CLIP Score​ CLIP Score中的CLIP指的就是OpenAI的CLIP（Contrastive Language-Image Pre-Training）模型。 具体来说，CLIP Score的计算方式是将用于生成图像的文字prompt输入至CLIP的Text Encoder中得到一个Representation，再将对应prompt生成的图像输入至CLIP的Image Encoder中得到对应的Representation，计算二者之间的距离，即得到CLIP Score。分数越小，代表文字和图像更align。","s":"Benchmark","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#benchmark","p":1010},{"i":1026,"t":"Generation Model​ Generation Model的生成过程其实就是Denoise的过程。具体来讲，输入文字Prompt以及从随机分布中sample出的与预期生成图像具有相同大小的噪声矩阵，预测出输入图片中的噪声分布，在输入图像中减去噪声，输出去噪后的图像。Generation Model的最终输出是中间产物，这个中间产物可以是图像的压缩版本，也可以是一个Latent Representation。因此，训练Generation Model其实就是训练一个Noise Predictor。 中间产物是压缩图像​ 当Generation Model的中间产物是压缩图像时，如Diffusion模型，在训练Generation Model时的训练资料可以通过对数据集中的原始图片添加与图像大小一致地从已知随机分布中sample出的噪声来获得。此时加入噪声后的图像可以作为压缩图像输入至Noise Predictor中，而需要预测出的噪声分布的Ground Truth就是sample出的噪声。 中间产物是Latent Representation​ 中间产物是Latent Representation时，同样采取从已知随机分布中sample出噪声再添加到网络的输入作为生成Ground Truth的策略，但是还额外需要一个Encoder来产生Latent Representation。 这里的Encoder使用数据集中的图片（即期待模型最终输出的图片）作为输入，输出该图片的某种Latent Representation，经过从随机分布中sample出的噪声的加入，输入至Noise Predictor中。从随机分布中sample出的噪声就是Noise Predictor的Ground Truth。 Decoder​ Generation Model的训练需要大量成对的（Pair）文字-图像资料。而对于Decoder来说，它的输入是中间产物（即Generation Model生成的压缩的图片或Latent Representation），输出的是还原出的高分辨率的图像，它的训练是不需要额外pair的文字-图像资料。 中间产物是压缩图像​ 当Generation Model的输出是压缩版本的图像时，Decoder的训练资料可以将从互联网上fetch到的图像作为label，并对这些图像做Down Sampling来获得压缩版本的图像作为Decoder训练时的输出。 中间产物是Latent Representation​ 当中间产物是Latent Representation时，需要训练一个Auto-Encoder，使用Encoder-Decoder的结构训练生成模型的Decoder。 具体来讲，向Encoder中输入数据集中的高清预期图片，Encoder将其转换为某种Latent Representation，Decoder再吃Encoder的输出，最终输出还原出的高清label图片，训练的方向是让输出的图片与输入的图片越接近越好。在这个过程中，不需要额外的标注，Auto-Encoder和生成模型的Decoder一起更新参数。","s":"通用框架解析","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#通用框架解析","p":1010},{"i":1028,"t":"在这个模块大致介绍目前常见的几种图像生成模型，其中Diffusion Model以及GAN将在以后的文章中详细讲解。","s":"常见图像生成模型速览","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#常见图像生成模型速览","p":1010},{"i":1030,"t":"变分自编码器（Variational Auto-Encoder, abbr. VAE）的训练策略是使用Encoder将输入图像对应（嵌入）到一个符合某随机分布的向量，再将该向量作为Decoder的输入，加上文字prompt后，期待模型产生合适的图像。 tip VAE在训练过程中，期待Ecoder输入多张图片后，输出的向量在一起符合某个随机分布（e.g. Normal Distribution），并不是Encoder直接输出一个Distrubution。","s":"变分自编码器（VAE）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#变分自编码器vae","p":1010},{"i":1032,"t":"基于流的生成模型采用特殊的网络结构的设计，将Encoder设计为可逆的（invertible），在训练阶段喂入多张图片，期待模型的向量符合某个随机分布。而在预测阶段，由于Encoder是可逆的，输入从该随机分布中sample出来的向量，期待输出对应的图像。 注意，由于Encoder是可逆的，在训练阶段其输入的图片矩阵的形状应该等于输出的随机分布向量的形状，在推理阶段亦然。","s":"基于流的生成模型（Flow-Based Generative Model）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#基于流的生成模型flow-based-generative-model","p":1010},{"i":1034,"t":"GAN模型的结构分为Generator和Discriminator，其中Generator接受来自随机分布的向量，产生预期图像；Discriminator接受生成器输出的图像或真实图像，输出输入的图像是真实图像的概率。在训练过程中，通过固定生成器参数来更新辨别器参数、固定辨别器参数更新生成器参数的往复交替训练来形成“两个网络对抗”的效果，从而使得生成器生成的图像更逼真（与输入的真实图像更近似）、辨别器识别是否是输入的真实图像的精确度更高。","s":"生成对抗网络（GAN）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#生成对抗网络gan","p":1010},{"i":1036,"t":"扩散模型的核心思想是对输入的图片加入噪声使其成为从某一随机分布sample出的向量，并在这个过程中训练出Noise Predictor；在生成图片时，输入从该随机分布中sample出的向量，使用训练出的Noise Predictor对噪声denoise从而获得生成的图片。 以DDPM（Denoising Diffusion Probabilistic Models）模型为例，模型在denoise时为每个denoise步骤赋予一个编号，越早进行denoise的步骤编号越大，因此，这个编号也代表着图像中噪声的严重程度。在Denoise模块中，模型根据输入的带有噪声的图片、文字prompt以及噪声的严重程度（即denoise的步骤）预测出该图片中噪声的分布，然后将输入的图片中减去预测出的噪声得到denoise后的图片。 Denoise模块的目标是预测出输入的噪声图片中的噪声，其资料可以通过对数据集中的图片不断加入从Gaussian Distribution中sample出的噪声的方法来获得，这个加噪声的过程我们称为Forward Process or Diffusion Process。此时将加入噪声后的图片、文字prompt以及denoise的步骤序号作为输入，sample出的噪声作为Ground Truth对noise predictor进行训练。","s":"扩散模型（Diffusion Model）","u":"/en/docs/Deep-Learning/大模型基础/Image-Generation-Models","h":"#扩散模型diffusion-model","p":1010},{"i":1038,"t":"本篇论文主要基于 IPL 的思想实现。本仓库大部分从 IPL-Zero-Shot-Generative-Model-Adaptation fork 而来并做出了一定修改。","s":"本科毕业论文：基于 Prompt Learning 的视觉-语言大模型在图像生成中的应用与研究","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"","p":1037},{"i":1041,"t":"conda create -n ipl python=3.8 conda activate ipl","s":"创建 Anaconda 虚拟环境","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#创建-anaconda-虚拟环境","p":1037},{"i":1043,"t":"请确保 NVIDIA 驱动、CUDA 以及 PyTorch 之间版本互相匹配。 conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia pip install ftfy regex tqdm ninja pip install git+https://github.com/openai/CLIP.git","s":"安装依赖","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#安装依赖","p":1037},{"i":1045,"t":"预训练的源域生成器可以通过 Google Drive 或者 Tsinghua Cloud 下载，并将其置于 ./pre_stylegan 文件夹中。","s":"下载预训练生成器","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#下载预训练生成器","p":1037},{"i":1049,"t":"ctx_init 参数用于初始化 prompts，官方提供的演示 context 是a photo of a。 source_prompts = [prompt_prefix + \" \" + args.source_class] target_prompts = [prompt_prefix + \" \" + args.target_class] 源域的初始提示词 source_prompts 是 ctx_init 与源域标签的组合。若源域标签为 photo，则源域的初始提示词是 a photo of a photo。目标域的初始提示词同理。","s":"prompts 的初始化","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#prompts-的初始化","p":1037},{"i":1051,"t":"源域以及目标域的初始提示词接下来会进行 tokenize： source_tokenized_prompts = torch.cat([clip.tokenize(p) for p in source_prompts]).to(device) # (1, 77) 'sot a photo of a photo eot' 在经过tokenize后为tensor [[49406, 320, 1125, 539, 320, 1125, 49407, etc]] # 77是CLIP在tokenize方法中缺省的context_length，超过context_length将被truncate，不足的将用0补齐 target_tokenized_prompts = torch.cat([clip.tokenize(p) for p in target_prompts]).to(device) # (1, 77) 'sot a photo of a disney' 在经过tokenize后为tensor [[49406, 320, 1125, 539, 320, 4696, 49407, etc]] # 77是CLIP在tokenize方法中缺省的context_length，超过context_length将被truncate，不足的将用0补齐 tokenize 是 CLIP 对送入的 prompt 字符串进行标记化处理，在头部和尾部添加 startoftext 以及 endoftext 标记，最终为两个首尾标记和全部单词生成 int 标记。其中 CLIP 模型缺省的 context_length 是77，若 prompt 大于 77 会进行截断（truncate），若小于 77 会进行补零，因此 source_tokenized_prompts 与 target_tokenized_prompts 的形状均为 (1, 77)。 在提示词标记化之后，将进行嵌入表示 embedding： source_embedding = clip_model.token_embedding(source_tokenized_prompts).type(clip_model.dtype) # (1, 77, 512) 其中512是CLIP中的n_dim，token_embedding层的词嵌入的维度 target_embedding = clip_model.token_embedding(target_tokenized_prompts).type(clip_model.dtype) # (1, 77, 512) 其中512是CLIP中的n_dim，token_embedding层的词嵌入的维度","s":"prompts 的 tokenize 与 embedding","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#prompts-的-tokenize-与-embedding","p":1037},{"i":1053,"t":"在 Mapper 生成 prompts 后进行 prompts 的特征提取时，需要传入 tokenize 之后的人工初始化 prompt（‘a photo of a photo.’或‘a photo of a disney.’），用于选择 eot 符号对应的维度来进行特征投影（因为 eot 作为整个句子的结尾，被认为该维度包含更多的信息。具体做法：由于在 tokenize 之后，eot 符号对应的维度的值最大，因此可使用 argmax 来定位），以保证最后得到的特征形状与图像特征提取的输出形状相同，使得后续可以进行对比学习的损失计算。","s":"compute_text_features 的实现细节","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#compute_text_features-的实现细节","p":1037},{"i":1055,"t":"Z空间与W空间​ # Z空间到W空间的变换 sample_z = mixing_noise(args.batch_mapper, 512, args.mixing, device) # (batch_size, 512) sample_w = net.generator_frozen.style(sample_z) # (batch_size, 512) Z 空间和 W 空间是 StyleGAN 模型中两种不同的隐变量空间，分别用于控制生成图像的随机特征和样式信息。W 空间通过对 Z 空间的映射得到。 Z 空间（Latent Space Z）： Z 空间是随机噪声空间，通常由随机噪声向量组成，表示了图像的随机特征。 在 StyleGAN 中，Z 空间的维度通常为 512 维。这意味着一个 Z 向量由 512 个数字组成，每个数字表示了图像的一个随机特征的强度或者方向。 W 空间（Style Space W）： W 空间经过特征解耦的隐空间，与 Z 空间相比更加解耦合。 在 StyleGAN 中，W 空间的维度也通常为 512 维，是通过mapping network进行映射得到的，mapping network 由 PixelNorm 层与 EqualLinear 层构成。以下代码节选自sg2_model.py： '''mapping network''' layers = [PixelNorm()] for i in range(n_mlp): layers.append( EqualLinear( style_dim, style_dim, lr_mul=lr_mlp, activation=\"fused_lrelu\" ) ) self.style = nn.Sequential(*layers) Z 空间与 W 空间的关系： 在 StyleGAN 中，通常会先将一个 Z 向量映射到 W 空间，然后再将 W 向量输入到生成器网络中生成图像。 Z 空间提供了初始随机噪声，而 W 空间则通过特征解耦提供更多控制图像风格的灵活性。通过对 Z 和 W 之间的映射以及 W 在生成器中的应用，StyleGan 实现了高度可控且具有良好生成效果的图像合成。 损失函数​ 在代码中，stage 1 的损失函数是 global_clip_loss，该损失由三部分组成： 对比学习损失：Mapper 生成的源域 prompts 的特征**（注意，这里的 prompts 特征是与人工初始化的 prompts 的特征做过 element-wise 相加后的特征）**与源域图像特征的余弦相似度组成的对比学习损失； 目标域正则化损失：Mapper 生成的目标域 prompts 的特征与目标域文本标签特征的余弦相似度，这里生成的目标域 prompts 特征同样也是与人工初始化的 prompts 做过加法的。注意该损失有权重 lambda_l。 源域正则化：计算生成的源域prompts与源域标签之间的余弦相似度，由 lambda_src 控制，默认是 0。","s":"训练 stage 1","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#训练-stage-1","p":1037},{"i":1057,"t":"确定目标域生成域需要更新的层​ 在训练的第二阶段进行前向传播时，需要先对目标域生成器（generator_trainable）的所有层进行 unfreeze，然后对更新哪些层做出选择，承担选择任务的功能函数：model.ZSSGAN.ZSSGAN.determine_opt_layers，最后 freeze 所有层后再 unfreeze 选择的网络层。 if self.training and self.auto_layer_iters > 0: self.generator_trainable.unfreeze_layers() # unfreeze train_layers = self.determine_opt_layers() # layer to train if not isinstance(train_layers, list): train_layers = [train_layers] self.generator_trainable.freeze_layers() self.generator_trainable.unfreeze_layers(train_layers) # unfreeze 具体选择带更新网络层的策略： 将 W 空间的隐向量送入目标域生成器（SG2Generator）中，并进行反向传播，此时可以通过反向传播后 W 空间隐向量不同维度的更新幅度来衡量不同网络层的影响力，因此选出更新幅度最大的维度就可以确定在 Model Adaption 中需要更新的网络层。 之所以 W 空间编码在 n_latent 维度上的序号就代表着对应的网络层数的序号，是因为 StyleGAN 生成器的结构决定了这一点：StyleGAN 生成器中，W 空间编码的不同维度会被送入生成器网络的不同层，控制这些层的特征映射 (feature mapping)。具体来说，W 空间编码的每个维度会被重复 n_latent 次，作为该层的风格向量 (style vector)，通过 AdaIN (Adaptive Instance Normalization) 层控制该层的特征映射。因此，W 空间编码的第 i 个维度会影响生成器网络中第 i 层的特征映射。当某个维度的 W 值被更新的程度较大时，就意味着该维度对应的层在生成目标图像时起到了重要作用，需要被优化。 损失函数​ stage 2 的损失函数是 CLIP Loss 类中的 clip_directional_loss，该损失函数由两部分组成： edit_direciton：源域生成器与目标域生成器生成的图片在经过 image encdoer 后做 element-wise 的相减，最后除以自身的 L2 Norm 方便后续与 target_direction 计算余弦相似度。 target_direction：Mapper 产生的源域和目标域 prompts 的 text_features 做element-wise相减后，最后初一自身的 L2 Norm 以便后续与 edit_direction 计算余弦相似度。","s":"训练 stage 2","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#训练-stage-2","p":1037},{"i":1059,"t":"测试所用 nada 权重 Google Drive 链接：StyleGAN-NADA Models 参考文献：GAN 的几种评价指标 Inception Score（IS） 评估图像的质量和多样性 质量：把生成的图片 xxx 输入 Inception V3 中，得到输出 1000 维的向量 yyy，向量的每个维度的值对应图片属于某类的概率。对于一个清晰的图片，它属于某一类的概率应该非常大，而属于其它类的概率应该很小。用专业术语说， p(y∣x)p(y|x)p(y∣x) 的熵应该很小（熵代表混乱度，均匀分布的混乱度最大，熵最大）。 多样性： 如果一个模型能生成足够多样的图片，那么它生成的图片在各个类别中的分布应该是平均的，假设生成了 10000 张图片，那么最理想的情况是，1000 类中每类生成了 10 张。转换成术语，就是生成图片在所有类别概率的边缘分布 p(y)p(y)p(y) 熵很大（均匀分布）。 因此，对于 IS 我们需要求的两个量就是 p(y∣x)p(y|x)p(y∣x) 和 p(y)p(y)p(y)。实际中，选取大量生成样本，用经验分布模拟 p(y)p(y)p(y)： p^(y)=1N∑i=1Np(y∣x(i))\\hat{p}(y)=\\frac{1}{N}\\sum_{i=1}^{N}p(y|\\mathbf{x}^{(i)})p^​(y)=N1​∑i=1N​p(y∣x(i)) Inception Score 的完整公式如下： IS=exp⁡(Ex[KL(p(y∣x)∣∣p(y))])IS=\\exp\\left(\\mathbb{E}_x[KL(p(y|x)||p(y))]\\right)IS=exp(Ex​[KL(p(y∣x)∣∣p(y))]) 其中 Ex\\mathbb{E}_xEx​ 表示对所有图像的期望，KL(p(y∣x)∣∣p(y))KL(p(y|x)||p(y))KL(p(y∣x)∣∣p(y)) 表示每张图像的 KL 散度，exp⁡\\expexp 表示取指数。 通常计算 Inception Score 时，会生成 50000 个图片，然后把它分成 10 份，每份 5000 个，分别代入公式计算 10 次 Inception Score，再计算均值和方差，作为最终的衡量指标（均值±方差）。但是 5000 个样本往往不足以得到准确的边缘分布 p(y)p(y)p(y)，尤其是像 ImageNet 这种包含 1000 个类的数据集。 StyleGAN-nada 以及 IPL 在经过 batch_size 为 2，iteration 为 300 的训练后（其中 IPL 的 Mapper 是以 batch_size 为 32，iteration 为 300 进行训练的），二者的 IS 分别为 (2.2960, 0.2042) 以及 (2.6420, 0.1959)。 Fréchet Inception Distance（FID） 评估目标域的风格 计算 IS 时只考虑了生成样本，没有考虑真实数据，即 IS 无法反映真实数据和样本之间的距离，IS 判断数据真实性的依据，源于 Inception V3 的训练集 ImageNet，在 Inception V3 的“世界观”下，凡是不像 ImageNet 的数据，都是不真实的，都不能保证输出一个 sharp 的 predition distribution。因此，要想更好地评价生成网络，就要使用更加有效的方法计算真实分布与生成样本之间的距离。 FID 距离计算真实样本，生成样本在特征空间之间的距离。首先利用 Inception 网络来提取特征，然后使用高斯模型对特征空间进行建模，再去求解两个特征之间的距离，较低的 FID 意味着较高图片的质量和多样性。 StyleGAN-nada 以及 IPL 在经过 batch_size 为 2，iteration 为 300 的训练后（其中 IPL 的 Mapper 是以 batch_size 为 32，iteration 为 300 进行训练的），二者的 FID 分别为 84 以及 58。 Single Image Fréchet Inception Score（SIFID） FID 测量生成的图像的深层特征分布与真实图像的分布之间的偏差。在 ICCV 2019 Best Paper 中提出了 SIFID，只使用一张真实目标域的图像。与 FID 不同，SFID 不使用 Inception Network 中最后一个池化层之后的激活矢量（每个图像一个向量），而是在第二个池层之前的卷积层输出处使用深层特征的内部分布（feature map 中每个位置一个向量）。最终 SIFID 是真实图像和生成的样本中这些特征的统计数据之间的 FID。 Structural Consistency Score（SCS） 评估图像的结构保存能力 Identity Similarity（ID） 评估图像的特征保存能力 定量分析结果​ IS（Inception Score）↑ 数据集 源域→目标域 NADA IPL IPL* FFHQ Photo→Disney 2.296 2.642 2.701 FFHQ Photo→Anime Painting 2.320 2.464 2.578 FFHQ Photo→Wall painting FFHQ Photo→Ukiyo-e 2.489 2.715 2.851 FFHQ Photo→Pixar character FFHQ Photo→Tolkien elf FFHQ Photo→Werewolf 2.173 2.482 2.517 AFHQ Photo→Cartoon AFHQ Photo→Pointillism AFHQ Photo→Cubism SFID（Single Fréchet Inception Distance）↓ 数据集 源域→目标域 NADA IPL IPL* FFHQ Photo→Disney 84 58 54 FFHQ Photo→Anime Painting FFHQ Photo→Wall painting FFHQ Photo→Ukiyo-e FFHQ Photo→Pixar character FFHQ Photo→Tolkien elf FFHQ Photo→Werewolf AFHQ Photo→Cartoon AFHQ Photo→Pointillism AFHQ Photo→Cubism","s":"定量分析指标","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#定量分析指标","p":1037},{"i":1062,"t":"新增了对自定义图像进行风格迁移的功能。 HyperStyle 中的 e4e encoder 将自定义的真实图像编码至 StyleGAN2 中的 W 空间生成 latent codes，再将其分别输入至源域生成器以及目标域生成器以代替原始的从正态分布中 sample 出的随机向量生成的 w_codes，从而得到相应的图片。其中 e4e encoder 来源于 HyperStyle 提供的预训练 checkpoint。 使用方法：运行 inference.py，设置对应的参数，如生成器以及 e4e encoder 的路径、图像路径等，最后运行即可。 修改日志​ 第一次尝试只加载了 w_encoder 类及其对应 checkpoint 参数，导致并未将真实图片编码到 StyleGAN 的 W 空间中，没有 inversion 出合理的结果。 第二次尝试使用了 restyle_e4e_encoder，但是没有使用 dlib 进行 alignment，也没有使用 restyle 模型在反演时使用的多次进行前向传播来修正 latent code 的策略。此次尝试虽然反演出了合理的人像，但是人像的特征保存能力非常弱。 第三次尝试解决了上一次发现的问题，加入 dlib 提供的 landmark 检测以实现 alignment，并且使用 run_loop 函数在 restyle_e4e_encoder 中进行多次前向传播以修正得到的 W 空间的 latent code，效果较好。 对比 pSp 和 e4e encoder，pSp 对人脸图像的还原能力较强，但是会导致目标域图像具有随机的彩色光晕。","s":"支持自定义图像的风格迁移","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#支持自定义图像的风格迁移","p":1037},{"i":1064,"t":"参考 MIT 开源项目 pytorch-deployment 进行生成模型的 Web UI 部署。参考项目使用的是 StarGANv2 模型，对其进行优化使得其可以部署 StyleGAN 模型。 分别对人像和宠物图像生成了两个单独的卡片和 HTML 网页，网页可以完成两种功能： 使用参考图像进行零样本跨域适应，同时可以在网页下拉框中选择预期的目标域风格（由于没有合适的 restyle encoder，宠物图像不支持选择参考图像） 直接使用随机数生成源域图像并进行零样本跨域适应 UI 独立代码可以参考本人仓库 stylegan-ui，但功能有限，完整的 UI 代码已经合并到主程序中，请参考 ./web_ui 中的具体代码。 部分效果展示图​ 主页： 人物画像的零样本域适应（初始状态）： 人物画像的零样本域适应（使用参考图像生成状态）： 宠物画像的零样本域适应（初始状态）： 宠物画像的零样本域适应（使用随机数生成状态）：","s":"Web UI","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#web-ui","p":1037},{"i":1067,"t":"Mapper 的作用是从 W 空间的隐式代码中学习出符合源域图片特征以及符合目标域文字特征的 prompts。 改进后的 Mapper 结构： class TransformerMapperV2(nn.Module): \"\"\" 改良版transformer mapper，增加多头注意力，减小transformer encoder的层数，防止学习到的源域图像细节过拟合 同时去掉开头的PixelNorm，防止与transformer中的layer normalization冲突 并在transformer encoder之后加入Pixel Norm以及全连接层 \"\"\" def __init__(self, opts, n_dim): super(TransformerMapperV2, self).__init__() self.opts = opts self.n_dim = n_dim layers = [] # transformer中有layer normalization，不需要进行PixelNorm # 自定义Transformer编码器层配置 transformer_layer = TransformerEncoderLayer(d_model=512, nhead=4, dim_feedforward=1024, dropout=0.1) # 构建Transformer编码器 self.transformer_encoder = TransformerEncoder(transformer_layer, num_layers=2) layers.append(self.transformer_encoder) # 再过一次PixelNorm以及全连接层，将每个点归一化（除以模长），避免输入noise的极端权重，改善稳定性 layers.append(PixelNorm()) self.linear = EqualLinear(512, 512, lr_mul=0.01, activation='fused_lrelu') layers.append(self.linear) # 最后一个全连接层，输出维度保持不变 self.final_linear = EqualLinear(512, n_dim * opts.n_ctx, lr_mul=0.01, activation='fused_lrelu') layers.append(self.final_linear) self.mapping = nn.Sequential(*layers).to(device)","s":"改进：Mapper 结构的设计","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#改进mapper-结构的设计","p":1037},{"i":1069,"t":"在 IPL 的官方代码实现中，人工设计的 prompts 有两处，一是 ctx_init，由命令行参数赋值，即 \"a photo of a\"，另一处是 utils/text_templates.py 中的 templates，下面分别分析这两处的具体作用。 ctx_init 的作用（与域标签拼接后的 ctx_init）​ ctx_init 在 compute_text_features 函数中用于定位 eot 层符号所表示的维度来进行投影，使得文字特征与图像特征维度相同，并不参与 text_features 的实际计算。但是在该函数中，Mapper 输出的 image-specific prompts 已经与域标签的嵌入表示进行了 concat。 在 stage 1 训练 Mapper 损失函数中，Mapper 学习到的 image-specfic prompts 在与源域标签进行 concat 并得到文字编码后，会与 ctx_init 的文字编码进行 element-wise 的相加，最后再与源域生成器输出的图片的图像编码进行对比损失计算； 同理，在 stage 2 训练目标域生成器时，Mapper 输出的 image-specific prompts 在分别与源域、目标域标签 concat 后送入文字编码器得到文字特征，再与 ctx_init 的文字特征进行 element-wise 相加，最后二者相减得到 text_direction。 templates 的作用​ templates 是提前准备好的一系列字符串，其中字符串的格式全部类似于 a photo of a {}. 原始 hhfq 数据集的模板共有 79 个字符串。 与 ctx_init 起作用的函数不同，templates 在第一阶段的训练的 domain regularization loss 中使用到的 get_text_features 函数起作用，用于与目标域标签进行格式化连接后成为 image-specific prompts 向目标域靠近的方向。即 domain loss 使学习到的 prompts 向以目标域标签为中心的字符串对齐。 思考​ IPL 方法对 Mapper 学习到的 prompts 除了（1）使用对比学习使 prompts 学习到源域图片的特征以及（2）使用域正则化使得 prompts 向目标域标签对齐之外，并没有使用其他与人工设计的 prompts 有关的正则化方式来约束 prompts 的学习，因此人工设计的 prompts 可能并没有起到太大的约束作用。 如果对比学习损失是为了让 Mapper 自监督学习到图片的特征外，那么是否可以对域正则化损失进行改进，约束学习到的 prompts 向人工设计的初始化 prompts 对齐，以实现类似于 Stable Diffusion 类似的 prompts 控制图像生成的效果。","s":"问题：训练阶段人工 prompts 的作用是什么？","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#问题训练阶段人工-prompts-的作用是什么","p":1037},{"i":1071,"t":"对第一阶段的损失函数做出修改，更新domain loss，将原始 domain loss 中使用的以目标域标签为中心的模板更换成自定义模板，使目标域的image-specific prompts与自定义模板对齐。 经过多次实验和分析，刻意让 Mapper 输出的image-specific prompts 去逼近用户设置的 prompts，会产生一些隐式细节的丢失。因为 Mapper 本身存在的目的就是学习出人工无法准确描述的细节（包括源域图像的自身细节以及目标域风格的细节），如果对 Mapper 的损失函数中加上太多人为设计的限制，很显然会造成细节的丢失并且出现同质的现象。 因此，为了达到既使用精心设计的 prompts 来优化域适应，同时又不影响 Mapper 自主学习双域特征，在原有两个损失函数的基础上，新增一个权重较小的损失函数，用于将 Mapper 学习到的目标域 prompts 向自定义模板对齐。 用于生成 prompts 的 GPT、Claude prompts​ 中文提示词： 针对将普通人像转换成迪士尼风格人物画像的任务，给出60个描述迪士尼人像特有特征的文字prompt。 将上述生成的60个prompts放在同一个Python列表中，即每一个prompt作为该列表的字符串元素，输出整个Python列表。 英文提示词： For the task of converting a {source class} photo into a {target_class} photo, provide some text prompts describing the distinctive features of Disney character portraits. Put the generated 60 prompts into the same Python list, with each prompt as a string element of the list, and output the entire Python list. 对 global_clip_loss 的改进​ IPL 训练第一阶段的损失函数除了源域 prompts 与源域图像之间的对比学习损失函数外，还有将目标域 prompts 与目标域标签计算余弦相似度的 domain regularization。 对 domain regularization 进行改进，引入开发者自定义的 prompts，约束 Mapper 学习到的目标域 prompts 向开发者自定义的 prompts 对齐，以此来进行 prompt tuning，发挥 prompt learning 的更大优势，并增强自定义性。 对 clip_directional_loss 的改进​ IPL 训练第二阶段的损失函数，使用 criteria.clip_loss.CLIPLoss.clip_directional_loss。","s":"改进：使学习到的 prompts 向用户自主设计的 prompts 模板对齐","u":"/en/docs/Deep-Learning/大模型基础/Prompt Learning/Undergraduate-Dissertation","h":"#改进使学习到的-prompts-向用户自主设计的-prompts-模板对齐","p":1037},{"i":1073,"t":"tip 在自监督学习的模型中，出现了很多以芝麻街任务命名的经典模型和论文。","s":"自监督学习（Self-Supervised Learning）","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"","p":1072},{"i":1075,"t":"自监督学习是无监督学习的一种方法，利用未标记的数据来训练模型。与传统的监督学习不同，自监督学习不需要依赖人工标注的标签数据，而是通过自动构建任务来生成伪标签，从而指导模型的学习。 自监督学习的基本原理是，通过对输入数据进行某种变换或操作，使得模型能够从中提取有用的特征和语义信息。例如，在自然语言处理领域，一种常见的自监督学习任务是预测下一个单词；在计算机视觉领域，一种常见的自监督学习任务是预测图像中的缺失部分。这些任务可以帮助模型学习到输入数据中的潜在结构和规律，从而提高其泛化能力和性能。","s":"介绍","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#介绍","p":1072},{"i":1077,"t":"下面以BERT为例，介绍自监督模型。","s":"BERT（Bidirectional Encoder Representation from Transformers）","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#bertbidirectional-encoder-representation-from-transformers","p":1072},{"i":1079,"t":"BERT的结构其实是Transformer的Encoder部分，仅使用Encoder做特征抽取器。 BERT（Bidirectional Encoder Representations from Transformers）本身是一种预训练的模型架构，通常是在大规模无标签数据上进行预训练，然后在特定任务上进行微调。BERT并不是一个用于特定任务的模型，而是一个通用的语言表示模型。 使用 BERT 的一般步骤包括： 预训练（Pretraining）：在大规模无标签数据上对 BERT 进行预训练，学习通用的语言表示。 微调（Fine-tuning）：将预训练的 BERT 模型应用于特定任务，并在有标签的数据上进行微调，以适应该任务。 应用于下游任务（Downstream Tasks）：微调后的 BERT 模型可以被用于执行特定的下游任务，如文本分类、命名实体识别等。","s":"结构","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#结构","p":1072},{"i":1081,"t":"Masking Input​ BERT模型的自监督性质主要体现在其训练数据并不需要人为标注label，而是通过对输入句子中的部分词汇做mask，将输入数据的部分内容使用special token或random token进行遮挡后，喂入Encoder中。对于每个被mask掉的词汇，BERT输出一个概率分布向量，表示这个词汇属于词汇表中的哪一个。 BERT的损失函数主要是Masked Language Model（MLM）任务的交叉熵损失，通过最小化Encoder输出的概率分布与Ground Truth之间的交叉熵损失函数来训练模型。 L=−1N∑i=1Nyilog⁡(pi)(1)L=-\\frac1N\\sum_{i=1}^Ny_i\\log(p_i)\\tag{1}L=−N1​i=1∑N​yi​log(pi​)(1) 其中： NNN是输出的概率分布向量的维度。 yiy_{i}yi​是概率分布向量标签。 pip_{i}pi​是模型预测的概率分布向量。 Next Sentence Prediction​","s":"Self-Supervised Pretraining","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#self-supervised-pretraining","p":1072},{"i":1083,"t":"在预训练之后，BERT 的模型参数可以被用于多个下游任务，如文本分类、命名实体识别、问答等。 首先，我们先来了解一下NLP任务中很重要的一个Benchmark：GLUE。 Benchmark: GLUE​ GLUE（General Language Understanding Evaluation）是一个评估自然语言处理模型在多个任务上综合性能的基准（benchmark）。它旨在测试模型对各种语言任务的通用理解能力。GLUE benchmark 包含了多个任务，每个任务都有一个对应的数据集和评估标准。 MNLI（MultiNLI）：自然语言推理任务，要求模型判断给定的两个句子之间的关系是蕴含、矛盾还是中立。 QQP（Quora Question Pairs）：问题匹配任务，要求模型判断两个问题是否语义上等价。 QNLI（Question-answering Natural Language Inference）：句子分类任务，要求模型判断给定问题和句子之间的关系。 RTE（Recognizing Textual Entailment）：文本蕴涵任务，要求模型判断给定的两个文本之间是否存在蕴涵关系。 STS-B（Semantic Textual Similarity Benchmark）：语义文本相似度任务，要求模型度量两个文本之间的语义相似度。 CoLA（Corpus of Linguistic Acceptability）：语言可接受性判断任务，要求模型判断一个句子是否语法上正确。 MRPC（Microsoft Research Paraphrase Corpus）：短语匹配任务，要求模型判断两个句子是否语义上等价。 SST-2（Stanford Sentiment Treebank）：情感分类任务，要求模型判断给定句子的情感极性。 WNLI（Winograd NLI）：自然语言推理任务，属于 Winograd 模式的变体，要求模型判断一个给定的句子对是否存在蕴含关系。 GLUE 提供了一个全面的测试平台，有助于评估和比较不同自然语言处理模型在多个任务上的性能。 Downstream Tasks​ Sentiment Analysis​ BERT作为自监督的预训练模型，从大语料库中学习到了一定的语言知识，在做文字情感分析时，只需要在下游连接上对应的分类器网络，即使只有比较少量的训练资料也能得到比较好的效果。 下图将Pre-training&Fine-tuning范式与Scratch范式的训练效果做了对比，其中Scratch范式即使用传统的随机初始化的方式从头训练整个分类网络。可以看到预训练&微调的训练范式可以加速模型的收敛（Convergence）并且效果也更好。 立场分析​ Extraction-based Question Answering​ BERT也可以用来完成截取式问答任务，提供一篇文章以及问题，要求输出两个integer代表答案短语在该文章中的起始位置以及结束位置。 具体的解决方案：选择输入文章的所有token所对应的输出向量，随机初始化两个相同维度的向量，分别与输出向量做Dot Product，在经过Softmax之后选择最大的得分所对应的索引。得到的两个索引分别是答案短语在文章中开始的位置以及结束的位置。 其他应用​ 虽然以上的应用都是NLP领域的，但是BERT是Seq2Seq模型，图片、语音等信号也都可以作为Sequence输入至BERT中，因此BERT也可以通过迁移学习应用至多模态领域。","s":"Fine-tuning","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#fine-tuning","p":1072},{"i":1085,"t":"BERT在大规模语料库（Corpus）上预训练后，输出的向量表示了对应输入token的意思，特别地，是考虑了上下文信息的意思。比如，对于水果的苹果已经苹果公司的苹果，BERT对一样的苹果有不同的输出。即对一个处在不同上下文信息中的相同词汇，BERT有不同的Embedding。类似于Word Embedding中的CBOW，BERT可以看作是Contextualized Word Embedding。 BERT模型的核心就是通过预训练来学习上下文信息，进而对每个输入token生成相应的向量表示。这个向量表示考虑了上下文信息，所以对于处在不同上下文中的相同词汇，BERT会有不同的Embedding。 ——来自讯飞星火大模型 虽然 BERT 的确是一种基于大规模语料库的预训练模型，但它并不是一种简单的词嵌入方法，而是一种深度神经网络模型。 在 BERT 中，每个单词都被嵌入到一个高维空间中，并且这些嵌入向量是通过多层 Transformer 编码器生成的。这些编码器的每一层都包含多个自注意力子层和一个前馈神经网络子层，它们共同作用来捕捉文本中的上下文关系和其他语义特征。 因此，BERT 中的嵌入向量不仅仅是单个单词的词义表征，还包括整个句子或段落中的语境信息。这意味着即使两个单词在不同的上下文中出现，它们的嵌入向量也可能非常相似，因为它们共享相同的语义结构。 总之，BERT 可以被视为一种上下文感知词嵌入技术，但它的实现方式比传统的词嵌入方法要复杂得多。 ——来自腾讯混元大模型","s":"Why does BERT work?","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#why-does-bert-work","p":1072},{"i":1087,"t":"GPT的结构是Transformer的Decoder部分，可以承担生成的任务。 GPT的自监督学习的特征体现在：在训练过程中，GPT根据输入的token预测输入的下一个token应该是什么，对输出的distribution与Ground Truth做Cross Entropy Loss来更新参数。","s":"GPT: Generative Pre-trained Transformer","u":"/en/docs/Deep-Learning/大模型基础/Self-Supervised-Learning","h":"#gpt-generative-pre-trained-transformer","p":1072},{"i":1089,"t":"参考资料：Sampling for Text Generation","s":"生成模型中的采样技巧","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"","p":1088},{"i":1091,"t":"生成模型的核心任务是学习数据的概率分布，并能够生成新的样本。采样（Sampling）是模型从其内部概率分布中生成新数据的关键步骤。采样不仅决定了生成数据的质量和多样性，还影响着模型对潜在空间的探索方式。 没有合适的采样策略，生成的结果可能会过于保守（总是生成相似的内容）或者过于随机（生成无意义的数据）。","s":"引言","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#引言","p":1088},{"i":1093,"t":"生成模型通常通过学习训练数据中的复杂概率分布来工作。这些模型试图捕捉到数据之间的统计规律，并以此为基础生成新的样本。我们通常希望模型能够捕捉到数据的复杂分布，并能够生成新的、合理的样本。如果不使用采样，模型可能会倾向于生成最常见的样本，这会导致生成的样本缺乏多样性，无法覆盖数据分布的全貌。直接从模型的概率分布中采样往往会带来一些问题，例如： 确定性解：模型可能倾向于给出单一最优解，这会导致生成结果缺乏多样性。 过拟合：模型可能过度拟合训练数据，从而在生成时重复相同的模式。 非典型样本：由于概率分布的长尾效应，直接采样可能会生成非常罕见且不符合实际的样本。","s":"为什么需要采样？","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#为什么需要采样","p":1088},{"i":1095,"t":"概率的长尾效应是指在概率分布中，某些事件虽然发生的概率非常小，但在分布的尾部仍存在一定的非零概率。在语言模型和其他生成模型中，长尾效应同样存在，指的是模型可能为一些非常罕见的词汇或序列分配非零的概率。 在生成模型中，长尾效应可能导致的问题在于模型有时会生成那些在训练数据集中很少见甚至从未出现过的样本。这是因为模型试图捕获训练数据中的所有统计特性，包括那些稀有的事件。在训练过程中，即使这些稀有事件只出现了一次，模型也可能学会为它们分配一定的概率。 当我们在生成过程中直接从模型的概率分布中采样时，如果模型的输出分布具有长尾特性，那么即使是那些概率很小的事件也可能被选中。例如，在文本生成中，模型可能会生成一些非常罕见的词汇组合，这些组合在语义上可能并不合理，或者语法上不符合常规。","s":"长尾效应","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#长尾效应","p":1088},{"i":1097,"t":"为了克服上述问题，研究人员开发了多种采样技巧，旨在控制生成过程中的随机性和多样性。","s":"采样技巧","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#采样技巧","p":1088},{"i":1099,"t":"温度是一种调整模型输出多样性的方法，它通过改变模型预测概率分布的形状来实现。较低的温度值会使得概率分布更加尖锐，而较高的温度值则会让分布变得更加平坦。 假设模型的原始概率分布为 p(xi)p(x_i)p(xi​)，那么通过温度调整后的概率分布可以表示为： p′(xi)=exp⁡(log⁡(p(xi))T)∑jexp⁡(log⁡(p(xj))T)p'(x_i) = \\frac{\\exp\\left(\\frac{\\log(p(x_i))}{T}\\right)}{\\sum_j \\exp\\left(\\frac{\\log(p(x_j))}{T}\\right)}p′(xi​)=∑j​exp(Tlog(p(xj​))​)exp(Tlog(p(xi​))​)​ p(xi)p(x_i)p(xi​)：表示原始模型对第 iii 个选项的预测概率。 TTT：温度参数，一般是一个正数。 当 T>1T > 1T>1 时，分布变得更平坦，这意味着模型更倾向于生成概率较低但仍然合理的样本；当 T<1T < 1T<1 时，分布变得更尖锐，模型倾向于生成高概率的样本，这会降低生成内容的多样性。","s":"温度（Temperature）","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#温度temperature","p":1088},{"i":1101,"t":"Top-k 采样是一种减少模型输出中低概率事件的方法。这种方法仅考虑最有可能发生的 k 个候选结果进行采样，从而避免生成那些极不可能出现的样本。 Top-k 采样算法从输出概率分布中选择概率最高的前 k 个候选项，并在这些候选项中进行随机采样。具体步骤如下： 获取模型输出的概率分布 P={p1,p2,…,pV}P=\\left\\{p_1, p_2, \\ldots, p_V\\right\\}P={p1​,p2​,…,pV​} ，其中 VVV 是词汇表的大小 选择概率最高的前 k 个候选项。设这些候选项的集合为 CkC_kCk​ 对于 CkC_kCk​ 中的每个候选项，重新归一化它们的概率，使得这些候选项的概率和为 1 从归一化后的概率分布中随机采样一个候选项。 公式化表示如下： Ck={wi∣pi is among the top k probabilities in P}C_k=\\left\\{w_i \\mid p_i \\text{ is among the top k probabilities in }P\\right\\}Ck​={wi​∣pi​ is among the top k probabilities in P}pi′=pi∑wj∈Ckpj for wi∈Ckp_i^{\\prime}=\\frac{p_i}{\\sum_{w_j \\in C_k} p_j} \\text { for } w_i \\in C_kpi′​=∑wj​∈Ck​​pj​pi​​ for wi​∈Ck​ 其中， pi′p_i^{\\prime}pi′​​ 是重新归一化后的概率。","s":"Top-k 采样","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#top-k-采样","p":1088},{"i":1103,"t":"Top-p 采样，也称为 Nucleus 采样，是一种更加灵活的采样方法，它不是固定选择前 k 个元素，而是选择累积概率达到某个阈值 p 的最小集合进行采样。 获取模型输出的概率分布 P={p1,p2,…,pV}P=\\left\\{p_1, p_2, \\ldots, p_V\\right\\}P={p1​,p2​,…,pV​} ，其中 VVV 是词汇表的大小 将概率分布按从高到低排序，得到排序后的候选项和对应的概率 {(w1,p1),(w2,p2),…,(wV,pV)}\\left\\{\\left(w_1, p_1\\right),\\left(w_2, p_2\\right), \\ldots,\\left(w_V, p_V\\right)\\right\\}{(w1​,p1​),(w2​,p2​),…,(wV​,pV​)} ，其中 p1≥p2≥…≥pVp_1 \\geq p_2 \\geq \\ldots \\geq p_Vp1​≥p2​≥…≥pV​ 找到最小的 k，使得前 k 个候选项的概率和大于等于阈值 ppp，设这些候选项的集合为 CpC_pCp​ 对于 CpC_pCp​ 中的每个候选项，重新归一化它们的概率，使得这些候选项的概率和为 1 从归一化后的概率分布中随机采样一个候选项 Cp={wi∣∑j=1ipj≥p}C_p=\\left\\{w_i \\mid \\sum_{j=1}^i p_j \\geq p\\right\\}Cp​={wi​∣j=1∑i​pj​≥p}pi′=pi∑wj∈Cppj for wi∈Cpp_i^{\\prime}=\\frac{p_i}{\\sum_{w_j \\in C_p} p_j} \\text { for } w_i \\in C_ppi′​=∑wj​∈Cp​​pj​pi​​ for wi​∈Cp​ 其中，pi′p_i^{\\prime}pi′​ 是重新归一化后的概率。","s":"Top-p 采样（Nucleus Sampling）","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#top-p-采样nucleus-sampling","p":1088},{"i":1105,"t":"采样技巧对于生成模型的成功至关重要。它们不仅可以帮助我们生成更加多样的结果，还可以改善生成质量。通过适当调整温度参数、使用 Top-k 或 Top-p 采样，我们可以有效地控制生成过程中的随机性，确保模型既能探索多样化的可能性，又能保持生成内容的合理性。","s":"结论","u":"/en/docs/Deep-Learning/大模型基础/Sampling-for-Generation","h":"#结论","p":1088},{"i":1107,"t":"tip 对于TensorFlow框架，可以使用TensorBoard实现可视化。 对于PyTorch框架，可以使用Visdom或TensorBoardX实现可视化，本篇主要讲述Visdom。","s":"Visdom可视化","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","h":"","p":1106},{"i":1109,"t":"pip install visdom","s":"安装Visdom","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","h":"#安装visdom","p":1106},{"i":1112,"t":"首先要通过终端启动Visdom，使用本机端口运行服务器。 以下二者均可。 visdom python -m visdom.server","s":"Visdom的启动","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","h":"#visdom的启动","p":1106},{"i":1114,"t":"from visdom import Visdom vis = Visdom() # 实例化 # 创建一条曲线，前两个参数分别为y轴数据、x轴数据，win参数是窗口的唯一标识，opt可选字典中可以给出窗口的title和legend vis.line([0.], [0.], win='win_id', opts=dict(title=\"win_title\")) # 在训练过程中的合适位置向初始化的曲线中喂数据 # viz.line([real_y_data], [global_step], win='win_id', update='append') # 查看训练loss vis.line([loss.item()], [epoch], win='win_id', update='append') # 对于非image数据，在传入visdom时仍需要先转化为numpy类型","s":"单窗口单曲线的可视化","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","h":"#单窗口单曲线的可视化","p":1106},{"i":1116,"t":"from visdom import Visdom vis = Visdom() vis.line([[0., 0.]], [0.], win='win_id', opts=dic(title=\"win_title\", legend=[\"curve_name_1\", \"curve_name_2\"])) # 在训练过程中的合适位置向初始化的曲线中喂数据 viz.line([[y1, y2]], [global_step], win='win_id', update='append')","s":"单窗口多曲线的可视化","u":"/en/docs/Deep-Learning/代码实现/Visdom-Visualization","h":"#单窗口多曲线的可视化","p":1106},{"i":1118,"t":"实战练习 Transformer实战练习，代码见Github仓库。 This is a practice of Transformer, follow the guide of Github Repo.","s":"Speaker Classification","u":"/en/docs/Deep-Learning/代码实现/Speaker-Classification","h":"","p":1117},{"i":1120,"t":"Classify the speaker of given features, learn how to use Transformer and how to adjust parameters of transformer.","s":"Overview","u":"/en/docs/Deep-Learning/代码实现/Speaker-Classification","h":"#overview","p":1117},{"i":1122,"t":"The original dataset is VoxCeleb1. We randomly select 600 speakers from VoxCeleb1, then preprocess the raw waveforms into mel-spectrograms. You can download the preprocessed dataset from Google Drive. Arguments: data_dir: The path to the data directory. metadata_path: The path to the metadata. segment_len: The length of audio segment for training. The architecture of dataset directory is shown below, where uttr-{random string}.pt represents PyTorch data file containing valid mel-spectrogram data. data directory/ ├── mapping.json ├── metadata.json ├── testdata.json └── uttr-{random string}.pt","s":"Dataset","u":"/en/docs/Deep-Learning/代码实现/Speaker-Classification","h":"#dataset","p":1117},{"i":1124,"t":"This is also the assignment solution of ML2021Spring HW4.","s":"Related","u":"/en/docs/Deep-Learning/代码实现/Speaker-Classification","h":"#related","p":1117},{"i":1126,"t":"important 参考链接: Illustrated: Self-Attention 动图轻松理解Self-Attention(自注意力机制)","s":"自注意力（Self-Attention）","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"","p":1125},{"i":1129,"t":"CNN模型的输入向量的形状是固定的，其输出向量的形状也是固定的或可以根据不同的下游任务而唯一确定，即输入形状与下游任务共同确定了一个CNN模型的架构，具有较强的固定性。 important 在视觉中，输入大多为数字图像，其形状可以大致分为由尺寸和通道数来决定。 从输入图像的尺寸看，当CNN中没有全连接层时，本质上可以接受任意尺寸的输入，但这是狭隘的。若考虑其下游任务以及输出，如FCN（Fully Convolution Network），FCN通过最后通过反卷积将tensor还原到原始图像尺寸，即在CNN中，输入与输出（下游任务的要求）都影响着CNN网络的结构。 从通道数看，CNN本质上可以接受任意通道数的图像输入，但是其模型效果将会受到极大的影响。以一个使用通道数为3的数据集进行训练的CNN模型，但在测试阶段分别使用通道数为 1 和 6 的数据进行推理的情形为例，进行分析： 通道数为1的测试集： 情况： 如果使用通道数为 1 的数据进行推理，即灰度图像，而模型在训练时是使用 RGB 数据集训练的，模型可能会受到一些影响。 解释： 模型可能在训练时学到了关于颜色的特定信息，而在测试时，如果输入是灰度图像，那些颜色信息将不可用。 建议： 在这种情况下，模型可能会失去对颜色信息的敏感性，可能需要进行进一步的调整或微调，以适应灰度图像的特性。 通道数为6的测试集： 情况： 如果使用通道数为 6 的数据进行推理，模型可能会面临额外的挑战，因为它在训练时只见过 3 个通道的数据。 解释： 模型在训练时学到的权重是基于 3 个通道的数据的，对于额外的通道，模型可能无法有效利用这些信息。 建议： 对于通道数不匹配的情况，可以考虑进行通道的适当组合或调整。这可能包括降低通道数（例如，只使用前 3 个通道），或者通过某种方式将 6 个通道映射到 3 个通道，例如通过某种特定的数据预处理。 当模型的输入更复杂（sophisticated），是长度不定的向量序列（sequence）时，CNN不能很好地处理，且不能解决输出由输入和模型自行决定的下游任务，如生成类任务。","s":"输入与输出的局限性","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#输入与输出的局限性","p":1125},{"i":1131,"t":"CNN中存在局部连接和权值共享的归纳偏置： **局部连接：**CNN使用卷积层通过滑动卷积核在输入上进行局部感受野的操作。每个神经元只与输入的一小部分区域相连，这意味着每个神经元只能接触到局部的上下文信息。 权值共享： 权值共享的主要思想是，对于输入图像的不同位置使用相同的权重参数进行卷积操作。这意味着，无论卷积操作发生在图像的左上角、右下角，或者其他任何位置，都使用相同的卷积核进行权值计算。CNN的权值共享使得模型能够学习到图像中的局部特征，这也是一种对于上下文的假设。相邻位置上的权重共享使得模型能够对局部结构进行建模，这种权重共享使得CNN具有更强的归纳偏置。 tip 在多通道卷积中，卷积核不同通道之间的权重参数是独立的。这使得网络能够学习不同通道之间的特征组合。这种设计有效地捕捉了输入数据中的多通道信息，提高了网络的表达能力。 CNN的设计理念认为：在图像任务中，局部结构通常更为重要，局部连接和权值共享使得CNN更适用于图像处理等任务。但也正是这种设计理念，使得CNN在面临长输入序列时不能很好地综合上下文信息、提取位置信息，因此Self-Attention应运而生，允许每个位置关注到序列中地所有其他位置。这种全局关联性质使得Transformer能够捕捉序列中的长距离依赖关系。","s":"关联上下文信息的局限性","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#关联上下文信息的局限性","p":1125},{"i":1134,"t":"A self-attention module takes in nnn inputs and returns nnn outputs. What happens in this module? In layman’s terms, the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores. Self-Attention接受任意向量数量的向量序列的输入，输出每一个向量所有向量（包括自身）的注意力分数。这使得Self-Attention在捕捉长距离依赖和处理序列中的全局关系时非常有效。","s":"什么是Self-Attention","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#什么是self-attention","p":1125},{"i":1136,"t":"自注意力机制的核心思想是为序列中的每个向量分配一个权重（即注意力分数），该权重表示该元素与其他元素的关联强度。这个权重是通过计算输入序列中所有元素与当前元素之间的关系来确定的。通常，这个计算过程使用一个可学习的权重矩阵来完成，即用来生成Key，Query以及Value的权重矩阵。","s":"Self-Attention的核心思想","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#self-attention的核心思想","p":1125},{"i":1138,"t":"important 定性分析详见文末。 注意考虑自注意力机制的实际意义，其输出是输入序列每个元素对每个位置的注意力分数。 因此，对于单头自注意力中的 Q、K 和 V 的维度 n_dim 通常等于输入序列的词嵌入维度 d_model。 对于多头自注意力机制而言，n_dim 通常等于 d_model // num_heads，即词嵌入维度除以多头自注意力的头数。 定义输入​ Self-Attention的输入是向量序列，其向量数量是任意的，计算每个输入向量之间的注意力分数。在本例中输入向量个数为3，同时为了统一性分析，计输入向量个数为batchbatchbatch个。 # define the input, which has a shape of (3, 4) inputs = [[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]] inputs = torch.tensor(inputs, dtype=torch.float32) 初始化权重矩阵​ 每个输入向量都会与3个权重向量做乘法得到3个新的向量，分别为key，query以及value。在本例中将新的向量维度设为3，由于输出的k、q、v矩阵大小均为(3,3)(3, 3)(3,3)，因此每个权重矩阵的形状应该是(4,3)(4, 3)(4,3)。为了统一性分析，计key，query以及value各向量维度为numnumnum。 In a neural network setting, these weights are usually small numbers, initialised randomly using an appropriate random distribution like Gaussian, Xavier and Kaiming distributions. This initialisation is done once before training. 在实际应用中，权重通常是较小的数字，通过适当的随机分布（比如高斯、Xavier和Kaiming分布）进行随机初始化。 # define the weights for keys, queries and values w_key = torch.tensor([[0, 0, 1], [1, 1, 0], [0, 1, 0], [1, 1, 0]], dtype=torch.float32) w_query = torch.tensor([[1, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 1]], dtype=torch.float32) w_value = torch.tensor([[0, 2, 0], [0, 3, 0], [1, 0, 3], [1, 1, 0]], dtype=torch.float32) 计算key，query以及value​ # compute keys, queries and values keys = inputs @ w_key queries = inputs @ w_query values = inputs @ w_value print(\"keys:\\n\", keys) # (3, 3) print(\"queries:\\n\", queries) # (3, 3) print(\"values:\\n\", values) # (3, 3) 计算原始的注意力分数​ 我们要为每一个输入向量计算它对所有向量的注意力分数，包括对自身的。 原始注意力分数的计算方式为，使用自身的query分别与所有向量的key做内积（dot product），得到的scalar数量与输入向量个数相同，都为batchbatchbatch，即scores矩阵的形状应为(batch,batch)(batch, batch)(batch,batch)。 # compute raw self-attention scores scores = queries @ keys.T print(\"attention scores:\\n\", scores) 注意，代码中提供的是计算所有向量的注意力分数，而图中演示的只是计算input #1的注意力分数。 对每一个向量计算出的注意力分数做softmax​ # normalize the attention score score_softmax = F.softmax(scores, dim=-1) # select the highest dimension print(\"attention scores after normalization:\\n\", score_softmax) 将注意力分数与对应的value相乘​ 每一个输入向量对所有batchbatchbatch个向量计算得到的注意力分数，都要与其对应的value向量相乘，计算加权的注意力分数。最终的注意力分数矩阵的形状应为(batch,num)(batch, num)(batch,num)。 # compute the weighted values by doting score_softmax with values # please be advised, this is dot product weighted_values = values[:, None] * score_softmax.T[:, :, None] print(\"weighted scores: \\n\", weighted_values) 加权注意力分数求和​ 最后一步，对于每个向量得到的加权注意力分数进行求和，得到维度为numnumnum的注意力分数向量，考虑到有batchbatchbatch个输入向量，因此最终的注意力分数矩阵的形状为(batch,num)(batch, num)(batch,num)。 根据推导，显然，最终Self-Attention的输出向量维度与value向量的维度相同，输出向量的数量与输入向量的数量相同。 # compute outputs outputs = weighted_values.sum(dim=0) 总结​ As mentioned in the above paragraph, we don’t only use dot product to find relevance. But we scale it as well by a factor of the square root of key dimension dk. This helps in making sure that the dot-products between query and key don’t grow too large for dk. If the dot product becomes too large then the softmax output will be very small. To avoid this, we scale the dot product. 在计算dot product后，为了避免点积运算经过softmax后的输出太小，在点积后除以key向量维度的平方根来进行缩放。 Attention(Q,K,V)=softmax(QKTdk)V(1)Attention(Q,K,V)=\\textit{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V \\tag{1}Attention(Q,K,V)=softmax(dk​​QKT​)V(1) 完整代码​ # simple code for Self-Attention import torch import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plot # define the input, which has the shape of (3, 4) inputs = [[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]] inputs = torch.tensor(inputs, dtype=torch.float32) # initialize the weights for keys, queries and values w_key = torch.tensor([[0, 0, 1], [1, 1, 0], [0, 1, 0], [1, 1, 0]], dtype=torch.float32) w_query = torch.tensor([[1, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 1]], dtype=torch.float32) w_value = torch.tensor([[0, 2, 0], [0, 3, 0], [1, 0, 3], [1, 1, 0]], dtype=torch.float32) # compute keys, queries and values keys = inputs @ w_key queries = inputs @ w_query values = inputs @ w_value print(\"keys:\\n\", keys) # (3, 3) print(\"queries:\\n\", queries) # (3, 3) print(\"values:\\n\", values) # (3, 3) # compute raw self-attention score scores = queries @ keys.T print(\"attention scores:\\n\", scores) # normalize the attention score score_softmax = F.softmax(scores, dim=-1) # select the highest dimension print(\"attention scores after normalization:\\n\", score_softmax) # compute the weighted values by doting score_softmax with values # please be advised, this is dot product weighted_values = values[:, None] * score_softmax.T[:, :, None] print(\"weighted scores: \\n\", weighted_values) # compute outputs outputs = weighted_values.sum(dim=0)","s":"Self-Attention的实现","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#self-attention的实现","p":1125},{"i":1140,"t":"多头自注意力机制是对自注意力机制的扩展，假设扩展成为nnn -head self-attention，则对每个输入向量生成对应的key，query和value后，再次使用nnn个可学习的权重矩阵生成nnn个不同的key0,...,keyn−1key^{0}, ..., key^{n-1}key0,...,keyn−1,query0,...,queryn−1query^{0}, ..., query^{n-1}query0,...,queryn−1以及value0,...,valuen−1value^{0}, ..., value^{n-1}value0,...,valuen−1。 在计算attention score时，使用每一个query查询对应的key，即query0query^{0}query0只与其他每一个输入向量的key0key^{0}key0做dot product。","s":"Multi-Head Self-Attention","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#multi-head-self-attention","p":1125},{"i":1142,"t":"Self-Attention可以看作是复杂化的CNN，CNN只能在感受野范围内考虑上下文信息，而Self-Attention可以自己学习感受野。","s":"Self-Attention与CNN的对比","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#self-attention与cnn的对比","p":1125},{"i":1144,"t":"自注意力机制（Self-Attention Mechanism）是深度学习领域中的一种重要机制，尤其在处理序列数据和图像特征时展现出了强大能力。下面将以一个典型的自注意力机制为例，使用 Transformer 架构中的多头自注意力（Multi-Head Self-Attention）来说明其计算过程，以帮助理解其工作原理。假设我们的输入是一个序列，比如文本序列，其长度为 LLL，每个词向量的维度为 DDD​。 important 注意考虑自注意力机制的实际意义，其输出是输入序列每个元素对每个位置的注意力分数。 因此，对于单头自注意力中的 Q、K 和 V 的维度 n_dim 通常等于输入序列的词嵌入维度 d_model。 对于多头自注意力机制而言，n_dim 通常等于 d_model // num_heads，即词嵌入维度除以多头自注意力的头数。","s":"Self Attention 的计算","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#self-attention-的计算","p":1125},{"i":1146,"t":"假设我们有一个文本序列，长度为 LLL，每个词表示为一个 DDD 维的向量。那么，整个输入的形状可以表示为 L×DL \\times DL×D 的矩阵。","s":"输入形状","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#输入形状","p":1125},{"i":1148,"t":"自注意力机制的核心思想是让序列中的每个元素（词向量）能够关注到序列中的其他元素，从而更好地理解它们之间的关系。具体来说，它通过计算三个向量——查询（Query，Q）、键（Key，K）和值（Value，V）来进行。 Query、Key、Value的生成： 首先，通过矩阵乘法，将输入的词向量矩阵分别与三个不同的权重矩阵相乘，生成 Q、K、V 三个矩阵。假设权重矩阵的维度均为 D×dkD \\times d_kD×dk​（其中 dkd_kdk​ 是查询和键的维度），那么 Q、K、V 的形状均为 L×dkL \\times d_kL×dk​。 计算注意力权重： 接下来，计算 Q 与 K 之间的相似度，通常使用点积（Dot Product）的方式，即每个查询向量 Q 与所有键向量 K 进行点积。点积的结果是一个 L×LL \\times LL×L 的矩阵，其中的每个元素表示查询向量与键向量之间的相似度。为了使这个矩阵中的元素处于同一尺度，通常会除以 dk\\sqrt{d_k}dk​​，以避免过大的点积值导致 softmax 函数饱和。 然后，对上述得到的矩阵应用 softmax 函数，得到注意力权重矩阵。这个矩阵的每一行都是一个概率分布，表示序列中每个元素对其他元素的注意力权重。 加权求和： 最后，将注意力权重矩阵与值矩阵 V 进行矩阵乘法，得到最终的注意力输出。这个过程可以视为对 V 矩阵中的每个值向量进行加权求和，权重由注意力权重矩阵决定。","s":"自注意力机制的计算步骤","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#自注意力机制的计算步骤","p":1125},{"i":1150,"t":"在实际应用中，往往采用多头自注意力（Multi-Head Attention）来增强模型的表示能力。具体来说，就是将上述过程重复多次，每次使用不同的权重矩阵来生成 Q、K、V，然后将多次得到的注意力输出进行拼接或平均，形成最终的多头注意力输出。","s":"多头自注意力","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#多头自注意力","p":1125},{"i":1152,"t":"假设输入的文本序列长度为 L=5L=5L=5，每个词向量的维度为 D=512D=512D=512，dk=512d_k=512dk​=512（查询和键的维度）。那么，Q、K、V 的形状均为 5×5125 \\times 5125×512。在计算注意力权重时，得到的矩阵形状为 5×55 \\times 55×5。最后的注意力输出形状同样为 5×5125 \\times 5125×512（如果是多头自注意力，则可能需要将多个头的输出拼接或平均，具体形状取决于头的数量和后续处理方式）。 自注意力机制通过让序列中的每个元素都能“看到”其他元素，并根据它们之间的关系调整自己的表示，从而增强了模型对序列数据的理解能力，这对于处理自然语言处理任务（如翻译、文本生成等）和图像特征提取（如在视觉 Transformer 中）等场景都有着重要作用。","s":"示例","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#示例","p":1125},{"i":1154,"t":"tip 参考资料：https://blog.csdn.net/yeziyezi210/article/details/103864518 自注意力中有多种掩码类型，但实现方式大致相同，其作用大概可以分为一下几种。 防止注意力机制关注填充（padding）标记 实现因果注意力（causal attention），即每个位置只能关注它自己和之前的位置，常用于 Decoder 中 在特定任务中屏蔽某些不相关的输入","s":"自注意力中的掩码 Mask","u":"/en/docs/Deep-Learning/大模型基础/Self-Attention","h":"#自注意力中的掩码-mask","p":1125},{"i":1156,"t":"tip 输入：shape为[5,5,3][5, 5, 3][5,5,3]的图像 输出要求：shape为[5,5,4][5, 5, 4][5,5,4]的feature map 使用3×33 \\times 33×3卷积核，padding=1,stride=1padding=1, stride=1padding=1,stride=1","s":"深度可分离卷积","u":"/en/docs/Deep-Learning/基础知识/深度可分离卷积","h":"","p":1155},{"i":1158,"t":"卷积层共4个filter（输出通道为4），每个filter3个kernel（输入通道为3） 其中，每个filter都对输入图像的所有通道完成一次卷积，filter中的kernel分别对输入的通道进行具体卷积运算 不考虑卷积偏置，参数量为 3×3×3×4=108(1)3 \\times 3 \\times 3 \\times 4 = 108 \\tag{1}3×3×3×4=108(1)","s":"常规卷积","u":"/en/docs/Deep-Learning/基础知识/深度可分离卷积","h":"#常规卷积","p":1155},{"i":1160,"t":"使用1个filter，其中包含3个kernel。每个kernel分别对输入图像的3个通道单独进行卷积，参数量为 3×3××3=27(2)3 \\times 3 \\times \\times 3 = 27 \\tag{2}3×3××3=27(2) 代码实现也较为简单，只需令Conv2d的输出通道与输入通道相同即可","s":"逐通道卷积-Depthwise Convolution","u":"/en/docs/Deep-Learning/基础知识/深度可分离卷积","h":"#逐通道卷积-depthwise-convolution","p":1155},{"i":1162,"t":"使用1×11 \\times 11×1卷积核，每个filter对上一步的feature map在深度方向进行一次加权组合，参数量为 1×1×3×4=12(3)1 \\times 1 \\times 3 \\times 4 = 12 \\tag{3}1×1×3×4=12(3) tip 图片源自知乎","s":"逐点卷积-Pointwise Convolution","u":"/en/docs/Deep-Learning/基础知识/深度可分离卷积","h":"#逐点卷积-pointwise-convolution","p":1155},{"i":1164,"t":"本篇论文主要基于 IPL 的思想实现。本仓库大部分从 IPL-Zero-Shot-Generative-Model-Adaptation fork 而来并做出了一定修改。","s":"本科毕业论文：基于 Prompt Learning 的视觉-语言大模型在图像生成中的应用与研究","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"","p":1163},{"i":1167,"t":"conda create -n ipl python=3.8 conda activate ipl","s":"创建 Anaconda 虚拟环境","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#创建-anaconda-虚拟环境","p":1163},{"i":1169,"t":"请确保 NVIDIA 驱动、CUDA 以及 PyTorch 之间版本互相匹配。 conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia pip install ftfy regex tqdm ninja pip install git+https://github.com/openai/CLIP.git","s":"安装依赖","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#安装依赖","p":1163},{"i":1171,"t":"预训练的源域生成器可以通过 Google Drive 或者 Tsinghua Cloud 下载，并将其置于 ./pre_stylegan 文件夹中。","s":"下载预训练生成器","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#下载预训练生成器","p":1163},{"i":1175,"t":"ctx_init 参数用于初始化 prompts，官方提供的演示 context 是a photo of a。 source_prompts = [prompt_prefix + \" \" + args.source_class] target_prompts = [prompt_prefix + \" \" + args.target_class] 源域的初始提示词 source_prompts 是 ctx_init 与源域标签的组合。若源域标签为 photo，则源域的初始提示词是 a photo of a photo。目标域的初始提示词同理。","s":"prompts 的初始化","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#prompts-的初始化","p":1163},{"i":1177,"t":"源域以及目标域的初始提示词接下来会进行 tokenize： source_tokenized_prompts = torch.cat([clip.tokenize(p) for p in source_prompts]).to(device) # (1, 77) 'sot a photo of a photo eot' 在经过tokenize后为tensor [[49406, 320, 1125, 539, 320, 1125, 49407, etc]] # 77是CLIP在tokenize方法中缺省的context_length，超过context_length将被truncate，不足的将用0补齐 target_tokenized_prompts = torch.cat([clip.tokenize(p) for p in target_prompts]).to(device) # (1, 77) 'sot a photo of a disney' 在经过tokenize后为tensor [[49406, 320, 1125, 539, 320, 4696, 49407, etc]] # 77是CLIP在tokenize方法中缺省的context_length，超过context_length将被truncate，不足的将用0补齐 tokenize 是 CLIP 对送入的 prompt 字符串进行标记化处理，在头部和尾部添加 startoftext 以及 endoftext 标记，最终为两个首尾标记和全部单词生成 int 标记。其中 CLIP 模型缺省的 context_length 是77，若 prompt 大于 77 会进行截断（truncate），若小于 77 会进行补零，因此 source_tokenized_prompts 与 target_tokenized_prompts 的形状均为 (1, 77)。 在提示词标记化之后，将进行嵌入表示 embedding： source_embedding = clip_model.token_embedding(source_tokenized_prompts).type(clip_model.dtype) # (1, 77, 512) 其中512是CLIP中的n_dim，token_embedding层的词嵌入的维度 target_embedding = clip_model.token_embedding(target_tokenized_prompts).type(clip_model.dtype) # (1, 77, 512) 其中512是CLIP中的n_dim，token_embedding层的词嵌入的维度","s":"prompts 的 tokenize 与 embedding","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#prompts-的-tokenize-与-embedding","p":1163},{"i":1179,"t":"在 Mapper 生成 prompts 后进行 prompts 的特征提取时，需要传入 tokenize 之后的人工初始化 prompt（‘a photo of a photo.’或‘a photo of a disney.’），用于选择 eot 符号对应的维度来进行特征投影（因为 eot 作为整个句子的结尾，被认为该维度包含更多的信息。具体做法：由于在 tokenize 之后，eot 符号对应的维度的值最大，因此可使用 argmax 来定位），以保证最后得到的特征形状与图像特征提取的输出形状相同，使得后续可以进行对比学习的损失计算。","s":"compute_text_features 的实现细节","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#compute_text_features-的实现细节","p":1163},{"i":1181,"t":"Z空间与W空间​ # Z空间到W空间的变换 sample_z = mixing_noise(args.batch_mapper, 512, args.mixing, device) # (batch_size, 512) sample_w = net.generator_frozen.style(sample_z) # (batch_size, 512) Z 空间和 W 空间是 StyleGAN 模型中两种不同的隐变量空间，分别用于控制生成图像的随机特征和样式信息。W 空间通过对 Z 空间的映射得到。 Z 空间（Latent Space Z）： Z 空间是随机噪声空间，通常由随机噪声向量组成，表示了图像的随机特征。 在 StyleGAN 中，Z 空间的维度通常为 512 维。这意味着一个 Z 向量由 512 个数字组成，每个数字表示了图像的一个随机特征的强度或者方向。 W 空间（Style Space W）： W 空间经过特征解耦的隐空间，与 Z 空间相比更加解耦合。 在 StyleGAN 中，W 空间的维度也通常为 512 维，是通过mapping network进行映射得到的，mapping network 由 PixelNorm 层与 EqualLinear 层构成。以下代码节选自sg2_model.py： '''mapping network''' layers = [PixelNorm()] for i in range(n_mlp): layers.append( EqualLinear( style_dim, style_dim, lr_mul=lr_mlp, activation=\"fused_lrelu\" ) ) self.style = nn.Sequential(*layers) Z 空间与 W 空间的关系： 在 StyleGAN 中，通常会先将一个 Z 向量映射到 W 空间，然后再将 W 向量输入到生成器网络中生成图像。 Z 空间提供了初始随机噪声，而 W 空间则通过特征解耦提供更多控制图像风格的灵活性。通过对 Z 和 W 之间的映射以及 W 在生成器中的应用，StyleGan 实现了高度可控且具有良好生成效果的图像合成。 损失函数​ 在代码中，stage 1 的损失函数是 global_clip_loss，该损失由三部分组成： 对比学习损失：Mapper 生成的源域 prompts 的特征**（注意，这里的 prompts 特征是与人工初始化的 prompts 的特征做过 element-wise 相加后的特征）**与源域图像特征的余弦相似度组成的对比学习损失； 目标域正则化损失：Mapper 生成的目标域 prompts 的特征与目标域文本标签特征的余弦相似度，这里生成的目标域 prompts 特征同样也是与人工初始化的 prompts 做过加法的。注意该损失有权重 lambda_l。 源域正则化：计算生成的源域prompts与源域标签之间的余弦相似度，由 lambda_src 控制，默认是 0。","s":"训练 stage 1","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#训练-stage-1","p":1163},{"i":1183,"t":"确定目标域生成域需要更新的层​ 在训练的第二阶段进行前向传播时，需要先对目标域生成器（generator_trainable）的所有层进行 unfreeze，然后对更新哪些层做出选择，承担选择任务的功能函数：model.ZSSGAN.ZSSGAN.determine_opt_layers，最后 freeze 所有层后再 unfreeze 选择的网络层。 if self.training and self.auto_layer_iters > 0: self.generator_trainable.unfreeze_layers() # unfreeze train_layers = self.determine_opt_layers() # layer to train if not isinstance(train_layers, list): train_layers = [train_layers] self.generator_trainable.freeze_layers() self.generator_trainable.unfreeze_layers(train_layers) # unfreeze 具体选择带更新网络层的策略： 将 W 空间的隐向量送入目标域生成器（SG2Generator）中，并进行反向传播，此时可以通过反向传播后 W 空间隐向量不同维度的更新幅度来衡量不同网络层的影响力，因此选出更新幅度最大的维度就可以确定在 Model Adaption 中需要更新的网络层。 之所以 W 空间编码在 n_latent 维度上的序号就代表着对应的网络层数的序号，是因为 StyleGAN 生成器的结构决定了这一点：StyleGAN 生成器中，W 空间编码的不同维度会被送入生成器网络的不同层，控制这些层的特征映射 (feature mapping)。具体来说，W 空间编码的每个维度会被重复 n_latent 次，作为该层的风格向量 (style vector)，通过 AdaIN (Adaptive Instance Normalization) 层控制该层的特征映射。因此，W 空间编码的第 i 个维度会影响生成器网络中第 i 层的特征映射。当某个维度的 W 值被更新的程度较大时，就意味着该维度对应的层在生成目标图像时起到了重要作用，需要被优化。 损失函数​ stage 2 的损失函数是 CLIP Loss 类中的 clip_directional_loss，该损失函数由两部分组成： edit_direciton：源域生成器与目标域生成器生成的图片在经过 image encdoer 后做 element-wise 的相减，最后除以自身的 L2 Norm 方便后续与 target_direction 计算余弦相似度。 target_direction：Mapper 产生的源域和目标域 prompts 的 text_features 做element-wise相减后，最后初一自身的 L2 Norm 以便后续与 edit_direction 计算余弦相似度。","s":"训练 stage 2","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#训练-stage-2","p":1163},{"i":1185,"t":"测试所用 nada 权重 Google Drive 链接：StyleGAN-NADA Models 参考文献：GAN 的几种评价指标 Inception Score（IS） 评估图像的质量和多样性 质量：把生成的图片 xxx 输入 Inception V3 中，得到输出 1000 维的向量 yyy，向量的每个维度的值对应图片属于某类的概率。对于一个清晰的图片，它属于某一类的概率应该非常大，而属于其它类的概率应该很小。用专业术语说， p(y∣x)p(y|x)p(y∣x) 的熵应该很小（熵代表混乱度，均匀分布的混乱度最大，熵最大）。 多样性： 如果一个模型能生成足够多样的图片，那么它生成的图片在各个类别中的分布应该是平均的，假设生成了 10000 张图片，那么最理想的情况是，1000 类中每类生成了 10 张。转换成术语，就是生成图片在所有类别概率的边缘分布 p(y)p(y)p(y) 熵很大（均匀分布）。 因此，对于 IS 我们需要求的两个量就是 p(y∣x)p(y|x)p(y∣x) 和 p(y)p(y)p(y)。实际中，选取大量生成样本，用经验分布模拟 p(y)p(y)p(y)： p^(y)=1N∑i=1Np(y∣x(i))\\hat{p}(y)=\\frac{1}{N}\\sum_{i=1}^{N}p(y|\\mathbf{x}^{(i)})p^​(y)=N1​∑i=1N​p(y∣x(i)) Inception Score 的完整公式如下： IS=exp⁡(Ex[KL(p(y∣x)∣∣p(y))])IS=\\exp\\left(\\mathbb{E}_x[KL(p(y|x)||p(y))]\\right)IS=exp(Ex​[KL(p(y∣x)∣∣p(y))]) 其中 Ex\\mathbb{E}_xEx​ 表示对所有图像的期望，KL(p(y∣x)∣∣p(y))KL(p(y|x)||p(y))KL(p(y∣x)∣∣p(y)) 表示每张图像的 KL 散度，exp⁡\\expexp 表示取指数。 通常计算 Inception Score 时，会生成 50000 个图片，然后把它分成 10 份，每份 5000 个，分别代入公式计算 10 次 Inception Score，再计算均值和方差，作为最终的衡量指标（均值±方差）。但是 5000 个样本往往不足以得到准确的边缘分布 p(y)p(y)p(y)，尤其是像 ImageNet 这种包含 1000 个类的数据集。 StyleGAN-nada 以及 IPL 在经过 batch_size 为 2，iteration 为 300 的训练后（其中 IPL 的 Mapper 是以 batch_size 为 32，iteration 为 300 进行训练的），二者的 IS 分别为 (2.2960, 0.2042) 以及 (2.6420, 0.1959)。 Fréchet Inception Distance（FID） 评估目标域的风格 计算 IS 时只考虑了生成样本，没有考虑真实数据，即 IS 无法反映真实数据和样本之间的距离，IS 判断数据真实性的依据，源于 Inception V3 的训练集 ImageNet，在 Inception V3 的“世界观”下，凡是不像 ImageNet 的数据，都是不真实的，都不能保证输出一个 sharp 的 predition distribution。因此，要想更好地评价生成网络，就要使用更加有效的方法计算真实分布与生成样本之间的距离。 FID 距离计算真实样本，生成样本在特征空间之间的距离。首先利用 Inception 网络来提取特征，然后使用高斯模型对特征空间进行建模，再去求解两个特征之间的距离，较低的 FID 意味着较高图片的质量和多样性。 StyleGAN-nada 以及 IPL 在经过 batch_size 为 2，iteration 为 300 的训练后（其中 IPL 的 Mapper 是以 batch_size 为 32，iteration 为 300 进行训练的），二者的 FID 分别为 84 以及 58。 Single Image Fréchet Inception Score（SIFID） FID 测量生成的图像的深层特征分布与真实图像的分布之间的偏差。在 ICCV 2019 Best Paper 中提出了 SIFID，只使用一张真实目标域的图像。与 FID 不同，SFID 不使用 Inception Network 中最后一个池化层之后的激活矢量（每个图像一个向量），而是在第二个池层之前的卷积层输出处使用深层特征的内部分布（feature map 中每个位置一个向量）。最终 SIFID 是真实图像和生成的样本中这些特征的统计数据之间的 FID。 Structural Consistency Score（SCS） 评估图像的结构保存能力 Identity Similarity（ID） 评估图像的特征保存能力 定量分析结果​ IS（Inception Score）↑ 数据集 源域→目标域 NADA IPL IPL* FFHQ Photo→Disney 2.296 2.642 2.701 FFHQ Photo→Anime Painting 2.320 2.464 2.578 FFHQ Photo→Wall painting FFHQ Photo→Ukiyo-e 2.489 2.715 2.851 FFHQ Photo→Pixar character FFHQ Photo→Tolkien elf FFHQ Photo→Werewolf 2.173 2.482 2.517 AFHQ Photo→Cartoon AFHQ Photo→Pointillism AFHQ Photo→Cubism SFID（Single Fréchet Inception Distance）↓ 数据集 源域→目标域 NADA IPL IPL* FFHQ Photo→Disney 84 58 54 FFHQ Photo→Anime Painting FFHQ Photo→Wall painting FFHQ Photo→Ukiyo-e FFHQ Photo→Pixar character FFHQ Photo→Tolkien elf FFHQ Photo→Werewolf AFHQ Photo→Cartoon AFHQ Photo→Pointillism AFHQ Photo→Cubism","s":"定量分析指标","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#定量分析指标","p":1163},{"i":1188,"t":"新增了对自定义图像进行风格迁移的功能。 HyperStyle 中的 e4e encoder 将自定义的真实图像编码至 StyleGAN2 中的 W 空间生成 latent codes，再将其分别输入至源域生成器以及目标域生成器以代替原始的从正态分布中 sample 出的随机向量生成的 w_codes，从而得到相应的图片。其中 e4e encoder 来源于 HyperStyle 提供的预训练 checkpoint。 使用方法：运行 inference.py，设置对应的参数，如生成器以及 e4e encoder 的路径、图像路径等，最后运行即可。 修改日志​ 第一次尝试只加载了 w_encoder 类及其对应 checkpoint 参数，导致并未将真实图片编码到 StyleGAN 的 W 空间中，没有 inversion 出合理的结果。 第二次尝试使用了 restyle_e4e_encoder，但是没有使用 dlib 进行 alignment，也没有使用 restyle 模型在反演时使用的多次进行前向传播来修正 latent code 的策略。此次尝试虽然反演出了合理的人像，但是人像的特征保存能力非常弱。 第三次尝试解决了上一次发现的问题，加入 dlib 提供的 landmark 检测以实现 alignment，并且使用 run_loop 函数在 restyle_e4e_encoder 中进行多次前向传播以修正得到的 W 空间的 latent code，效果较好。 对比 pSp 和 e4e encoder，pSp 对人脸图像的还原能力较强，但是会导致目标域图像具有随机的彩色光晕。","s":"支持自定义图像的风格迁移","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#支持自定义图像的风格迁移","p":1163},{"i":1190,"t":"参考 MIT 开源项目 pytorch-deployment 进行生成模型的 Web UI 部署。参考项目使用的是 StarGANv2 模型，对其进行优化使得其可以部署 StyleGAN 模型。 分别对人像和宠物图像生成了两个单独的卡片和 HTML 网页，网页可以完成两种功能： 使用参考图像进行零样本跨域适应，同时可以在网页下拉框中选择预期的目标域风格（由于没有合适的 restyle encoder，宠物图像不支持选择参考图像） 直接使用随机数生成源域图像并进行零样本跨域适应 UI 独立代码可以参考本人仓库 stylegan-ui，但功能有限，完整的 UI 代码已经合并到主程序中，请参考 ./web_ui 中的具体代码。 部分效果展示图​ 主页： 人物画像的零样本域适应（初始状态）： 人物画像的零样本域适应（使用参考图像生成状态）： 宠物画像的零样本域适应（初始状态）： 宠物画像的零样本域适应（使用随机数生成状态）：","s":"Web UI","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#web-ui","p":1163},{"i":1193,"t":"Mapper 的作用是从 W 空间的隐式代码中学习出符合源域图片特征以及符合目标域文字特征的 prompts。 改进后的 Mapper 结构： class TransformerMapperV2(nn.Module): \"\"\" 改良版transformer mapper，增加多头注意力，减小transformer encoder的层数，防止学习到的源域图像细节过拟合 同时去掉开头的PixelNorm，防止与transformer中的layer normalization冲突 并在transformer encoder之后加入Pixel Norm以及全连接层 \"\"\" def __init__(self, opts, n_dim): super(TransformerMapperV2, self).__init__() self.opts = opts self.n_dim = n_dim layers = [] # transformer中有layer normalization，不需要进行PixelNorm # 自定义Transformer编码器层配置 transformer_layer = TransformerEncoderLayer(d_model=512, nhead=4, dim_feedforward=1024, dropout=0.1) # 构建Transformer编码器 self.transformer_encoder = TransformerEncoder(transformer_layer, num_layers=2) layers.append(self.transformer_encoder) # 再过一次PixelNorm以及全连接层，将每个点归一化（除以模长），避免输入noise的极端权重，改善稳定性 layers.append(PixelNorm()) self.linear = EqualLinear(512, 512, lr_mul=0.01, activation='fused_lrelu') layers.append(self.linear) # 最后一个全连接层，输出维度保持不变 self.final_linear = EqualLinear(512, n_dim * opts.n_ctx, lr_mul=0.01, activation='fused_lrelu') layers.append(self.final_linear) self.mapping = nn.Sequential(*layers).to(device)","s":"改进：Mapper 结构的设计","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#改进mapper-结构的设计","p":1163},{"i":1195,"t":"在 IPL 的官方代码实现中，人工设计的 prompts 有两处，一是 ctx_init，由命令行参数赋值，即 \"a photo of a\"，另一处是 utils/text_templates.py 中的 templates，下面分别分析这两处的具体作用。 ctx_init 的作用（与域标签拼接后的 ctx_init）​ ctx_init 在 compute_text_features 函数中用于定位 eot 层符号所表示的维度来进行投影，使得文字特征与图像特征维度相同，并不参与 text_features 的实际计算。但是在该函数中，Mapper 输出的 image-specific prompts 已经与域标签的嵌入表示进行了 concat。 在 stage 1 训练 Mapper 损失函数中，Mapper 学习到的 image-specfic prompts 在与源域标签进行 concat 并得到文字编码后，会与 ctx_init 的文字编码进行 element-wise 的相加，最后再与源域生成器输出的图片的图像编码进行对比损失计算； 同理，在 stage 2 训练目标域生成器时，Mapper 输出的 image-specific prompts 在分别与源域、目标域标签 concat 后送入文字编码器得到文字特征，再与 ctx_init 的文字特征进行 element-wise 相加，最后二者相减得到 text_direction。 templates 的作用​ templates 是提前准备好的一系列字符串，其中字符串的格式全部类似于 a photo of a {}. 原始 hhfq 数据集的模板共有 79 个字符串。 与 ctx_init 起作用的函数不同，templates 在第一阶段的训练的 domain regularization loss 中使用到的 get_text_features 函数起作用，用于与目标域标签进行格式化连接后成为 image-specific prompts 向目标域靠近的方向。即 domain loss 使学习到的 prompts 向以目标域标签为中心的字符串对齐。 思考​ IPL 方法对 Mapper 学习到的 prompts 除了（1）使用对比学习使 prompts 学习到源域图片的特征以及（2）使用域正则化使得 prompts 向目标域标签对齐之外，并没有使用其他与人工设计的 prompts 有关的正则化方式来约束 prompts 的学习，因此人工设计的 prompts 可能并没有起到太大的约束作用。 如果对比学习损失是为了让 Mapper 自监督学习到图片的特征外，那么是否可以对域正则化损失进行改进，约束学习到的 prompts 向人工设计的初始化 prompts 对齐，以实现类似于 Stable Diffusion 类似的 prompts 控制图像生成的效果。","s":"问题：训练阶段人工 prompts 的作用是什么？","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#问题训练阶段人工-prompts-的作用是什么","p":1163},{"i":1197,"t":"对第一阶段的损失函数做出修改，更新domain loss，将原始 domain loss 中使用的以目标域标签为中心的模板更换成自定义模板，使目标域的image-specific prompts与自定义模板对齐。 经过多次实验和分析，刻意让 Mapper 输出的image-specific prompts 去逼近用户设置的 prompts，会产生一些隐式细节的丢失。因为 Mapper 本身存在的目的就是学习出人工无法准确描述的细节（包括源域图像的自身细节以及目标域风格的细节），如果对 Mapper 的损失函数中加上太多人为设计的限制，很显然会造成细节的丢失并且出现同质的现象。 因此，为了达到既使用精心设计的 prompts 来优化域适应，同时又不影响 Mapper 自主学习双域特征，在原有两个损失函数的基础上，新增一个权重较小的损失函数，用于将 Mapper 学习到的目标域 prompts 向自定义模板对齐。 用于生成 prompts 的 GPT、Claude prompts​ 中文提示词： 针对将普通人像转换成迪士尼风格人物画像的任务，给出60个描述迪士尼人像特有特征的文字prompt。 将上述生成的60个prompts放在同一个Python列表中，即每一个prompt作为该列表的字符串元素，输出整个Python列表。 英文提示词： For the task of converting a {source class} photo into a {target_class} photo, provide some text prompts describing the distinctive features of Disney character portraits. Put the generated 60 prompts into the same Python list, with each prompt as a string element of the list, and output the entire Python list. 对 global_clip_loss 的改进​ IPL 训练第一阶段的损失函数除了源域 prompts 与源域图像之间的对比学习损失函数外，还有将目标域 prompts 与目标域标签计算余弦相似度的 domain regularization。 对 domain regularization 进行改进，引入开发者自定义的 prompts，约束 Mapper 学习到的目标域 prompts 向开发者自定义的 prompts 对齐，以此来进行 prompt tuning，发挥 prompt learning 的更大优势，并增强自定义性。 对 clip_directional_loss 的改进​ IPL 训练第二阶段的损失函数，使用 criteria.clip_loss.CLIPLoss.clip_directional_loss。","s":"改进：使学习到的 prompts 向用户自主设计的 prompts 模板对齐","u":"/en/docs/Deep-Learning/代码实现/Undergraduate-Dissertation","h":"#改进使学习到的-prompts-向用户自主设计的-prompts-模板对齐","p":1163},{"i":1201,"t":"σ(x)=11+e−x(1)\\sigma(x) = \\frac{1}{1 + e^{-x}} \\tag{1}σ(x)=1+e−x1​(1)dσdx=σ (1−σ)(2)\\frac{{\\rm d}\\sigma}{{\\rm d}x} = \\sigma \\space (1 - \\sigma) \\tag{2}dxdσ​=σ (1−σ)(2) 优点：可以将数据压缩至[0, 1)区间内，有较大实用意义 致命问题：在输入值较小或较大时，Sigmoid函数的梯度趋近于零，会导致网络参数长时间得不到更新，即梯度弥散问题 from torch.nn import functional as F import torch x = torch.linspace(-100, 100, 10) F.sigmoid(x) # 当x为100时，sigmoid(x)就接近于0了","s":"Sigmoid函数 / Logistic函数","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","h":"#sigmoid函数--logistic函数","p":1198},{"i":1203,"t":"f(x)={0x<0xx≥0(3) f(x) = \\begin{cases} 0 & x < 0\\\\ x & x \\geq 0\\\\ \\end{cases} \\tag{3}f(x)={0x​x<0x≥0​(3)df(x)dx={0x<01x≥0(4) \\frac {{\\text d}f(x)}{{\\text d}x} = \\begin{cases} 0 & x < 0\\\\ 1 & x \\geq 0\\\\ \\end{cases} \\tag{4}dxdf(x)​={01​x<0x≥0​(4) from torch.nn import functional as F import torch x = torch.linspace(-100, 100, 10) F.relu(x)","s":"线性整流单元（Rectified Linear Unit, ReLU）","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","h":"#线性整流单元rectified-linear-unit-relu","p":1198},{"i":1206,"t":"L2范数是对元素求平方和后再开根号，需要.pow(2)后才可作为损失函数 微小的误差可能对网络性能带来极大的影响 LossMSE=∑[y−f(x)]2(5)Loss_{MSE} = \\sum{[{y - f(x)]^2}} \\tag{5}LossMSE​=∑[y−f(x)]2(5) ∥y−f(x)∥2=∑[y−f(x)]22(6)\\Vert y - f(x) \\Vert_2 = \\sqrt[2]{\\sum{[y - f(x)]^2}} \\tag{6}∥y−f(x)∥2​=2∑[y−f(x)]2​(6)","s":"Mean Squared Error 均方误差","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","h":"#mean-squared-error-均方误差","p":1198},{"i":1208,"t":"信息熵​ Cross Entropy中的Entropy指的是信息熵，可以理解为不确定性。衡量一个概率分布本身的不确定程度。 It's a measure of surprise, higher entrpoy means less information and higher uncertainty. 假设一个离散型随机变量XXX的可能取值为X=x1,x2,...,xnX=x_1,x_2,...,x_nX=x1​,x2​,...,xn​，而取值事件xix_ixi​发生的概率为PiP_iPi​，则其信息熵的定义为 H(P)=−∑inPi log2(Pi)=∑inPi log2(1Pi)\\begin{align} H(P) &= -\\sum_i^n{P_i}\\space{log_2(P_i)} \\\\ &= \\sum_i^n{P_i}\\space{log_2({\\frac{1}{P_i}}}) \\tag{7} \\end{align}H(P)​=−i∑n​Pi​ log2​(Pi​)=i∑n​Pi​ log2​(Pi​1​)​(7)​ KL散度​ 在概率论或信息论中，KL散度( Kullback–Leibler Divergence)，又称相对熵（relative entropy)，是描述两个概率分布P和Q差异的一种方法。 存在两个概率分布P和Q，其离散型随机变量XXX的可能取值为X=x1,x2,...,xnX=x_1,x_2,...,x_nX=x1​,x2​,...,xn​，而取值事件xix_ixi​发生的概率分别为Pi,QiP_i,Q_iPi​,Qi​. KL散度是非对称的，即 DKL(P ∣∣ Q)≠DKL(Q ∣∣ P)(8)D_{KL}(P \\space || \\space Q) \\neq D_{KL}(Q \\space || \\space P)\\tag{8}DKL​(P ∣∣ Q)=DKL​(Q ∣∣ P)(8)DKL(P ∣∣ Q)=∑Pi [log2(Pi)−log2(Qi)](9)D_{KL}(P \\space || \\space Q) = \\sum{P_i\\space [log_2(P_i)-log_2(Q_i)]}\\tag{9}DKL​(P ∣∣ Q)=∑Pi​ [log2​(Pi​)−log2​(Qi​)](9) 特别的，DKL(PLabel ∣ QPred)D_{KL}(P_{Label} \\space | \\space Q_{Pred})DKL​(PLabel​ ∣ QPred​)表示当用概率分布Q来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。 交叉熵​ 衡量两个概率分布P和Q之间的不确定性程度。交叉熵的数学表达为 H(P, Q)=H(P)+DKL(P ∣∣ Q)=−∑Pi log2(Qi)\\begin{align} H(P, \\space Q) &= H(P) + D_{KL}(P\\space || \\space Q) \\\\ &= - \\sum{P_i}\\space{log_2({Q_i})} \\tag{10} \\end{align}H(P, Q)​=H(P)+DKL​(P ∣∣ Q)=−∑Pi​ log2​(Qi​)​(10)​ PyTorch中的CrossEntropyLoss​ torch.nn.CrossEntropyLoss相当于torch.softmax + torch.log + torch.nn.nllloss. import torch.nn as nn # 使用NLLLoss实现 nllloss = nn.NLLLoss() predict = torch.Tensor([[2, 3, 1], [3, 7, 9]]) predict = torch.log(torch.softmax(predict, dim=-1)) label = torch.tensor([1, 2]) nllloss(predict, label) # output: tensor(0.2684) # 使用CrossEntropyLoss实现 cross_loss = nn.CrossEntropyLoss() predict = torch.Tensor([[2, 3, 1], [3, 7, 9]]) label = torch.tensor([1, 2]) cross_loss(predict, label) # output: tensor(0.2684)","s":"Cross Entropy Loss 交叉熵损失","u":"/en/docs/Deep-Learning/基础知识/激活函数与Loss的梯度","h":"#cross-entropy-loss-交叉熵损失","p":1198},{"i":1210,"t":"tip 正则化与权重衰退","s":"正则化与权重衰退","u":"/en/docs/Deep-Learning/基础知识/正则化与权重衰退","h":"","p":1209},{"i":1212,"t":"正则化(Regularization)是机器学习中用于控制模型过拟合的一种技术。在模型训练过程中，我们通常要最小化一个损失函数来得到最佳的模型参数。但是当模型过于复杂时，容易出现过拟合现象，即在训练数据上表现很好，但在测试数据上表现很差。这是因为模型过于依赖训练数据的噪声和细节，而忽略了真正的规律。 正则化通过在损失函数中增加一个**惩罚项(Penalty)**来对模型进行约束，防止其过分依赖训练数据。 常见的正则化方法包括L1正则化(硬性限制)、L2正则化(柔性限制)等。 L1正则化会使得一部分参数变为0，从而实现特征选择的效果；L2正则化则会使得模型参数尽量接近0，也就是使得模型更加平滑。在使用正则化时，需要调整正则化强度的超参数，以达到最优的泛化性能。","s":"什么是正则化","u":"/en/docs/Deep-Learning/基础知识/正则化与权重衰退","h":"#什么是正则化","p":1209},{"i":1214,"t":"min l(w,b) subject to ∥w∥12≤θ(1)min \\space l(w, b) \\space \\text{subject to} \\space \\Vert w \\Vert^2_1 \\leq \\theta \\tag{1}min l(w,b) subject to ∥w∥12​≤θ(1) L1正则化限制权重参数的L1范数小于某一特定的超参数 通常不限制偏移bbb 更小的超参数θ\\thetaθ意味着更强的正则项","s":"L1正则化","u":"/en/docs/Deep-Learning/基础知识/正则化与权重衰退","h":"#l1正则化","p":1209},{"i":1216,"t":"L2正则化是指在模型的损失函数中，加入对模型参数的L2范数进行惩罚的一种方法。公式如下所示： l(w,b)+λ2∥w∥12(2)l(w, b) + \\frac{\\lambda}{2} \\Vert w \\Vert^2_1 \\tag{2}l(w,b)+2λ​∥w∥12​(2) 其中，λ\\lambdaλ是一个正则化系数超参数 此时在更新梯度时，具有如下公式 ∂∂w(l(w,b)+λ2∥w∥12)=∂l(w,b)∂w+λw(3)\\frac{\\partial}{\\partial w} \\big(l(w, b) + \\frac{\\lambda}{2} \\Vert w \\Vert^2_1 \\big) = \\frac{\\partial l(w, b)}{\\partial w} + \\lambda w \\tag{3}∂w∂​(l(w,b)+2λ​∥w∥12​)=∂w∂l(w,b)​+λw(3)wt+1=(1−ηλ)wt+η∂l(wt,bt)∂wt(4)w_{t+1}=(1-\\eta \\lambda)w_t + \\eta \\frac{\\partial l(w_t, b_t)}{\\partial w_t} \\tag{4}wt+1​=(1−ηλ)wt​+η∂wt​∂l(wt​,bt​)​(4) 通常ηλ<1\\eta \\lambda < 1ηλ<1，因此又叫做权重衰退","s":"L2正则化与权重衰退","u":"/en/docs/Deep-Learning/基础知识/正则化与权重衰退","h":"#l2正则化与权重衰退","p":1209},{"i":1219,"t":"AlexNet是指2012年由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton提出的一种卷积神经网络模型，它主要应用于图像分类任务。在当时，AlexNet的表现远远超过了其他参赛的网络模型，并且在ImageNet比赛中获得了第一名。 标志着新的一轮神经网络热潮的开始","s":"背景","u":"/en/docs/Deep-Learning/基础知识/AlexNet","h":"#背景","p":1217},{"i":1221,"t":"ReLU激活函数 Dropout正则化、丢弃法 最大池化MaxPooling","s":"新的概念和技术","u":"/en/docs/Deep-Learning/基础知识/AlexNet","h":"#新的概念和技术","p":1217},{"i":1223,"t":"由于输入的图片更大，设置了更大的卷积核尺寸和步长 更大的池化窗口，使用最大池化 在卷积层中设置了更大的输出通道，提取更深层的特征、识别更多的模式 激活函数从Sigmoid改成了ReLU，减缓梯度消失 在卷积层和输出层之间仍使用两个全连接隐藏层，但在输出层之前增加了Dropout层做正则化 使用了数据增强data augmentation","s":"与LeNet比较","u":"/en/docs/Deep-Learning/基础知识/AlexNet","h":"#与lenet比较","p":1217},{"i":1226,"t":"K-fold cross-validation is a technique used in machine learning to evaluate the performance of a model. The basic idea behind k-fold cross-validation is to split the dataset into kkk partitions, or folds, and then train and test the model kkk times, using a different fold for testing each time.","s":"What is k-fold cross-validation?","u":"/en/docs/Deep-Learning/基础知识/K-foldCross-validation","h":"#what-is-k-fold-cross-validation","p":1224},{"i":1228,"t":"In each iteration of k-fold cross-validation, one of the kkk folds is used as the test set, while the remaining k−1k-1k−1 folds are used as the training set. This process is repeated kkk times, with each fold being used exactly once as the test set. The results from each iteration can then be averaged to produce a more accurate estimate of the model's performance.","s":"How does k-fold cross-validation work?","u":"/en/docs/Deep-Learning/基础知识/K-foldCross-validation","h":"#how-does-k-fold-cross-validation-work","p":1224},{"i":1230,"t":"train set: to train the model and do parameter update validation set: to choose hyperparameter test set: the final test, only used once","s":"Summary","u":"/en/docs/Deep-Learning/基础知识/K-foldCross-validation","h":"#summary","p":1224},{"i":1233,"t":"kh=kw=1k_h=k_w=1kh​=kw​=1的卷积不识别空间模式，丢弃了空间信息，只是融合通道 相当于输入形状为HW×ciHW \\times c_iHW×ci​，权重形状为co×cic_o \\times c_ico​×ci​的全连接层","s":"1x1 卷积","u":"/en/docs/Deep-Learning/基础知识/ConvolutionalLayer","h":"#1x1-卷积","p":1231},{"i":1235,"t":"输入：ci×H×Wc_i \\times H \\times Wci​×H×W 核：co×ci×kh×kwc_o \\times c_i \\times k_h \\times k_wco​×ci​×kh​×kw​ 偏差：co×cic_o \\times c_ico​×ci​ 输出：co×H′×W′c_o \\times H' \\times W'co​×H′×W′ 输出H′以及W′H'以及W'H′以及W′的计算： shapeoutput=shapeinput−sizekernel+2∗paddingstride+1(1)shape_{output} = \\frac{shape_{input}-size_{kernel}+2*padding}{stride}+1 \\tag{1}shapeoutput​=strideshapeinput​−sizekernel​+2∗padding​+1(1) 计算复杂度：O(co×ci×H×W×H′×W′)O(c_o \\times c_i \\times H \\times W \\times H' \\times W')O(co​×ci​×H×W×H′×W′) 总结： 输出的通道数是卷积层的超参数 每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出结果 每个输出通道有独立的三维卷积核","s":"二维卷积层","u":"/en/docs/Deep-Learning/基础知识/ConvolutionalLayer","h":"#二维卷积层","p":1231},{"i":1238,"t":"现代图片具有较大的像素，使用全连接层导致参数爆炸 针对图片的特征提取和模式识别，应具备以下原则： 平移不变性 局部性","s":"卷积的诞生&核心特征","u":"/en/docs/Deep-Learning/基础知识/FromFullyConnectedLayerToConvolutionalLayer","h":"#卷积的诞生核心特征","p":1236},{"i":1240,"t":"将全连接层的一维输入和输出变换为二维的矩阵，公式如下 其中i,ji,ji,j代表输出神经元的二维索引坐标，h,wh,wh,w代表输入神经元的二维索引坐标 yi,j=∑h,wwi,j,h,w∗xh,w(1)y_{i,j}=\\sum_{h,w}{w_{i,j,h,w}*x_{h,w}} \\tag{1}yi,j​=h,w∑​wi,j,h,w​∗xh,w​(1) 进一步将权重以及输入的索引变形，公式如下 其中a,ba,ba,b的取值可负可正，直到遍历所有权重以及输入神经元，实现全连接 yi,j=∑h,wwi,j,h,w∗xh,w=∑a,bvi,j,a,b∗xi+a,j+b(2)y_{i,j}=\\sum_{h,w}{w_{i,j,h,w}*x_{h,w}}=\\sum_{a,b}{v_{i,j,a,b}*x_{i+a,j+b}} \\tag{2}yi,j​=h,w∑​wi,j,h,w​∗xh,w​=a,b∑​vi,j,a,b​∗xi+a,j+b​(2) 在公式(2)中，当i,ji,ji,j发生变化时，即产生平移，权重也发生平移，不满足平移不变性。 为了解决这一问题，将公式(2)变形为如下 此时参数权值共享，满足了平移不变性 yi,j=∑a,bvi,j,a,b∗xi+a,j+b=∑a,bva,b∗xi+a,j+b(3)y_{i,j}=\\sum_{a,b}{v_{i,j,a,b}*x_{i+a,j+b}}=\\sum_{a,b}{v_{a,b}*x_{i+a,j+b}} \\tag{3}yi,j​=a,b∑​vi,j,a,b​∗xi+a,j+b​=a,b∑​va,b​∗xi+a,j+b​(3) 再考虑局部性，在进行特征提取以及模式识别时，只需关注周围的局部特征，因此公式(3)中的a,ba,ba,b可缩小范围，并不用来实现全连接，此时a,ba,ba,b代表着卷积核的感受野，即kernel size 此时完成了全连接层到卷积层的转换","s":"重新考察全连接层","u":"/en/docs/Deep-Learning/基础知识/FromFullyConnectedLayerToConvolutionalLayer","h":"#重新考察全连接层","p":1236},{"i":1242,"t":"对全连接层使用平移不变性和局部性得到卷积层，卷积是特殊的全连接 yi,j=∑a,bva,b∗xi+a,j+b=∑a=−ΔΔ∑b=−ΔΔva,b∗xia,j+b(4)y_{i,j}=\\sum_{a,b}{v_{a,b}*x_{i+a,j+b}}=\\sum_{a=-\\Delta}^{\\Delta}\\sum_{b=-\\Delta}^{\\Delta}{v_{a,b}*x_{i_a,j+b}} \\tag{4}yi,j​=a,b∑​va,b​∗xi+a,j+b​=a=−Δ∑Δ​b=−Δ∑Δ​va,b​∗xia​,j+b​(4) 卷积层输出形状的计算 shapeoutput=shapeinput−sizekernel+2∗paddingstride+1(5)shape_{output} = \\frac{shape_{input}-size_{kernel}+2*padding}{stride}+1 \\tag{5}shapeoutput​=strideshapeinput​−sizekernel​+2∗padding​+1(5)","s":"总结","u":"/en/docs/Deep-Learning/基础知识/FromFullyConnectedLayerToConvolutionalLayer","h":"#总结","p":1236},{"i":1245,"t":"Logistic Regression直译为逻辑回归，是一种用来解决二分类问题的机器学习方法，用于估计某种事物的可能性。 逻辑回归经过sigmoid函数输出的结果可将其视为probability，而后根据设定的置信度阈值来判断该特征向量对应的标签是1还是0，用以解决二分类问题。","s":"什么是Logistic Regression","u":"/en/docs/Deep-Learning/基础知识/LogisticRegression","h":"#�什么是logistic-regression","p":1243},{"i":1247,"t":"线性回归要求因变量是连续性数值变量，而逻辑回归要求因变量是离散的变量。 逻辑回归以线性回归为理论支持，通过Sigmoid函数引入了非线性因素。 线性回归常用MSE函数作为损失函数，而逻辑回归作为分类任务的解决方案通常搭配交叉熵损失函数进行训练。","s":"逻辑回归（Logistic Regression）和线性回归（Linear Regression）","u":"/en/docs/Deep-Learning/基础知识/LogisticRegression","h":"#逻辑回归logistic-regression和线性回归linear-regression","p":1243},{"i":1249,"t":"从历史角度方面看，逻辑回归在诞生时使用MSE作为损失函数，其目标是让输出的概率更接近于1，与回归任务的目标相似。","s":"逻辑回归到底是回归任务（Regression）还是分类任务（Classification）？","u":"/en/docs/Deep-Learning/基础知识/LogisticRegression","h":"#逻辑回归到底是回归任务regression还是分类任务classification","p":1243},{"i":1251,"t":"逻辑回归以及其他分类任务在测试角度上的目标让提高分类准确率acc，但并不会将maximize accuracy作为数学上的训练方法，即在训练过程中不使用与acc有关的损失函数。 逻辑回归中的训练目标（评估函数）与预测目标（评估函数）并不相同，但方向一致。 acc=∑I(predi==yi)len(Y)(1)acc = \\frac{\\sum{I(pred_i==y_i)}}{len(Y)} \\tag{1}acc=len(Y)∑I(predi​==yi​)​(1) 如果在训练过程中以最大化acc为目标，当参数在训练过程中向标签方向更新使得逻辑回归输出的正确类的概率增大时，考虑以下两种情况： gradient = 0 if accuracy unchanged but weights changed: xxxxxxxxxx19 1a = torch.tensor([1, 2, 3])2b = torch.tensor([4, 5, 6])3c = zip(a, b)4for i in c:5 print(i)6'''7(tensor(1), tensor(4))8(tensor(2), tensor(5))9(tensor(3), tensor(6))10'''11a = torch.tensor([[1, 2, 3], [3, 2, 1]])12b = torch.tensor([[4, 5, 6], [6, 5, 4]])13c = zip(a, b)14for i in c:15 print(i)16'''17(tensor([1, 2, 3]), tensor([4, 5, 6]))18(tensor([3, 2, 1]), tensor([6, 5, 4]))19'''python gradient not continuous since the number of correct is not continunous: 当上一轮迭代的输出概率很接近阈值时，下一次迭代的概率提升了很少一点但是仍超过了阈值，且一个batch中有大量样本均存在这种情况，此时acc有显著提升而网络的权重的更新极小，此时，与acc有关的Loss函数对权重求导得到的梯度会出现梯度爆炸或者说不连续的情况。","s":"为什么逻辑回归或其他分类任务不使用分类准确率作为损失函数？","u":"/en/docs/Deep-Learning/基础知识/LogisticRegression","h":"#为什么逻辑回归或其他分类任务不使用分类准确率作为损失函数","p":1243},{"i":1254,"t":"卷积层会对输入的局部区域进行卷积操作，因此对于输入图像中的每个位置都会产生一个响应。然而，在某些情况下，我们并不关心输入图像中每个位置的细节，而只是想获取该区域的一些重要特征。 假设我们想分类一张猫的图片，那么我们可能只需要提取出它的眼睛、鼻子、嘴巴和耳朵等特征，而不必考虑这些特征在图像中的精确位置。","s":"卷积对像素位置信息是敏感的","u":"/en/docs/Deep-Learning/基础知识/PoolingLayer","h":"#卷积对��像素位置信息是敏感的","p":1252},{"i":1256,"t":"池化层通过对输入的局部区域进行降采样操作，减少了特征图的大小，从而使得模型对于输入位置的微小变化更加鲁棒。例如，如果我们将一个对象稍微平移一点，它依然可以被正确地识别，因为池化层可以保留输入图像的关键特征，而忽略掉微小的位置变化。 但是需要注意的是，当池化的步幅和池化区域的大小过大时，会导致模型丢失较多的细节信息，从而影响模型性能。因此，在实际应用中，需要根据具体任务来选择适当的池化参数。 缓解卷积层对位置的敏感性，提高鲁棒：池化操作通常用于卷积层之后，使模型对于输入位置的微小变化更加鲁棒，减少图像中的噪声和冗余信息 减小特征图大小：池化操作会通过在特定位置上合并特征值来缩小输入特征图的空间大小，降低计算开销。 减少参数数量：池化操作减小了特征图的空间大小，从而也减小了需要训练的权重参数数量，更容易训练和优化。","s":"池化层的作用","u":"/en/docs/Deep-Learning/基础知识/PoolingLayer","h":"#池化层的作用","p":1252},{"i":1258,"t":"池化层将输入特征图分割成若干个区域，然后对每个区域进行汇聚操作，将该区域内的特征值合并成一个值。这个操作可以使用不同的方法实现，如最大值池化、平均值池化等。 最常见的是最大值池化，其中每个区域的输出值是该区域内特征值的最大值，这样可以保留图像中最显著的特征，同时减少噪声和冗余信息的影响。","s":"池化的实现","u":"/en/docs/Deep-Learning/基础知识/PoolingLayer","h":"#池化的实现","p":1252},{"i":1261,"t":"感知机是一种二元线性分类模型，旨在寻找一个超平面（在二维空间中即为一条直线），将不同类别的实例划分到不同的区域。感知机的训练过程包括迭代地对样本进行分类，并根据分类错误的情况调整超平面的参数，使得分类准确率逐步提高。感知机是基础的机器学习算法之一，其思想和方法对神经网络等更复杂的模型也具有启发意义。","s":"什么是感知机","u":"/en/docs/Deep-Learning/基础知识/Perceptron","h":"#什么是感知机","p":1259},{"i":1263,"t":"输入向量：感知机的输入向量是一个n维向量x=(x1,x2,...,xn)x=(x_1,x_2,...,x_n)x=(x1​,x2​,...,xn​)，表示一个样本的各个特征值。 权值向量：感知机的权值向量也是一个n维向量w=(w1,w2,...,wn)w=(w_1,w_2,...,w_n)w=(w1​,w2​,...,wn​)，表示每个特征对应的权重。 偏置项：偏置项bbb是一个常数，可看作是模型的截距，用于调整阈值函数的位置。 内积运算：感知机将输入向量和权值向量进行内积运算，并加上偏置项，得到输入信号z=w∗x+bz=w*x+bz=w∗x+b。 阈值函数：将输入信号zzz带入阈值函数，如符号函数sign(z)sign(z)sign(z)，即可得到分类结果。 损失函数：感知机使用误分类点到超平面的距离来作为损失函数，即 L(y,z)=max(0,−y∗z)(1)L(y,z)=max(0,-y*z) \\tag{1}L(y,z)=max(0,−y∗z)(1) 其中yyy是样本的真实标签，zzz是预测值。 参数更新：根据当前样本误分类情况来对权值向量www和偏置项bbb进行迭代更新。 收敛条件：当全部训练样本被正确分类或达到最大迭代次数时，感知机算法停止迭代。 感知机训练流程伪代码如下所示： initialize w = 0 and b = 0 repeat if yi * zi <= 0 then w = w + yi * xi and b = b + yi end if until all classified correctly","s":"详细原理","u":"/en/docs/Deep-Learning/基础知识/Perceptron","h":"#详细原理","p":1259},{"i":1265,"t":"感知机是一个二分类模型，最早的AI模型之一 求解算法等价于使用批量大小为1的梯度下降 要求数据集线性可分，不能拟合XOR异或等非线性问题，导致第一次AI寒冬","s":"总结","u":"/en/docs/Deep-Learning/基础知识/Perceptron","h":"#总结","p":1259},{"i":1268,"t":"LeNet是由Yann LeCun等人于1998年提出的卷积神经网络结构，该结构由卷积层、池化层和全连接层组成，可以高效地处理手写数字图像，并在MNIST数据集上取得了很好的性能。 LeNet-5的成功标志着卷积神经网络在计算机视觉领域中的崛起，并促进了深度学习的快速发展。","s":"背景","u":"/en/docs/Deep-Learning/基础知识/LeNet","h":"#背景","p":1266},{"i":1270,"t":"import torch import numpy as np from torch import nn as nn from torch.nn import functional as F from d2l import torch as d2l from matplotlib import pyplot as plt import os os.environ['http_proxy'] = 'http://127.0.0.1:7890' os.environ['https_proxy'] = 'https://127.0.0.1:7890' class LeNetReshape(nn.Module): def __init__(self): super(LeNetReshape, self).__init__() def forward(self, x): return x.reshape(-1, 1, 28, 28) class LeNet5(nn.Module): def __init__(self): super(LeNet5, self).__init__() self.net = torch.nn.Sequential( LeNetReshape(), # 激活函数应为Sigmoid nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.LeakyReLU(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), nn.LeakyReLU(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(16 * 5 * 5, 120), nn.LeakyReLU(), nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10)) def forward(self, x): return self.net(x) def evaluate_accuracy_gpu(net, data_iter, device=None): if isinstance(net, torch.nn.Module): net.eval() if not device: device = next(iter(net.parameters())).device metric = d2l.Accumulator(2) for X, y in data_iter: if isinstance(X, list): X = [x.to(device) for x in X] else: X = X.to(device) y = y.to(device) metric.add(d2l.accuracy(net(X), y), y.numel()) # 此处accuracy是统计 return metric[0] / metric[1] def accuracy(y_hat, y): return torch.sum(y_hat.argmax(dim=1) == y) def train(net, train_iter, test_iter, num_epochs, lr, device): def init_weights(m): if type(m) == nn.Linear or type(m) == nn.Conv2d: nn.init.xavier_uniform_(m.weight) net.apply(init_weights) net.to(device) optimizer = torch.optim.SGD(net.parameters(), lr=lr) loss = torch.nn.CrossEntropyLoss() loss.to(device) animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], legend=['train loss', 'train acc', 'test acc']) timer, num_batches = d2l.Timer(), len(train_iter) metric = d2l.Accumulator(3) net.train() for epoch in range(num_epochs): for batch, (X, y) in enumerate(train_iter): timer.start() optimizer.zero_grad() X, y = X.to(device), y.to(device) y_hat = net(X) l = loss(y_hat, y) l.backward() optimizer.step() metric.add(l * X.shape[0], accuracy(y_hat, y), y.numel()) timer.stop() train_l = metric[0] / metric[2] train_acc = metric[1] / metric[2] if (batch + 1) % (num_batches // 5) == 0 or batch == num_batches - 1: animator.add(epoch + (batch + 1) / num_batches, (train_l, train_acc, None)) test_acc = evaluate_accuracy_gpu(net, test_iter) animator.add(epoch + 1, (None, None, test_acc)) print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, test acc {test_acc:.3f}') print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on {str(device)}') plt.show() batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) lr, num_epochs = 0.9, 10 lenet = LeNet5() train(lenet, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())","s":"代码实现","u":"/en/docs/Deep-Learning/基础知识/LeNet","h":"#代码实现","p":1266},{"i":1272,"t":"在分类模型中，最后两个全连接层之间不要使用ReLU激活函数。因为ReLU的范围是[0, +∞)，它会将所有负数都变成0。而最后一层全连接层输出了类别信息，倒数第二层的输出值包含着非常重要的类别信息，此时使用激活函数很可能会导致信息丢失。","s":"问题","u":"/en/docs/Deep-Learning/基础知识/LeNet","h":"#问题","p":1266},{"i":1275,"t":"concat与stack函数 stack函数对输入的两个张量在指定的维度进行堆叠，是创建了新的维度 concat函数对输入的张量在指定维度进行拼接，没有创建新的维度 # stack和concat函数 a = torch.rand(4, 3) # A班4位同学，每位同学3科成绩 b = torch.rand(4, 3) # B班4位同学，每位同学3科成绩 c = torch.stack((a, b), dim=0) # 理解：年级所有同学的3科成绩（假设年级只有A班和B班两个班，每个班只有四名同学） print(c.shape) # torch.Size([2, 4, 3]) d = torch.concat((a, b), dim=1) # 理解：a是A班4位同学3科成绩，b是这4名同学其他3门课的成绩，拼接后代表这4名同学的6科成绩 print(d.shape) # torch.Size([4, 6]) list和tensor乘法不同之处 list的*乘法是复制元素，改变list的shape tensor的*乘法是对tensor中的元素进行点乘计算 a = torch.tensor([[3, 3, 3, 3]]) b = [3] # list的*乘是复制元素进行扩展 print(a * 3) # tensor([[9, 9, 9, 9]]) print(b * 3) # [3, 3, 3] 最大值 / 最小值索引：argmax / argmin 需要通过参数dim指定操作的维度，dim的理解 官方解释：The dimension to reduce 以二维张量举例，dim=1即在每一行中选出一个最大值 / 最小值元素的索引，索引的shape应为[dim0, 1]，即reduce了dim=1的维度 # 最大值最小值索引 a = torch.tensor([[0.1, 0.9, 0.3], [0.9, 0.8, 0.99], [0.1, 0.7, 0.8], [0.88, 0.1, 0.2]]) # [4, 3] print(\"argmax output: \", a.argmax(dim=0), a.argmax(dim=1)) # argmax output: tensor([1, 0, 1]) tensor([1, 2, 2, 0]) Python zip函数 zip函数可以理解为压缩，将输入的两个迭代器的最外层对应元素压缩为一个新的元素 a = torch.tensor([1, 2, 3]) b = torch.tensor([4, 5, 6]) c = zip(a, b) for i in c: print(i) ''' (tensor(1), tensor(4)) (tensor(2), tensor(5)) (tensor(3), tensor(6)) ''' a = torch.tensor([[1, 2, 3], [3, 2, 1]]) b = torch.tensor([[4, 5, 6], [6, 5, 4]]) c = zip(a, b) for i in c: print(i) ''' (tensor([1, 2, 3]), tensor([4, 5, 6])) (tensor([3, 2, 1]), tensor([6, 5, 4])) '''","s":"常用函数部分","u":"/en/docs/Deep-Learning/基础知识/PytorchBasics","h":"#常用函数部分","p":1273},{"i":1277,"t":"原文链接：https://arxiv.org/pdf/2406.11838 参考资料：何恺明：Autoregressive Image Generation without Vector Quantization.","s":"Autoregressive Image Generation without Vector Quantization","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"","p":1276},{"i":1279,"t":"Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications.","s":"摘要","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#摘要","p":1276},{"i":1281,"t":"由于语言先天的离散性质，在自然语言领域的自回归模型都是在离散的隐空间中学习数据的概率分布。因此，当自回归模型推广到连续的数据空间（如图像生成）进行发展时，研究人员将主要的精力放在了如何更有效地将连续数据离散化上，如 VQGAN、VQVAE、VAR、MaskGIT 等。 然而，本文作者从 Autoregressive 模型的性质本身（即根据先前的 token 预测下一个 token）出发进行思考，产生了自回归模型是否有必要与向量量化（Vector Quantization）表示相结合的疑问，作者发现，自回归生成模型与 token 是离散表示的还是连续表示的并没有关系，真正需要解决的问题是如何对每个 token 的概率分布进行建模，同时该概率分布可以通过损失函数来学习和约束（训练），并用于从中进行采样（生成）。","s":"研究动机","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#研究动机","p":1276},{"i":1283,"t":"本文提出的模型融合了目前图像生成领域的三大主流范式的思想，分别为自回归生成、掩码生成以及扩散生成： 模型首先根据自回归模型性质为每个 token 生成条件向量（conditional vector）zzz 之后使用扩散过程建模每个 token 的概率分布 最后结合掩码生成的思想加速训练和推理过程","s":"主要方法与实现","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#主要方法与实现","p":1276},{"i":1285,"t":"具体而言，如下图所示，假设连续值向量 x∈Rdx \\in \\mathbb{R}^dx∈Rd，是当前位置要预测的 token。自回归模型在这个位置产生一个条件向量 z∈RDz \\in \\mathbb{R}^Dz∈RD​​ 。 本文使用 Diffusion Loss 根据 zzz 对 xxx 建模概率分布 p(x∣z)p(x \\mid z)p(x∣z)​ 。 L(z,x)=Eε,t[∥ε−εθ(xt∣t,z)∥2]L(z, x)=\\mathbb{E}_{\\varepsilon, t}\\left[\\left\\|\\varepsilon-\\varepsilon_\\theta\\left(x_t \\mid t, z\\right)\\right\\|^2\\right]L(z,x)=Eε,t​[∥ε−εθ​(xt​∣t,z)∥2] 其中： ε∈Rd\\varepsilon \\in \\mathbb{R}^dε∈Rd 是从 N(0,I)\\mathcal{N}(0, \\mathbf{I})N(0,I) 采样的噪声向量 xt=αˉtx+1−αˉtεx_t=\\sqrt{\\bar{\\alpha}_t x}+\\sqrt{1-\\bar{\\alpha}_t} \\varepsilonxt​=αˉt​x​+1−αˉt​​ε，其中 αˉt\\bar{\\alpha}_tαˉt​ 定义了一个 noise schedule ttt 是 noise schedule 的时间步 噪声预测网络 εθ\\varepsilon_\\thetaεθ​ 由参数 θ\\thetaθ 控制，是一个小型 MLP 网络，εθ(xt∣t,z)\\varepsilon_\\theta\\left(x_t \\mid t, z\\right)εθ​(xt​∣t,z) 表示此网络将 xtx_txt​ 作为输入，并且以 ttt 和 zzz​​​ 作为条件。MLP 网络的具体组成如下所示： 对于扩散过程，作者采用了 cosine 函数作为 noise schedule，在训练过程中设定为 1000 步，在推理过程中设定为 100 步。同时，Diffusion Loss 原生支持 CFG 策略。","s":"Diffusion Loss","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#diffusion-loss","p":1276},{"i":1287,"t":"在推理时，需要从分布 p(x∣z)p(x \\mid z)p(x∣z) 中采样样本。采样是通过逆扩散过程进行的： xt−1=1αt(xt−1−αt1−αˉtϵθ(xt∣t,z))+σtδx_{t-1}=\\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta\\left(x_t \\mid t, z\\right)\\right)+\\sigma_t \\deltaxt−1​=αt​​1​(xt​−1−αˉt​​1−αt​​ϵθ​(xt​∣t,z))+σt​δ 这里 δ\\deltaδ 是从高斯分布 N(0,I)\\mathcal{N}(0, I)N(0,I) 中采样的， σt\\sigma_tσt​ 是时间步 ttt 的噪声水平。以 xT∼N(0,I)x_T \\sim \\mathcal{N}(0, I)xT​∼N(0,I) 开始，此过程生成一个样本 x0x_0x0​ 使得 x0∼p(x∣z)x_0 \\sim p(x \\mid z)x0​∼p(x∣z)​。","s":"采样","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#采样","p":1276},{"i":1290,"t":"本文为了提高生成速度，结合了 MAE 和 MaskGIT 模型的思想，如下图的 b 和 c 所示，将随机顺序生成的思想和每次预测出一个 patch 的 token 的思想加入到本文的模型中。 掩码信息通过位置编码送入 Decoder 中，使模型在以随机顺序预测序列时获取到 token 的位置信息。","s":"结合掩码生成模型的思想 MAR","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#结合掩码生成模型的思想-mar","p":1276},{"i":1297,"t":"对于 MAR 模型，AR 步数越小，每次预测的一个 patch 的 token 数越大，速度越快，精度越低，默认设置为 64 步。","s":"MAR 模型速度与效果的 trade-off","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#mar-模型速度与效果的-trade-off","p":1276},{"i":1299,"t":"相关链接 论文：arXiv 参考资料： Transformer模型详解（图解最完整版） 【機器學習2021】Transformer (下) Transformer是Sequence-to-Sequence (Seq2Seq) 模型，模型的输入是向量序列，输出同样是向量序列，且输出的长度由模型经过学习决定。","s":"NeurIPS 2017: Attention Is All You Need","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"","p":1298},{"i":1301,"t":"Transformer由Encoder和Decoder组成，编码器和解码器都包含6个Block，整体结构如下图所示。","s":"整体结构","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#整体结构","p":1298},{"i":1304,"t":"Transformer Encoder结构如下图所示。其中，Add指的是残差连接Residual Connection，Norm指的是Layer Normalization。","s":"整体结构","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#整体结构-1","p":1298},{"i":1306,"t":"对于输入的句子，对一个词汇的嵌入向量的奇数维度使用sine函数进行编码，对偶数维度使用cosine函数计算编码。 公式如下所示，其中pospospos指的是该词汇在整个输入句子中的位置，2i2i2i以及2i+12i+12i+1指的是该词汇的嵌入向量中的维度，dmodeld_{model}dmodel​指的是在嵌入层之后嵌入向量的总维度。即对于每个输入词汇，都要计算dmodeld_{model}dmodel​次位置编码。 PE(pos,2i)=sin(pos100002i/dmodel)(1)PE_{(pos,2i)}=sin(\\frac{pos}{10000^{2i/d_{\\mathrm{model}}}}) \\tag{1}PE(pos,2i)​=sin(100002i/dmodel​pos​)(1)PE(pos,2i+1)=cos(pos100002i/dmodel)(2)PE_{(pos,2i+1)}=cos(\\frac{pos}{10000^{2i/d_{\\mathrm{model}}}}) \\tag{2}PE(pos,2i+1)​=cos(100002i/dmodel​pos​)(2) 根据三角函数的性质，对于pos+kpos+kpos+k位置的嵌入向量的某一维度（2i2i2i或2i+12i+12i+1）而言，可以表示为pospospos位置与kkk位置的嵌入向量的2i2i2i与2i+12i+12i+1维度的线性组合，使得位置向量中蕴含了相对位置的信息。 PE(pos+k,2i)=PE(pos,2i)×PE(k,2i+1)+PE(pos,2i+1)×PE(k,2i)PE(pos+k,2i+1)=PE(pos,2i+1)×PE(k,2i+1)−PE(pos,2i)×PE(k,2i)(3)\\begin{array}{l}PE(pos+k,2i)=PE(pos,2i)\\times PE(k,2i+1)+PE(pos,2i+1)\\times PE(k,2i)\\\\PE(pos+k,2i+1)=PE(pos,2i+1)\\times PE(k,2i+1)-PE(pos,2i)\\times PE(k,2i)\\end{array} \\tag{3}PE(pos+k,2i)=PE(pos,2i)×PE(k,2i+1)+PE(pos,2i+1)×PE(k,2i)PE(pos+k,2i+1)=PE(pos,2i+1)×PE(k,2i+1)−PE(pos,2i)×PE(k,2i)​(3) 最终，位置编码向量的维度与词汇的嵌入维度相同，进行element-wise的相加操作。 InputEmbedding(pos,i)=WordEmbedding(pos,i)+PositionEncoding(pos,i)(4)InputEmbedding(pos,i)=WordEmbedding(pos,i)+PositionEncoding(pos,i) \\tag{4}InputEmbedding(pos,i)=WordEmbedding(pos,i)+PositionEncoding(pos,i)(4)","s":"位置编码（Positional Encoding）","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#位置编码positional-encoding","p":1298},{"i":1308,"t":"输入向量由Word Embedding和Positional Embedding相加得到。输入序列经过Mutil-Head Self-Attention之后，通过Residual Connection加上自身的输入向量，再经过Layer Normalization，之后送入FCN并进行Residual Connection加上送入FCN的输入自身，最终再进行Layer Normalization，以上构成了一个Encoder Block。每一个Block输出的向量序列长度等于输入的向量序列长度。","s":"具体结构","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#具体结构","p":1298},{"i":1310,"t":"Decoder的任务是生成输出，可以根据是否一次性生成输出分为Autoregressive（自回归，abbr. AT）以及Non-Autoregressive（非自回归，abbr. NAT）两种模式。 自回归类型的Decoder需要逐步生成输出，并将之前自身输出的所有词汇经过嵌入层后生成token作为下一次的输入，通常每次生成一个词或一个符号。这种方式的缺点是需要保存和更新词表中的所有可能选项，因此在大词汇表上可能会变得非常慢。然而，它的优点是能够利用上下文信息来生成输出，这有助于提高翻译的质量。 非自回归类型的Decoder试图在一次操作中生成整个输出序列。这通常通过使用诸如注意力机制等策略来实现，这些策略允许解码器关注输入序列的不同部分，同时生成输出序列的不同部分。NAT的优点在于其高效性，因为它不需要保存和更新大量的可能选项。然而，由于它不能利用上下文信息来生成输出，因此其生成的输出质量普遍会低于AT。","s":"Decoder","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#decoder","p":1298},{"i":1312,"t":"整体结构​ 词汇表（Vocabulary）​ 词汇表（Vocabulary）是一个包含了在特定语言或任务中所有可能出现的所有单词或标记的集合。在自然语言处理（NLP）中，词汇表是训练模型时所使用的唯一单词的集合，由具体的生成任务而确定。 Decoder每一步的输出是一个经过Softmax的Probability Distribution（概率分布），代表着词汇表中每一个词汇当前生成的概率，取最大概率值的词汇便是模型当前时间步输出的词汇。 Begin符号​ 解码器（Decoder）在每个时间步（或每个解码步骤）的输入都来自于前一个时间步自身的输出以及编码器（Encoder）的输出。特别地，首个时间步的输入是Begin符号以及编码器（Encoder）的输出，在每个后续的时间步，解码器的输入会是前一个时间步自身的输出以及编码器（Encoder）的输出，直到生成序列的结束。 特殊符号 Begin符号是在Lexicon中添加的特殊符号，用来表示Decoder生成的开始。Begin符号通常被嵌入到一个低维的连续向量空间中，这个向量空间是通过嵌入层（Embedding Layer）学习得到的，在嵌入层中，离散的符号被映射到一个实数向量。 Begin符号又叫Start符号或SOS符号（Start Of Sentence），都是表示生成的开始。End符号又叫EOS符号（End Of Sentence）。 End符号​ 在Decoder的生成中，每一个时间步的输出是词汇表中每一个单词经过Softmax之后的概率分布。为了保证生成任务可以通过模型自己停止而不是一直重复，我们向Decoder的输出中加入End符号的生成，即每一次输出除了词汇表的所有词汇外还有End符号的概率，当End符号是在所有词汇中概率最大的词汇时，生成停止。 掩码多头自注意力机制（Masked Multi-Head Self-Attention）​ 为什么使用掩码多头自注意力 掩码多头自注意力与Transformer训练时采取的Teacher Forcing策略有很大的关系，具体分析见下文《Teacher Forcing与Masked Multi-Head Self-Attention》的讨论环节：Teacher Forcing与Masked Multi-Head Self-Attention 观察Decoder的整体结构，掩码多头自注意力的输入是添加位置编码之后的Decoder当前时间步之前的所有输出单词经过嵌入后的向量表示。 掩码多头自注意力机制用于确保在生成序列的过程中，每个位置只能关注到该位置及其之前的位置。这是通过在Self-Attention的计算中应用一个掩码（mask）来实现的。这确保了在生成序列时，每个位置只能查看到它之前的信息，而不能查看到未来的信息，从而实现了自回归性质。 具体来说，添加掩码后的自注意力机制在生成注意力分数时不再考虑输入序列的所有向量。如在输入向量aia^iai在计算注意力分数时，只将aia^iai的query向量与a1a^1a1至aia^{i}ai的iii个key向量做dot product，而不考虑aia^iai之后的输入的key。 tip 对于第sss个时间步，Masked Mutil-Head Self-Attention的输入是时间步sss之前Decoder生成的所有输出单词的嵌入表示。 交叉注意力（Cross-Attention）​ 交叉注意力是连接Encoder和Decoder的桥梁，也是Decoder输入的重要组成部分。 交叉注意力接收两个输入序列，一个来自编码器（Encoder）的输出序列（通常是输入序列的表示），另一个来自解码器（Decoder），是经过掩码多头自注意力机制的输出序列（通常是正在生成的序列的中间表示）。 在交叉注意力中，每次计算注意力得分的query来自解码器，key和value来自编码器。解码器每个向量的查询（Query）与编码器位置的键（Key）进行点积得到了注意力分数，通过Softmax操作后转换为注意力权重，再与编码器位置的值（Value）weighted sum得到加权注意力分数，最终将加权注意力分数求和得到每个输入向量的输出。","s":"Autoregressive Decoder（AT）","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#autoregressive-decoderat","p":1298},{"i":1316,"t":"在 Transformer 中，Encoder 不像 Decoder 需要生成序列，因此它通常不涉及标签的预测。Encoder 的训练通常是在整个模型中的联合训练中进行的，通过优化整个模型的损失函数来进行。 Transformer 的整体训练过程一般分为以下几个步骤： 编码器（Encoder）的正向传播： 输入序列经过编码器的正向传播，产生一组上下文表示。 解码器（Decoder）的正向传播： 解码器接收上下文表示，并生成目标序列。 计算损失： 通过比较生成的目标序列与实际目标序列，计算损失。在 Decoder 中，通常使用交叉熵损失函数。 反向传播： 根据损失，进行反向传播，更新模型参数。这个过程中，梯度通过整个模型传播，包括 Encoder 和 Decoder。 整个模型的参数（包括 Encoder 和 Decoder）都是通过最小化整体损失来进行联合训练的。这是因为整体模型需要协同工作，Encoder 的表示对于 Decoder 的性能至关重要。在训练过程中，梯度从损失函数传播回整个模型，包括 Encoder 和 Decoder，从而更新它们的参数。 需要注意的是，Transformer 模型通常使用的是端到端的训练方式，整个模型的参数是一次性更新的。在某些场景下，你可能会看到对 Encoder 或 Decoder 进行微调（fine-tuning）的情况，但这是在特定应用场景下的调整，不是 Transformer 模型的标准训练方式。","s":"损失函数","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#损失函数","p":1298},{"i":1318,"t":"在Transformer的推理阶段，自回归类型的Decoder根据分词方式的不同，一个词汇一个词汇的输出，将当前时间步之前生成的所有词汇作为输入load进入Decoder中。但在训练时如果遵从同样的生成范式会大大降低效率，并且面临则一步错步步错的风险（Error Propagation）。 因此使用Teacher Forcing策略，将Ground Truth一次性喂到Decoder中，使模型更快收敛并且避免误差积累的问题。 但是，自回归Decoder在推理时是一个一个词汇产生的，在产生第iii个词汇时其后续的词汇是未知的，更不用说进行注意力分数的就算了，而在训练过程中使用Teacher Forcing时却可以得到第i+1i+1i+1个及其之后词汇的注意力信息，如果不添加其他策略显然会对模型的泛化能力造成很大的影响，而且这并不符合自回归（Autoregression）的特性。为了解决这个问题，掩码多头注意力机制应运而生，在训练阶段将模型在时间发展顺序的右侧的输入masked掉，防止模型学习到不该学习的注意力。","s":"Teacher Forcing","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#teacher-forcing","p":1298},{"i":1320,"t":"参考文献：MultiHead-Attention和Masked-Attention的机制和原理 与Encoder的多头自注意力不同，在Decoder中，为注意力机制应用了掩码，使模型只能关注到当前位置及其之前的位置，而不能访问未来的信息。这解决了引入Teacher Forcing出现的问题，避免了训练与推理阶段的Mismatch，维护了自回归的特性。 具体来说，模拟推理过程中第一个词汇时的场景。当模型只有voc1voc_1voc1​词汇向量输入时，在Decoder中，voc1voc_1voc1​与自身计算注意力分数，于是有 [o1]=[α1,1′][v1](5)\\begin{bmatrix}o_1\\end{bmatrix}=\\begin{bmatrix}\\alpha_{1,1}^{\\prime}\\end{bmatrix}\\begin{bmatrix}v_1\\end{bmatrix}\\tag{5}[o1​​]=[α1,1′​​][v1​​](5) 我们再模拟训练过程中使用Teacher Forcing，一次性输入为两个词汇voc1voc_1voc1​与voc2voc_2voc2​的情况，于是有 [o1o2]=[α1,1′α2,1′α1,2′α2,2′][v1v2](6)\\begin{bmatrix}o_1\\\\o_2\\end{bmatrix}=\\begin{bmatrix}\\alpha_{1,1}^{\\prime}&\\alpha_{2,1}^{\\prime}\\\\\\alpha_{1,2}^{\\prime}&\\alpha_{2,2}^{\\prime}\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix} \\tag{6}[o1​o2​​]=[α1,1′​α1,2′​​α2,1′​α2,2′​​][v1​v2​​](6) 然而，为了使训练过程中符合推理时自回归的特性，理想的输出应该是 [o1o2]=[α1,1′0α1,2′α2,2′][v1v2](7)\\begin{bmatrix}o_1\\\\o_2\\end{bmatrix}=\\begin{bmatrix}\\alpha_{1,1}^{\\prime}&0\\\\\\alpha_{1,2}^{\\prime}&\\alpha_{2,2}^{\\prime}\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix} \\tag{7}[o1​o2​​]=[α1,1′​α1,2′​​0α2,2′​​][v1​v2​​](7) 继续扩展，当有nnn个输入词汇时，应该有 [o1o2⋮on]=[α1,1′0⋯0α1,2′α2′⋯0⋮⋮⋮α1,n′α2,n′⋯αn,n′][v1v2⋮vn](8)\\begin{bmatrix}o_1\\\\o_2\\\\\\vdots\\\\o_n\\end{bmatrix}=\\begin{bmatrix}\\alpha'_{1,1}&0&\\cdots&0\\\\\\alpha'_{1,2}&\\alpha'_2&\\cdots&0\\\\\\vdots&\\vdots&&\\vdots\\\\\\alpha'_{1,n}&\\alpha'_{2,n}&\\cdots&\\alpha'_{n,n}\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{bmatrix}\\tag{8}​o1​o2​⋮on​​​=​α1,1′​α1,2′​⋮α1,n′​​0α2′​⋮α2,n′​​⋯⋯⋯​00⋮αn,n′​​​​v1​v2​⋮vn​​​(8) 因此，我们需要将当前时间步计算的词汇的时间顺序右侧的输入词汇全部掩码，置为0。 在源码中，有如下片段实现掩码： if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = scores.softmax(dim=-1) 在源码中，将mask置为负无穷是因为这是在经过Softmax之前进行的掩码，在经过Softmax之后负无穷小就变成了0。","s":"Teacher Forcing与Masked Multi-Head Self-Attention","u":"/en/docs/Deep-Learning/论文笔记/Attention-Is-All-You-Need","h":"#teacher-forcing与masked-multi-head-self-attention","p":1298},{"i":1322,"t":"相关链接 原文链接：https://arxiv.org/pdf/2112.10752 参考资料：Stable Diffusion 原理介绍与源码分析（一、总览）、Stable Diffusion 原理介绍与源码分析（二、DDPM、DDIM、PLMS算法分析）、Latent Diffusion Models 论文解读","s":"CVPR 2022: High-Resolution Image Synthesis with Latent Diffusion Models","u":"/en/docs/Deep-Learning/论文笔记/Latent-Diffusion-Models","h":"","p":1321},{"i":1324,"t":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.","s":"摘要","u":"/en/docs/Deep-Learning/论文笔记/Latent-Diffusion-Models","h":"#摘要","p":1321},{"i":1327,"t":"相关链接 论文：arXiv 代码：GitHub 参考资料： 54、Probabilistic Diffusion Model概率扩散模型理论与完整PyTorch代码详细解读 【论文精读】Diffusion Model 开山之作DDPM","s":"NeurIPS 2020: Denoising Diffusion Probabilistic Models","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","h":"","p":1326},{"i":1330,"t":"条件概率的定义​ 条件概率是指在给定另一个事件发生的条件下，某一事件发生的概率。条件概率通常用符号P(A∣B)P(A\\mid B)P(A∣B)表示，读作“在 B 发生的条件下 A 发生的概率”。 条件概率的计算公式为： P(A∣B)=P(A,B)P(B)P(A\\mid B)=\\frac{P(A,B)}{P(B)}P(A∣B)=P(B)P(A,B)​ 其中： P(A,B)P(A,B)P(A,B)是事件AAA、BBB同时发生的概率，也叫联合概率 P(B)P(B)P(B)是事件BBB独立发生的概率 全概率公式​ 对于事件AAA而言，假设有一组互斥且穷尽的条件事件B1,B2,…BnB_{1},B_{2},\\ldots B_{n}B1​,B2​,…Bn​构成一个完备事件组，则事件AAA的概率等于事件AAA在每个条件事件BiB_iBi​下发生的概率与该条件事件发生概率的乘积和。 P(A)=∑i=1nP(A∣Bi)⋅P(Bi)P(A)=\\sum_{i=1}^nP(A\\mid B_i)\\cdot P(B_i)P(A)=i=1∑n​P(A∣Bi​)⋅P(Bi​) 可以看出，全概率公式是由“因”（条件事件BiB_iBi​）推“果”（结果事件AAA）的过程，即当知道某结果事件的原因后，推断由该原因导致这件事发生的概率是多少。 贝叶斯公式​ 贝叶斯公式在观测到结果事件AAA发生后，计算其条件事件BiB_iBi​在事件AAA已经发生的条件下而发生的后验概率。 继续沿用上述全概率公式的符号定义，则有： P(Bi∣A)=P(A∣Bi)⋅P(Bi)P(A)P(B_i\\mid A)=\\frac{P(A\\mid B_i)\\cdot P(B_i)}{P(A)}P(Bi​∣A)=P(A)P(A∣Bi​)⋅P(Bi​)​ 其中： P(Bi)P(B_i)P(Bi​)以及P(A)P(A)P(A)称为先验概率 P(Bi∣A)P(B_i\\mid A)P(Bi​∣A)称为后验概率 P(A∣Bi)P(A\\mid B_i)P(A∣Bi​)称为似然","s":"先验概率与后验概率","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","h":"#先验概率与后验概率","p":1326},{"i":1332,"t":"条件概率的一般形式​ P(A,B,C)=P(C∣A,B)⋅P(A,B)=P(C∣A,B)⋅P(B∣A)⋅P(A)P(A,B,C)=P(C\\mid A,B)\\cdot P(A,B)=P(C\\mid A,B)\\cdot P(B\\mid A)\\cdot P(A)P(A,B,C)=P(C∣A,B)⋅P(A,B)=P(C∣A,B)⋅P(B∣A)⋅P(A)P(B,C∣A)=P(B∣A)⋅P(C∣A,B)P(B,C\\mid A)=P(B\\mid A)\\cdot P(C\\mid A,B)P(B,C∣A)=P(B∣A)⋅P(C∣A,B) 其中，第二行公式的推导如下： P(B,C∣A)=P(A,B,C)P(A)=P(A,B,C)P(A,B,C)P(C∣A,B)⋅P(B∣A)=P(B∣A)⋅P(C∣A,B)\\begin{align*} P(B,C \\mid A) &= \\frac{P(A,B,C)}{P(A)} \\\\ &= \\frac{P(A,B,C)}{\\frac{P(A,B,C)}{P(C \\mid A,B) \\cdot P(B \\mid A)}} \\\\ &= P(B \\mid A) \\cdot P(C \\mid A,B) \\end{align*}P(B,C∣A)​=P(A)P(A,B,C)​=P(C∣A,B)⋅P(B∣A)P(A,B,C)​P(A,B,C)​=P(B∣A)⋅P(C∣A,B)​ 高斯分布的KL散度​ 对于两个单一变量的高斯分布p∼N(μ1,σ12)p\\sim \\mathcal{N}(\\mu_1,\\sigma_1^2)p∼N(μ1​,σ12​)和q∼N(μ2,σ22)q\\sim \\mathcal{N}(\\mu_2,\\sigma_2^2)q∼N(μ2​,σ22​)而言，它们的KL散度定义为： DKL(p,q)=log⁡σ2σ1+σ12+(μ1−μ2)22σ22−12D_{KL}(p,q)=\\log\\frac{\\sigma_2}{\\sigma_1}+\\frac{\\sigma_1^2+(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}-\\frac12DKL​(p,q)=logσ1​σ2​​+2σ22​σ12​+(μ1​−μ2​)2​−21​","s":"条件概率与高斯分布的KL散度","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","h":"#条件概率与高斯分布的kl散度","p":1326},{"i":1334,"t":"马尔科夫链指当前状态的概率只与上一时刻有关，例如若满足马尔科夫关系A→B→CA\\to B\\to CA→B→C，则有： P(A,B,C)=P(C∣A,B)⋅P(A,B)=P(C∣B)⋅P(B∣A)⋅P(A)P(A,B,C) =P(C\\mid A,B)\\cdot P(A,B)=P(C\\mid B)\\cdot P(B\\mid A)\\cdot P(A)P(A,B,C)=P(C∣A,B)⋅P(A,B)=P(C∣B)⋅P(B∣A)⋅P(A)P(B,C∣A)=P(B∣A)⋅P(C∣B)P(B,C\\mid A)=P(B\\mid A)\\cdot P(C\\mid B)P(B,C∣A)=P(B∣A)⋅P(C∣B)","s":"马尔科夫链条件概率形式","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","h":"#马尔科夫链条件概率形式","p":1326},{"i":1336,"t":"从任意高斯分布N(μ,σ2)\\mathcal{N}(\\mu,\\sigma^2)N(μ,σ2)采样xxx时，可以先从标准高斯分布N(0,1)\\mathcal{N}(0,1)N(0,1)中sample出zzz，再令 x=σ∗z+μx=\\sigma * z + \\mux=σ∗z+μ 优势： 由于线性变化，采样过程中对其他参数都有明确的导数，可以进行反向传播 可以通过线性变换来控制参数化的采样 标准正态分布具有易采样的性质","s":"参数重整化技巧","u":"/en/docs/Deep-Learning/论文笔记/Denoising-Diffusion-Probabilistic-Models","h":"#参数重整化技巧","p":1326},{"i":1338,"t":"原文链接：https://arxiv.org/pdf/2406.06525","s":"Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"","p":1337},{"i":1340,"t":"We introduce LlamaGen, a new family of image generation models that apply original “next-token prediction” paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality. The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. (2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256×256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment. (4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup. We release all models and codes to facilitate open-source community of visual generation and multimodal foundation models.","s":"摘要","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#摘要","p":1337},{"i":1342,"t":"文章首先介绍了在 NLP 领域发展迅猛的 Autoregressive 模型，引出了在视觉领域的 Autoregressive 模型的发展。VQVAE、VQGA、DALL-E 和 Parti 等。同时指出了先前视觉 AR 模型存在开源社区发展不足，限制了 AR 方法继续研究的问题。此外，作者还提到了以 MaskGIT 为代表的 Masked-prediction Model 以及 VAR 方法，但是仍然体现出了与语言 LLMs 流行使用的自回归方法的较大差异。 同时还介绍了与 AR 思想不同的 Diffusion Models，作者指出，由于 DMs 与在 NLP 的 LLMs 常用的自回归思想有较大的差异，因此给语言和视觉的统一带来了很大的挑战。 作者同时总结了图像生成模型的三个要点： 优秀的 image compressor，即 AR 模型所使用的 image tokenizer、quantizer scalable image generation models，即模型的可扩展性（指参数量方面） 高质量的训练数据 可以看出，本文作者从统一语言和视觉两个模态的想法出发，强调与语言 LLMs 统一，而不是在视觉归纳偏置的引导下改进模型的结构，引出了本文的工作。 图像重建能力高、Codebook 利用率高达 97% 的 image tokenizer 基于语言模态中的 SOTA 模型 Llama 的可扩展的图像生成模型 高质量的训练数据，本文提出的文本条件图像生成模型，首先在 LAION-COCO 的 50 million 子集上进行训练，然后在 10 million 高质量图像上进行微调。展示出了视觉质量和文本对齐的竞争性能。 使用 语言模态中常用的 LLM serving framework vLLM 来优化图像生成模型的推理速度。","s":"Motivation 与主要贡献","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#motivation-与主要贡献","p":1337},{"i":1345,"t":"使用了与 VQGAN 相同的 encoder-quantizer-decoder 结构。 作者认为 Codebook 会极大地影响 image tokenization 的表现，因此对 Codebook 做出了以下优化，这种设计极大地增强了图像重建质量和 Codebook 的利用率。 对 Codebook 中的向量施加 l2l_2l2​-normalization，降低 Codebook 中向量的维度 CCC，增加 Codebook 的容量大小 KKK​​。 Codebook 以及 Image Tokenizer 的训练损失函数如下所示。","s":"Image Tokenizer","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#image-tokenizer","p":1337},{"i":1347,"t":"本文的 Autoregressive Model 的结构主要参考了 Llama 的结构，将语言模态中的 SOTA 模型直接引入至图像生成中，统一语言和视觉模态的操作。同时，这样做的好处是可以让视觉生成模型充分利用语言模态的 LLMs 社区中取得的前沿技术。 使用 RMSNorm 进行归一化，RMSNorm 是对 Layer Normalization 的一种改进。 RMSNorm 使用平方根的均值来归一化，而不是像 LayerNorm 那样使用整个样本的均值和方差。 RMSNorm 移除了 LayerNorm 中的 re-center 操作（即移除了均值项），可以看作 LayerNorm 在均值为 0 时的一个特例。 RMSNorm(x)=x1n∑i=1nxi2∗gRMSNorm(x)=\\frac x{\\sqrt{\\frac1n\\sum_{i=1}^nx_i^2}}*gRMSNorm(x)=n1​∑i=1n​xi2​​x​∗g 其中，xxx 是输入向量，nnn 是向量维度，ggg​ 是可缩放的参数。 RMSNorm 的优势在于其计算简单，尤其是在处理较长序列时，可以更有效地进行归一化。 使用 SwiGLU 激活函数，SwiGLU 结合了 GLU（Gated Linear Unit）和 Swish 函数，引入了门控机制来控制输入信号的传递方式。 组成部分： GLU 部分：使用 sigmoid 函数作为门控器，对输入信号进行筛选和选择性放大。 Swish 部分：非线性函数，类似于 ReLU，但在负值区域有平滑的非线性特性。 SwiGLU(x)=σ(xW1+b1)⊙(xW2+b2)\\mathrm{SwiGLU}(x)=\\sigma(xW_1+b_1)\\odot(xW_2+b_2)SwiGLU(x)=σ(xW1​+b1​)⊙(xW2​+b2​) 其中，xxx 是输入向量，W1,W2W_{1}, W_{2}W1​,W2​ 是权重矩阵，b1,b2b_1, b_2b1​,b2​ 是偏置向量，σ\\sigmaσ 是激活函数，通常为 GELU（高斯误差线性单元），⊙\\odot⊙ 表示逐元素乘法。 SwiGLU 引入两个线性变换和一个门控机制来增强模型的表现力，优点在于通过门控机制对输入进行加权，能够更灵活地捕捉复杂的输入模式。 本文提出的LlamaGen 模型的每一层都使用了 2D RoPE。 RoPE（Rotary Position Embedding）是一种位置编码方法，旨在解决绝对位置编码在处理较长序列时的局限性。RoPE 通过将位置信息引入到输入向量的相位中，增强了模型对相对位置的敏感度。其基本思想是将输入向量按位旋转，旋转角度与位置相关。 RoPE⁡(xi)=xi⋅cos⁡(θi)+xi+1⋅sin⁡(θi)\\operatorname{RoPE}\\left(x_i\\right)=x_i \\cdot \\cos \\left(\\theta_i\\right)+x_{i+1} \\cdot \\sin \\left(\\theta_i\\right)RoPE(xi​)=xi​⋅cos(θi​)+xi+1​⋅sin(θi​) xix_ixi​ 是输入向量的第 iii 个元素。 θi\\theta_iθi​ 是位置 iii 处的旋转角度，通常由固定的正弦和余弦函数生成。 RoPE 的优势在于能够更好地捕捉序列中的相对位置关系，提高模型的长距离依赖能力。 为了与语言模态的 LLMs 保持统一，作者没有使用 AdaLN。 AdaLN（Adaptive Layer Normalization）是一种自适应层归一化方法，旨在为不同的样本动态调整归一化参数。与传统的层归一化方法不同，AdaLN 根据输入数据自适应地调整均值和方差，从而更好地适应不同的输入特征。 AdaLN⁡(x)=x−μ(x)σ(x)⋅γ+β\\operatorname{AdaLN}(x)=\\frac{x-\\mu(x)}{\\sigma(x)} \\cdot \\gamma+\\betaAdaLN(x)=σ(x)x−μ(x)​⋅γ+β xxx 是输入向量。 μ(x)\\mu(x)μ(x) 和 σ(x)\\sigma(x)σ(x) 分别是输入 xxx 的均值和标准差。 γ\\gammaγ 和 β\\betaβ 是可训练的缩放和平移参数。 在 AdaLN 中，均值 μ(x)\\mu(x)μ(x) 和标准差 σ(x)\\sigma(x)σ(x)​ 是通过一个子网络（通常是一个简单的前馈网络）从输入数据中动态预测的。这使得归一化过程更加灵活，能够适应更复杂的输入模式。 LlamaGen 的训练使用了 CFG（Classifier-free Guidance）策略来提高视觉质量和文本-图像对齐。 训练阶段： 在训练期间，条件信息被随机丢弃，并用一个空的无条件嵌入进行替换。这种方法有助于减少模型对特定条件的依赖性，从而改善生成结果的一般化能力。 推理阶段： 对于每个 token，其 logit ℓg\\ell_gℓg​ 是通过以下方式形成的： ℓg=ℓu+s(ℓc−ℓu)\\ell_g = \\ell_u + s(\\ell_c - \\ell_u)ℓg​=ℓu​+s(ℓc​−ℓu​) 其中： ℓc\\ell_cℓc​ 表示条件logit，即基于输入文本提示的信息生成的logit。 ℓu\\ell_uℓu​ 表示无条件logit，即不考虑任何条件信息时生成的logit。 sss 是Classifier-free Guidance的比例因子，用于控制条件logit和无条件logit之间的平衡。 通过这种方式，模型可以利用无条件logit提供的通用信息，同时保留条件logit中的特定上下文信息。这种混合方法有助于提高生成图像的质量和与输入文本的对齐程度。 作者在文章的实验部分给出了使用 CFG 的结果，实验表明，CFG 过高会导致 FID 分数的下降，可以看作是多样性和保真度之间的权衡（trade-off），随着 CFG 的提高，准确率的提高和召回率的降低证明了这点。","s":"Image Generation Autoregressive Model","u":"/en/docs/Deep-Learning/论文笔记/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#image-generation-autoregressive-model","p":1337},{"i":1349,"t":"相关链接 论文：CVPR 2023 open access 代码：Piscart-AI-Research 文章的命名风格借鉴了CVPR 2022的文章Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment","s":"CVPR 2023: Zero-shot Generative Model Adaptation via Image-specific Prompt Learning","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"","p":1348},{"i":1351,"t":"本文提出了Image-specific Prompt Learning（IPL）方法来解决风格迁移任务中生成模型从源域到目标域的适应问题。一个Latent Mapper来从源域图像中学习出包含图像特征且适应目标域的prompt，从而指导目标域生成器的训练。 This produces a more precise adaptation direction for every cross-domain image pair, endowing the target-domain generator with greatly enhanced flexibility. 训练资料是源域和目标域的文字标签以及源域的图像，并不需要目标域的图像。此外，IPL独立于生成模型，可以自由选择Diffusion Model或GAN等。","s":"摘要","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#摘要","p":1348},{"i":1354,"t":"Generative Model Adaption的任务是使在大规模源域图片上训练的生成模型适应到数据有限的目标域中，根据目标域训练资料的大小可以分为few-shot和zero-shot。 few-shot​ 对于few-shot任务，一般是通过有限的目标域训练集资料fine-tune预训练模型。 然而，fine-tune通常会导致过拟合。为了解决过拟合问题，通常使用的方法是施加强正则化、使用扰动法、跨域对齐或数据增强。 相关文献方法 强正则化：Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for generative adversarial networks. In ICLR, 2019. 扰动法：Sangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze the discriminator: a simple baseline for fine-tuning GANs. In CVPR Workshops, 2020. 跨域对齐：Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A Efros, Yong Jae Lee, Eli Shechtman, and Richard Zhang. Fewshot image generation via cross-domain correspondence. In CVPR, 2021. 数据增强：Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and Ngai-Man Cheung. On data augmentation for GAN training. TIP, 2021. zero-shot​ 对于零样本的图像生成模型的适应任务，NADA率先引入了CLIP模型来获取必须的先验知识，通过预训练大模型的语言理解能力实现在目标域只需要文字标签而不需要图片，将源域和目标域之间的差距编码为在CLIP空间上文字引导的适应方向。 此后，CVPR 2022发表的DiffusionCLIP使用了Diffusion模型代替NADA中的StyleGANs，获得了更好的特征保存能力。 然而这些方法都是采用了固定的适应方向，只包含基础的域知识，而不是图片特定的特征。在本文中，作者发现这种共享的、固定的适应方向会导致Mode Collapse（模式坍塌），因此提出了从每个源域图像中学习出多样且准确的prompt，为生成模型向目标域的适应提供更精确的方向。","s":"Generative Model Adaption","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#generative-model-adaption","p":1348},{"i":1356,"t":"Prompt工程最初是一种Knowledge Probing（知识探测）方法，给定完形填空（cloze-style）类的prompt，引导模型产生相对应的答案。 然而人工设计的prompt通常不是最优的，可能提供不准确的适应方向。为了解决这个问题，在NLP领域的Prompt Learning发展迅速，并随着视觉-语言大模型的发展，应用在了视觉任务中。 Kaiyang Zhou等人首先在图像分类任务中采用上下文优化，在词嵌入空间中对具有连续向量的上下文词进行建模。随后Prompt Learning在计算机视觉中的许多下游任务都得到了探索，例如目标检测、视频理解和迁移学习等。","s":"Prompt Learning","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#prompt-learning","p":1348},{"i":1359,"t":"IPL方法分两个阶段。 第一阶段：训练Latent Mapper​ 第一阶段的主要任务是训练Lantent Mapper来为每一个训练集的源域图片生成一组prompt。Latent Mapper接收源域图像的latent representation，生成一组prompt向量。第一阶段需要解决两个问题，即在zero-shot的背景下，如何实现prompt与源域图像特征的对齐以及prompt与目标域空间的对齐，因此第一阶段的训练分两部分进行。 第一部分是Latent Mapper输出的prompt与目标域标签concat后送入来自CLIP的Text Encoder得到目标域图片prompt在CLIP空间的编码表示，并与目标域标签经过Text Encoder后的编码共同作为Domain Loss的输入来约束从源域中学习到的prompt与目标域空间对齐。 第二部分是Latent Mapper输出的prompt与源域标签concat后送入来自CLIP的Text Encoder得到源域图片prompt描述在CLIP空间的编码表示，同时源域图像再经过来自CLIP的Image Encoder后得到其在CLIP空间的编码表示。将源域的prompt文字和图像编码表示作为contrastive learning loss的输入，约束学习到的prompt与源域图像的特征对齐。 第二阶段：将Latent Mapper插入目标域生成器的训练过程​ 第二阶段利用Directional CLIP Loss来训练目标域生成器，使源于生成器向目标域迁移学习。需要输入源域以及目标域图像、源域以及目标域的prompt描述。源域图像的latent representation分别输入至源域生成器和目标域生成器中得到对应的图像，同时指导风格迁移方向的源域以及目标域的prompt描述由Latent Mapper接收源域图像的隐式表示后输出再分别与源域和目标域标签concat而得到。分别将源域图像、生成的目标域图像以及源域、目标域的图片prompt描述一起输入至Directional CLIP Loss，从而约束由源域图像生成器初始化的目标域图像生成器向目标域的迁移学习。","s":"概述","u":"/en/docs/Deep-Learning/论文笔记/Zero-shot-Generative-Model-Adaptation-via-Image-specific-Prompt-Learning","h":"#概述","p":1348},{"i":1362,"t":"原文链接：https://arxiv.org/pdf/2406.11838 参考资料：何恺明：Autoregressive Image Generation without Vector Quantization.","s":"自回归模型：MAR","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"","p":1361},{"i":1364,"t":"Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications.","s":"摘要","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#摘要","p":1361},{"i":1366,"t":"由于语言先天的离散性质，在自然语言领域的自回归模型都是在离散的隐空间中学习数据的概率分布。因此，当自回归模型推广到连续的数据空间（如图像生成）进行发展时，研究人员将主要的精力放在了如何更有效地将连续数据离散化上，如 VQGAN、VQVAE、VAR、MaskGIT 等。 然而，本文作者从 Autoregressive 模型的性质本身（即根据先前的 token 预测下一个 token）出发进行思考，产生了自回归模型是否有必要与向量量化（Vector Quantization）表示相结合的疑问，作者发现，自回归生成模型与 token 是离散表示的还是连续表示的并没有关系，真正需要解决的问题是如何对每个 token 的概率分布进行建模，同时该概率分布可以通过损失函数来学习和约束（训练），并用于从中进行采样（生成）。","s":"研究动机","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#研究动机","p":1361},{"i":1368,"t":"本文提出的模型融合了目前图像生成领域的三大主流范式的思想，分别为自回归生成、掩码生成以及扩散生成： 模型首先根据自回归模型性质为每个 token 生成条件向量（conditional vector）zzz 之后使用扩散过程建模每个 token 的概率分布 最后结合掩码生成的思想加速训练和推理过程","s":"主要方法与实现","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#主要方法与实现","p":1361},{"i":1370,"t":"具体而言，如下图所示，假设连续值向量 x∈Rdx \\in \\mathbb{R}^dx∈Rd，是当前位置要预测的 token。自回归模型在这个位置产生一个条件向量 z∈RDz \\in \\mathbb{R}^Dz∈RD​​ 。 本文使用 Diffusion Loss 根据 zzz 对 xxx 建模概率分布 p(x∣z)p(x \\mid z)p(x∣z)​ 。 L(z,x)=Eε,t[∥ε−εθ(xt∣t,z)∥2]L(z, x)=\\mathbb{E}_{\\varepsilon, t}\\left[\\left\\|\\varepsilon-\\varepsilon_\\theta\\left(x_t \\mid t, z\\right)\\right\\|^2\\right]L(z,x)=Eε,t​[∥ε−εθ​(xt​∣t,z)∥2] 其中： ε∈Rd\\varepsilon \\in \\mathbb{R}^dε∈Rd 是从 N(0,I)\\mathcal{N}(0, \\mathbf{I})N(0,I) 采样的噪声向量 xt=αˉtx+1−αˉtεx_t=\\sqrt{\\bar{\\alpha}_t x}+\\sqrt{1-\\bar{\\alpha}_t} \\varepsilonxt​=αˉt​x​+1−αˉt​​ε，其中 αˉt\\bar{\\alpha}_tαˉt​ 定义了一个 noise schedule ttt 是 noise schedule 的时间步 噪声预测网络 εθ\\varepsilon_\\thetaεθ​ 由参数 θ\\thetaθ 控制，是一个小型 MLP 网络，εθ(xt∣t,z)\\varepsilon_\\theta\\left(x_t \\mid t, z\\right)εθ​(xt​∣t,z) 表示此网络将 xtx_txt​ 作为输入，并且以 ttt 和 zzz​​​ 作为条件。MLP 网络的具体组成如下所示： 对于扩散过程，作者采用了 cosine 函数作为 noise schedule，在训练过程中设定为 1000 步，在推理过程中设定为 100 步。同时，Diffusion Loss 原生支持 CFG 策略。","s":"Diffusion Loss","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#diffusion-loss","p":1361},{"i":1372,"t":"在推理时，需要从分布 p(x∣z)p(x \\mid z)p(x∣z) 中采样样本。采样是通过逆扩散过程进行的： xt−1=1αt(xt−1−αt1−αˉtϵθ(xt∣t,z))+σtδx_{t-1}=\\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta\\left(x_t \\mid t, z\\right)\\right)+\\sigma_t \\deltaxt−1​=αt​​1​(xt​−1−αˉt​​1−αt​​ϵθ​(xt​∣t,z))+σt​δ 这里 δ\\deltaδ 是从高斯分布 N(0,I)\\mathcal{N}(0, I)N(0,I) 中采样的， σt\\sigma_tσt​ 是时间步 ttt 的噪声水平。以 xT∼N(0,I)x_T \\sim \\mathcal{N}(0, I)xT​∼N(0,I) 开始，此过程生成一个样本 x0x_0x0​ 使得 x0∼p(x∣z)x_0 \\sim p(x \\mid z)x0​∼p(x∣z)​。","s":"采样","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#采样","p":1361},{"i":1375,"t":"本文为了提高生成速度，结合了 MAE 和 MaskGIT 模型的思想，如下图的 b 和 c 所示，将随机顺序生成的思想和每次预测出一个 patch 的 token 的思想加入到本文的模型中。 掩码信息通过位置编码送入 Decoder 中，使模型在以随机顺序预测序列时获取到 token 的位置信息。","s":"结合掩码生成模型的思想 MAR","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#结合掩码生成模型的思想-mar","p":1361},{"i":1382,"t":"对于 MAR 模型，AR 步数越小，每次预测的一个 patch 的 token 数越大，速度越快，精度越低，默认设置为 64 步。","s":"MAR 模型速度与效果的 trade-off","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Image-Generation-without-Vector-Quantization","h":"#mar-模型速度与效果的-trade-off","p":1361},{"i":1384,"t":"自回归模型（Autoregressive Models）在图像生成领域扮演着重要角色，它们基于一个核心假设：当前像素值依赖于之前的像素值。这种依赖关系可以通过条件概率来表达，其中每一个像素的生成都是基于之前已经生成的像素的条件分布。在传统的视觉自回归图像生成任务中，这通常意味着从左到右和从上到下的顺序生成每个像素。","s":"图像生成：自回归模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"","p":1383},{"i":1386,"t":"自回归（Autoregressive，简称AR）模型是一种统计模型，用于描述时间序列中的每个值作为其之前值的函数。在机器学习中，自回归模型被广泛应用于序列数据的生成任务，如文本、语音和图像生成。自回归模型通过逐步预测下一个值，依次生成整个序列。 假设有一个序列 x=(x1,x2,…,xT)x=\\left(x_1, x_2, \\ldots, x_T\\right)x=(x1​,x2​,…,xT​) ，自回归模型的目标是学习条件概率分布 p(xt∣x<t)p\\left(x_t \\mid x_{<t}\\right)p(xt​∣x<t​) ， 即在给定序列前面部分 x<t=(x1,x2,…,xt−1)x_{<t}=\\left(x_1, x_2, \\ldots, x_{t-1}\\right)x<t​=(x1​,x2​,…,xt−1​) 的情况下，预测当前值 xtx_txt​ 。 对于一个给定的序列 xxx ，自回归模型的联合概率分布可以分解为以下形式: p(x)=∏t=1Tp(xt∣x<t)p(x)=\\prod_{t=1}^T p\\left(x_t \\mid x_{<t}\\right)p(x)=t=1∏T​p(xt​∣x<t​) 在图像生成任务中，图像被看作是一个二维序列。在传统的自回归图像生成模型（VQVAE、VQGAN 等）将图像的像素视作一个一维序列，通过条件概率分布 p(xi,j∣x<i,j)p\\left(x_{i, j} \\mid x_{<i, j}\\right)p(xi,j​∣x<i,j​) 来生成每个像素值 xi,jx_{i, j}xi,j​ 。","s":"自回归模型的数学定义","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#自回归模型的数学定义","p":1383},{"i":1389,"t":"PixelRNN 是早期的自回归模型之一，它使用循环神经网络（RNN）来建模条件概率分布。由于 RNN 能够捕捉长序列依赖性，PixelRNN 能够生成高质量的图像，但是计算效率较低，因为每次生成一个像素都需要遍历整个序列。","s":"PixelRNN（2016）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#pixelrnn2016","p":1383},{"i":1391,"t":"为了克服 PixelRNN 的计算瓶颈，PixelCNN 使用卷积神经网络（CNN）并引入了遮罩技术，确保每个像素仅依赖于其上方和左侧的像素。","s":"PixelCNN（2016）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#pixelcnn2016","p":1383},{"i":1393,"t":"学习链接 Variant AutoEncoder(VAE)和 VQVAE 学习笔记和代码 68、VQVAE预训练模型的论文原理及PyTorch代码逐行讲解 在科学空间：VQ-VAE 的简明介绍中，提到了 VQ-VAE 与之前的 PixelCNN 和 PixelRNN 的关系。 受 PixelCNN 和 PixelRNN 专注于像素级的生成的启发，VQ-VAE（Vector Quantized Variational Autoencoder）提出了另一种方法，通过离散的潜在空间来建模数据。VQ-VAE 的主要贡献在于引入了向量量化层，将连续的潜在变量映射到一组离散的嵌入向量上，从而能够更高效地学习复杂的数据分布。 主要思想：VQ-VAE（Vector Quantized Variational AutoEncoder）通过将连续的图像表示量化为离散的代码来进行图像生成。它引入了一个离散的代码本，将连续的潜在向量映射到离散的代码上，进行编码和解码。 主要贡献： 采用向量量化方法，使得生成模型能够高效地处理离散代码。 在生成模型中，离散化的潜在空间简化了模型的训练和推理过程。 与之前模型的关系： VQ-VAE 在 VAEs（Variational AutoEncoders）的基础上，引入了离散的潜在空间，使得生成效果更好，训练更稳定。","s":"VQ-VAE（2017）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#vq-vae2017","p":1383},{"i":1395,"t":"VQVAE-2 是 VQ-VAE 的一个扩展版本，它引入了层次化的结构，使用多个层次的量化向量来建模不同尺度的特征，从而提高了模型的灵活性和生成图像的质量。 主要思想：VQ-VAE-2 引入了多层级的离散潜在变量，通过多层级的自回归模型进行生成。它使用了层次化的编码器和解码器结构，更好地捕捉图像中的复杂结构。 主要贡献： 引入了层次化的离散潜在空间，提高了生成图像的质量和分辨率。 在多个层级上进行自回归生成，提高了模型对图像全局和局部信息的捕捉能力。 改进点： 相较于 VQ-VAE，VQ-VAE-2 通过层次化的设计，显著提升了生成图像的细节和清晰度。","s":"VQVAE-2（2019）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#vqvae-22019","p":1383},{"i":1397,"t":"VQGAN 结合了 VQ-VAE 的思想和生成对抗网络（GANs）的优点。它使用编码器和解码器之间的量化瓶颈，并利用 GAN 框架来优化图像质量。VQGAN 在高分辨率图像生成方面表现出了强大的能力。 主要思想：VQGAN（Vector Quantized Generative Adversarial Networks）结合了 VQ-VAE 的离散潜在表示和 GAN（Generative Adversarial Networks）的对抗训练，提升了生成图像的质量。 主要贡献： 利用对抗训练，生成的图像更加逼真和细腻。 通过结合 VQ-VAE 和 GAN 的优势，解决了各自方法的一些固有缺陷。 改进点： 相较于 VQ-VAE-2，VQGAN 通过对抗训练机制，进一步提升了生成图像的视觉质量。","s":"VQGAN（2021）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#vqgan2021","p":1383},{"i":1399,"t":"RQTransformer 是一种改进的自回归模型，它利用了自注意力机制，能够有效地处理序列数据。相比于传统的自回归模型，RQTransformer 在大规模数据集上展现了更好的性能，尤其是在自然语言处理和图像生成任务中。 主要思想：RQ-Transformer（Residual Quantization Transformer）结合了残差量化和 Transformer 架构，通过对潜在空间进行分块量化来提高生成效率和质量。 主要贡献： 引入残差量化方法，提高了潜在空间的表示能力。 结合 Transformer 架构，增强了对复杂图像结构的建模能力。 改进点： 相较于 VQ-VAE 和 VQGAN，RQ-Transformer 在潜在空间表示上更为灵活，提高了生成图像的分辨率和质量。","s":"RQTransformer（2021）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#rqtransformer2021","p":1383},{"i":1401,"t":"主要思想：DALL-E 是 OpenAI 提出的一个基于 Transformer 的图像生成模型，能够根据文本描述生成图像。它使用了大规模的文本-图像对数据进行训练，并采用了自回归生成策略。其创新之处在于如何将文本和图像模态融合在一起。 主要贡献： 展示了基于文本描述生成图像的强大能力。 引入了大规模数据和 Transformer 架构，使得生成效果更加多样化和逼真。 改进点： 相较于之前的图像生成模型，DALL-E 在生成多样性和一致性上有了显著提升，特别是在文本到图像的生成任务中表现出色。","s":"DALL-E（2021）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#dall-e2021","p":1383},{"i":1403,"t":"Parti 是谷歌研究团队提出的一种模型，专注于分层次的图像生成。它首先生成图像的组成部分（如物体或场景元素），然后将这些部分组合成完整的图像。Parti 在保持图像细节的同时，还能够控制图像的语义内容。 主要思想：Parti 是一个基于 Transformer 的图像生成模型，能够通过部分图像生成剩余部分。它采用了多阶段生成策略，通过逐步细化生成图像，保证生成效果的一致性和逼真性。 主要贡献： 引入了多阶段的生成策略，提高了生成图像的一致性。 在部分图像生成任务中表现出色，能够根据现有图像生成补全部分。 改进点： 相较于 DALL-E 和其他自回归模型，Parti 在生成图像的一致性和细节捕捉上有了显著改进。","s":"Parti（2022）","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Models","h":"#parti2022","p":1383},{"i":1406,"t":"We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine “next-scale prediction” or “next-resolution prediction”, diverging from the standard raster-scan “next-token prediction”. This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and can generalize well: VAR, for the first time, makes GPT-style AR models surpass diffusion transformers in image generation. On ImageNet 256×256 benchmark, VAR significantly improve AR baseline by improving Fréchet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with 20× faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near −0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.","s":"摘要","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#摘要","p":1404},{"i":1408,"t":"通过回顾先前的视觉 Autoregressive 模型（即采用 next-token 范式进行生成），从 scaling laws 和性能的角度将其与 NLP 领域的 LLMs 进行比较，宏观上说明了传统视觉 AR 模型存在的问题。 传统视觉 AR 模型是否遵循 scaling laws 有待探索和验证 性能有待提升 上图展示了传统 Visual AR 模型在 scaling laws 方面的局限性，并以 FID 作为评价指标，展示了性能效果方面的不足。","s":"研究背景与动机","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#研究背景与动机","p":1404},{"i":1410,"t":"首先文章花费了大量篇幅来用数学语言描述以 next-token prediction 作为生成范式的传统视觉自回归模型的定义和流程，大致包括以下流程： 输入 raw image，Encoder 得到对应的 feature map。 f=E(im)f=\\mathcal{E}(im)f=E(im) 输入feature map，Quantizer 得到对应的量化 image token。 q=Q(f)q=\\mathcal{Q}(f)q=Q(f) 在量化步骤，会将每个特征向量映射到与其在欧几里得意义上最接近的 Codebook ZZZ 中的 code。 q(i,j)=(arg⁡min⁡v∈[V]∥lookup(Z,v)−f(i,j)∥2)∈[V]q^{(i,j)}=\\left(\\arg\\min_{v\\in[V]}\\|\\text{lookup}(Z,v)-f^{(i,j)}\\|_2\\right)\\in[V]q(i,j)=(argv∈[V]min​∥lookup(Z,v)−f(i,j)∥2​)∈[V] Decoder 通过接收在 Codebook 中查找得到的 f^\\hat{f}f^​ 生成重构的图像。 f^=lookup⁡(Z,q)\\hat{f}=\\operatorname{lookup}(Z,q)f^​=lookup(Z,q) im^=D(f^)\\hat{im}=\\mathcal{D}(\\hat{f})im^=D(f^​) L=∥im−im^∥2+∥f−f^∥2+λPLP(im^)+λGLG(im^)\\mathcal{L}=\\|im-i\\hat{m}\\|_2+\\|f-\\hat{f}\\|_2+\\lambda_\\text{P}\\mathcal{L}_\\text{P}(i\\hat{m})+\\lambda_\\text{G}\\mathcal{L}_\\text{G}(i\\hat{m})L=∥im−im^∥2​+∥f−f^​∥2​+λP​LP​(im^)+λG​LG​(im^) 下图展示了传统 VAR 方法与本文提出的 VAR 方法的对比。其中图（a）展示了 NLP 领域的 next-token prediction，图（b）展示了上述公式定义的 next-image-token prediction 的过程，包含量化与展平的步骤，图（c）展示了本文提出的 next-scale prediction。 文章发现了目前传统 VAR 模型存在的三个问题。 VQGAN 违反了 Autoregressive 的数学前提。Autoregressive 模型假设当前时间步的 token xtx_txt​ 只取决于其之前时间步的 token 前缀 (x1,x2,...,xt−1)(x_1, x_2, ..., x_{t-1})(x1​,x2​,...,xt−1​)，具有单向相关性（unidirectional）。而 VQGAN 中的 image encoder 直接从具有双向相关性（bidirectional）的 feature map 中进行量化和展平，因此得到的 image tokens 同样具有双向相关性，违反了数学假设。 作者在附录中检查了 VQGAN 模型在量化步骤之前的注意力层输出的图像注意力分数，证明了较强的双向相关性。 作者解释，这是因为在图像 VAE 以及其他类似的工作的自注意力层中，并没有使用任何注意力掩码机制，如 sequence mask 以及 causal attention 等，导致了双向相关性。 This is not surprising since the VQVAE model, trained to reconstruct images, leverages self-attention layers without any attention mask. Some work [67] has used causal attention in self-attention layers of a video VAE, but we did not find any image VAE work uses causal self-attention. image tokens 的空间结构性被破坏。由于先前的 VAR 工作均是采用类似于先列后行的一维顺序存储 image tokens 并进行 Autoregressive 生成，image tokens 的扁平化破坏了图像特征图固有的空间局部性。 时间复杂度过高，影响生成效率。使用传统的视觉 Autoregressive 方法生成 x=(x1,x2,…,xn×n)x=(x_1,x_2,\\ldots,x_{n\\times n})x=(x1​,x2​,…,xn×n​) 的 token 序列，需要 O(n2)\\mathcal{O}(n^{2})O(n2) 的注意力步骤以及 O(n6)\\mathcal{O}(n^{6})O(n6) 的计算复杂度。","s":"问题发现与提出","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#问题发现与提出","p":1404},{"i":1413,"t":"本文的工作重新考虑了以什么样的顺序生成图像。人类通常以分层的方式感知或创建图像，首先捕获全局结构，然后捕获局部细节。这种多尺度、由粗到精（coarse-to-fine）的方法很自然地给图像暗示了一种顺序。此外，受广泛使用的多尺度（multi-scale）设计工作的启发，本文将图像的自回归学习定义为下图展示的 next-scale prediction。 自回归过程从分辨率为 1×11\\times11×1​ 的 token map 开始，并逐步扩大分辨率：在每一步，Transformer 以之前生成的所有分辨率的 token maps 为条件预测下一大分辨率的 token map。 通过从 next-token prediction 策略转变为 next-scale prediction 策略，重新概念化了对图像的自回归建模。在这里，自回归单元是整个 token map，而不是传统方法的单个 token。先将特征图 f∈Rh×w×Cf \\in \\mathbb{R}^{h \\times w \\times C}f∈Rh×w×C 量化为 KKK 个多尺度标记图 (r1,r2,…,rK)(r_1, r_2, \\ldots, r_K)(r1​,r2​,…,rK​)，每个图的分辨率 hk×wkh_k \\times w_khk​×wk​ 逐步增加，最终达到 rKr_KrK​ 与原始特征图的分辨率 h×wh \\times wh×w 匹配。自回归似然性公式为： p(r1,r2,…,rK)=∏k=1Kp(rk∣r1,r2,…,rk−1)p(r_1, r_2, \\ldots, r_K) = \\prod_{k=1}^{K} p(r_k \\mid r_1, r_2, \\ldots, r_{k-1})p(r1​,r2​,…,rK​)=k=1∏K​p(rk​∣r1​,r2​,…,rk−1​) 其中每个自回归单元 rk∈[V]hk×wkr_k \\in [V]^{h_k \\times w_k}rk​∈[V]hk​×wk​ 是包含 hk×wkh_k \\times w_khk​×wk​ 个标记的第 kkk 个尺度的标记图，而序列 (r1,r2,…,rk−1)(r_1, r_2, \\ldots, r_{k-1})(r1​,r2​,…,rk−1​) 作为 rkr_krk​ 的“前缀”。在第 kkk 个自回归步骤中，所有 rkr_krk​ 中的 h_k \\times w_k 标记的分布将并行生成，并以 rkr_krk​ 的前缀和关联的第 kkk 个位置嵌入图为条件。如下图所示。 请注意，在 VAR 的训练中，使用逐块的因果注意力掩码，以确保每个 rkr_krk​ 只能关注其前缀 r≤kr_{\\leq k}r≤k​，从而满足 Autoregressive 模型的数学假设前提。 本文在结构上的主要贡献是开发了适用于 next-scale prediction 的 multi-scale VQ quantizer，同时结合新的 VQ quantizer 提出了新的图像自回归生成模型，并进一步论证了该方法展示出的与 LLMs 类似的 scaling laws 能力。","s":"主要方法","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#主要方法","p":1404},{"i":1415,"t":"Multi-scale VQ quantizer​ 首先，需要设计一个满足多尺度要求的 VQ tokenizer，作者使用了与 VQVAE 相同的框架，并采用了改进的多尺度量化层，并加入了对 feature map 的残差设计：对 encoder 输出的 feature map 进行 interpolate 构建不同尺度的 feature map，不同 feature map 之间通过计算残差的方式进行连接，并结合 quantizer 得到离散序列。VQ quantizer（VAR tokenizer）的具体算法如下图所示。 VAR Transformer​ 作者将重点放在了 VAR tokenizer 的理念和设计上，在 VAR Transformer 中保持了与 GPT-2 和 VQGAN 相同的简洁设计，在结构设计上只融合了 adaptive normalization（AdaLN）。 在训练完 VQ tokenizer 后，需要在离散化后的序列上训练生成模型，上图中 r1,...,rkr_1,...,r_kr1​,...,rk​ 分别表示不同尺度的离散序列。作者将传统的单向自回归模型修改为双向与单向混合的模式，同一个尺度的图片内部使用双向 attention，token 彼此可见，不同尺度的图片之间使用单向 attention ，具有从粗粒度到细粒度的 causal dependency，保证了满足 Autoregressive 假设的数学前提。 单个尺度的图片可以一步生成，生成所需的迭代步数取决于 VQ tokenizer 设计的尺度层数 KKK。","s":"模型主要结构","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#模型主要结构","p":1404},{"i":1417,"t":"本文主要从视觉 Autoregressive 模型生成图像像素的顺序出发，重新思考，通过人类感知图像的方式设计了 next-scale 的生成范式，每一步迭代生成一张完整的图像，但生成图像的分辨率逐步提升，最终得到高像素目标图像。 对文章开篇提出的问题的解决： 使用 causal attention 对自注意力进行掩码，从而满足 Autoregressive 模型对时间序列的数学假设。 在 quantization 步骤使用二维的方式存储整个 image token map，保证了结构的完整性。 得益于多尺度思想的生成方式，时间复杂度和计算开销显著降低。以生成 n2n^2n2 个 image tokens 为例，传统的视觉 Autoregressive 生成需要 O(n2)\\mathcal{O}(n^2)O(n2) 次解码迭代和 O(n6)\\mathcal{O}(n^6)O(n6) 次总计算。相比之下，本文提出的 VAR 只需要 O(log(n))\\mathcal{O}(log(n))O(log(n)) 次迭代和 O(n4)\\mathcal{O}(n^4)O(n4)​ 次总计算量。 文章还论证了 VAR 模型展现出的与 LLMs 相同的 scaling laws 能力。","s":"总结","u":"/en/docs/Deep-Learning/论文笔记/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#总结","p":1404},{"i":1419,"t":"原文链接：https://arxiv.org/pdf/2406.06525","s":"自回归模型：LlamaGen","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"","p":1418},{"i":1421,"t":"We introduce LlamaGen, a new family of image generation models that apply original “next-token prediction” paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality. The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. (2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256×256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment. (4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup. We release all models and codes to facilitate open-source community of visual generation and multimodal foundation models.","s":"摘要","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#摘要","p":1418},{"i":1423,"t":"文章首先介绍了在 NLP 领域发展迅猛的 Autoregressive 模型，引出了在视觉领域的 Autoregressive 模型的发展。VQVAE、VQGA、DALL-E 和 Parti 等。同时指出了先前视觉 AR 模型存在开源社区发展不足，限制了 AR 方法继续研究的问题。此外，作者还提到了以 MaskGIT 为代表的 Masked-prediction Model 以及 VAR 方法，但是仍然体现出了与语言 LLMs 流行使用的自回归方法的较大差异。 同时还介绍了与 AR 思想不同的 Diffusion Models，作者指出，由于 DMs 与在 NLP 的 LLMs 常用的自回归思想有较大的差异，因此给语言和视觉的统一带来了很大的挑战。 作者同时总结了图像生成模型的三个要点： 优秀的 image compressor，即 AR 模型所使用的 image tokenizer、quantizer scalable image generation models，即模型的可扩展性（指参数量方面） 高质量的训练数据 可以看出，本文作者从统一语言和视觉两个模态的想法出发，强调与语言 LLMs 统一，而不是在视觉归纳偏置的引导下改进模型的结构，引出了本文的工作。 图像重建能力高、Codebook 利用率高达 97% 的 image tokenizer 基于语言模态中的 SOTA 模型 Llama 的可扩展的图像生成模型 高质量的训练数据，本文提出的文本条件图像生成模型，首先在 LAION-COCO 的 50 million 子集上进行训练，然后在 10 million 高质量图像上进行微调。展示出了视觉质量和文本对齐的竞争性能。 使用语言模态中常用的 LLM serving framework vLLM 来优化图像生成模型的推理速度。","s":"Motivation 与主要贡献","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#motivation-与主要贡献","p":1418},{"i":1426,"t":"使用了与 VQGAN 相同的 encoder-quantizer-decoder 结构。 作者认为 Codebook 会极大地影响 image tokenization 的表现，因此对 Codebook 做出了以下优化，这种设计极大地增强了图像重建质量和 Codebook 的利用率。 对 Codebook 中的向量施加 l2l_2l2​-normalization，降低 Codebook 中向量的维度 CCC，增加 Codebook 的容量大小 KKK​​。 Codebook 以及 Image Tokenizer 的训练损失函数如下所示。","s":"Image Tokenizer","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#image-tokenizer","p":1418},{"i":1428,"t":"本文的 Autoregressive Model 的结构主要参考了 Llama 的结构，将语言模态中的 SOTA 模型直接引入至图像生成中，统一语言和视觉模态的操作。同时，这样做的好处是可以让视觉生成模型充分利用语言模态的 LLMs 社区中取得的前沿技术。 使用 RMSNorm 进行归一化，RMSNorm 是对 Layer Normalization 的一种改进。 RMSNorm 使用平方根的均值来归一化，而不是像 LayerNorm 那样使用整个样本的均值和方差。 RMSNorm 移除了 LayerNorm 中的 re-center 操作（即移除了均值项），可以看作 LayerNorm 在均值为 0 时的一个特例。 RMSNorm(x)=x1n∑i=1nxi2∗gRMSNorm(x)=\\frac x{\\sqrt{\\frac1n\\sum_{i=1}^nx_i^2}}*gRMSNorm(x)=n1​∑i=1n​xi2​​x​∗g 其中，xxx 是输入向量，nnn 是向量维度，ggg​ 是可缩放的参数。 RMSNorm 的优势在于其计算简单，尤其是在处理较长序列时，可以更有效地进行归一化。 使用 SwiGLU 激活函数，SwiGLU 结合了 GLU（Gated Linear Unit）和 Swish 函数，引入了门控机制来控制输入信号的传递方式。 组成部分： GLU 部分：使用 sigmoid 函数作为门控器，对输入信号进行筛选和选择性放大。 Swish 部分：非线性函数，类似于 ReLU，但在负值区域有平滑的非线性特性。 SwiGLU(x)=σ(xW1+b1)⊙(xW2+b2)\\mathrm{SwiGLU}(x)=\\sigma(xW_1+b_1)\\odot(xW_2+b_2)SwiGLU(x)=σ(xW1​+b1​)⊙(xW2​+b2​) 其中，xxx 是输入向量，W1,W2W_{1}, W_{2}W1​,W2​ 是权重矩阵，b1,b2b_1, b_2b1​,b2​ 是偏置向量，σ\\sigmaσ 是激活函数，通常为 GELU（高斯误差线性单元），⊙\\odot⊙ 表示逐元素乘法。 SwiGLU 引入两个线性变换和一个门控机制来增强模型的表现力，优点在于通过门控机制对输入进行加权，能够更灵活地捕捉复杂的输入模式。 本文提出的 LlamaGen 模型的每一层都使用了 2D RoPE。 RoPE（Rotary Position Embedding）是一种位置编码方法，旨在解决绝对位置编码在处理较长序列时的局限性。RoPE 通过将位置信息引入到输入向量的相位中，增强了模型对相对位置的敏感度。其基本思想是将输入向量按位旋转，旋转角度与位置相关。 RoPE⁡(xi)=xi⋅cos⁡(θi)+xi+1⋅sin⁡(θi)\\operatorname{RoPE}\\left(x_i\\right)=x_i \\cdot \\cos \\left(\\theta_i\\right)+x_{i+1} \\cdot \\sin \\left(\\theta_i\\right)RoPE(xi​)=xi​⋅cos(θi​)+xi+1​⋅sin(θi​) xix_ixi​ 是输入向量的第 iii 个元素。 θi\\theta_iθi​ 是位置 iii 处的旋转角度，通常由固定的正弦和余弦函数生成。 RoPE 的优势在于能够更好地捕捉序列中的相对位置关系，提高模型的长距离依赖能力。 为了与语言模态的 LLMs 保持统一，作者没有使用 AdaLN。 AdaLN（Adaptive Layer Normalization）是一种自适应层归一化方法，旨在为不同的样本动态调整归一化参数。与传统的层归一化方法不同，AdaLN 根据输入数据自适应地调整均值和方差，从而更好地适应不同的输入特征。 AdaLN⁡(x)=x−μ(x)σ(x)⋅γ+β\\operatorname{AdaLN}(x)=\\frac{x-\\mu(x)}{\\sigma(x)} \\cdot \\gamma+\\betaAdaLN(x)=σ(x)x−μ(x)​⋅γ+β xxx 是输入向量。 μ(x)\\mu(x)μ(x) 和 σ(x)\\sigma(x)σ(x) 分别是输入 xxx 的均值和标准差。 γ\\gammaγ 和 β\\betaβ 是可训练的缩放和平移参数。 在 AdaLN 中，均值 μ(x)\\mu(x)μ(x) 和标准差 σ(x)\\sigma(x)σ(x)​ 是通过一个子网络（通常是一个简单的前馈网络）从输入数据中动态预测的。这使得归一化过程更加灵活，能够适应更复杂的输入模式。 LlamaGen 的训练使用了 CFG（Classifier-free Guidance）策略来提高视觉质量和文本-图像对齐。 训练阶段： 在训练期间，条件信息被随机丢弃，并用一个空的无条件嵌入进行替换。这种方法有助于减少模型对特定条件的依赖性，从而改善生成结果的一般化能力。 推理阶段： 对于每个 token，其 logit ℓg\\ell_gℓg​ 是通过以下方式形成的： ℓg=ℓu+s(ℓc−ℓu)\\ell_g = \\ell_u + s(\\ell_c - \\ell_u)ℓg​=ℓu​+s(ℓc​−ℓu​) 其中： ℓc\\ell_cℓc​ 表示条件logit，即基于输入文本提示的信息生成的logit。 ℓu\\ell_uℓu​ 表示无条件logit，即不考虑任何条件信息时生成的logit。 sss 是Classifier-free Guidance的比例因子，用于控制条件logit和无条件logit之间的平衡。 通过这种方式，模型可以利用无条件logit提供的通用信息，同时保留条件logit中的特定上下文信息。这种混合方法有助于提高生成图像的质量和与输入文本的对齐程度。 作者在文章的实验部分给出了使用 CFG 的结果，实验表明，CFG 过高会导致 FID 分数的下降，可以看作是多样性和保真度之间的权衡（trade-off），随着 CFG 的提高，准确率的提高和召回率的降低证明了这点。","s":"Image Generation Autoregressive Model","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation","h":"#image-generation-autoregressive-model","p":1418},{"i":1431,"t":"参考链接 原文 URL：Recent Advances in (Image) Generative Foundation Models","s":"图像生成基座模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Image-and-Video-Generative-Foundation-Model","h":"#图像生成基座模型","p":1429},{"i":1433,"t":"GAN​ 使用对抗生成策略，判别器根据真实图像判断生成器生成的图像是否逼真，二者交替训练。 Autoregressive（AR）​ 自回归生成范式，利用输入自身之前各期 [x1,...,xt−1][x_1,...,x_{t-1}][x1​,...,xt−1​] 来预测本期 xtx_txt​​​ 的表现。在图像生成中，自回归模型可以逐像素或逐块生成图像，每一步的生成基于之前已经生成的部分。自回归模型的优点在于能够捕捉图像中的复杂依赖关系，从而生成更加逼真的图像。 代表模型：VAE、VQVAE（2017）​ 参考资料 VQ-VAE 的简明介绍：量子化子编码器 Variant AutoEncoder（VAE）和 VQ-VAE 学习笔记和代码 VAE 和 VQ-VAE 都通过学习数据分布的潜在表示来生成新的样本。VAE 使用高斯分布来表示潜在空间，而 VQ-VAE 使用离散的代码簿来表示潜在空间。 具体来说，VAE 的工作原理是通过一个编码器将输入数据映射到一个潜在空间，然后通过一个解码器将潜在空间中的向量重构为原始数据。在训练过程中，VAE 会学习到数据分布的潜在表示，并能够生成与训练数据类似的新样本。 VQ-VAE 的工作原理与 VAE 类似，但它使用离散的代码簿来表示潜在空间。VQ-VAE 首先将编码器输出的向量进行量化，将其映射到代码簿中的最近向量。然后，解码器使用代码簿中的向量来重构原始数据。VQ-VAE 的优势在于，它可以学习到数据中的离散结构和语义信息，并可以避免过拟合。 代表模型：VQGAN（2021）​ 代表模型：ViT-VQGAN（2022）​ VQ (Vector Quantization) 的改进：在原有的 VQ-VAE 基础上进行了改进，通过引入更复杂的量化器和更强大的解码器，使得生成的图像质量得到了显著提升。 GAN (Generative Adversarial Network) 的结合：将 VQ 和 GAN 结合，利用 Style-GAN 的判别器来提升生成图像的细节和逼真度。 **Transformer 的使用：**将 VQ-VAE 和 VQGAN 的 Encoder、Decoder 中原来使用的 CNN 结构替换为 ViT。一是因为数据量丰富，二是 CNN 的归纳偏置对模型的约束是有限的，三是计算效率和重建质量更显著。 代表模型：VAR​ 上图展示了不同方式的自回归生成模型，VAR 方法在每个时间序列节点上都根据之前各时间步的输出预测出当前时间步的，且每个时间步均预测出完整的目标图像，且分辨率随时间推移逐步提升至高清图像，即 next-resolution prediction。 Masked-prediction model（Non-AR）​ 参考资料 生成周刊·第一期 代表模型：MaskGIT: Masked Generative Image Transformer​ 这种生成模型依赖一个预训练好的 VQGAN，能将图片 tokenize 成一组量化后的 visual tokens。VQGAN 编码图片得到的 tokens 是离散的，所有可能的 tokens 构成一个 codebook，假设其中包含 KKK 个 token 选项。MVTM 训练就是指给定 masked tokens，让网络预测这些被 masked 掉的 tokens。对于每个被 masked 掉的 token，网络给出一个 KKK 维向量预测当前 token 属于 codebook 中每个 token 的可能性，类似于完成一个 KKK 分类任务。 上图展示了传统 AR 模型和 MaskGIT 在推理过程中的区别。与之前 SOTA 使用的 Autoregressive 方法——逐行再逐列依次生成 image token 不同，MaskGIT 在推理时的每次迭代从一组 masked tokens 中预测出每个位置出现 visual token 的可能性，然后仅保留那些置信度足够高的位置的 visual token，然后继续将当前预测结果再送入网络进行下一轮预测，直到所有位置的 visual token 都被预测出来。与之前常用的自回归方法不同，每轮预测都是基于对图片的全局感知，可以并行预测。这样网络仅需 8 次前向传播就能生成高质量的图片。 顺序解码与 MaskGIT 计划并行解码的比较。第 1 行和第 3 行是每次迭代时的输入潜在掩码，第 2 行和第 4 行是每次迭代时每个模型生成的样本。MaskGIT 的解码从所有未知代码（浅灰色标记）开始，逐渐用更多更分散的预测并行填充潜表征（深灰色标记），预测标记的数量随着迭代急剧增加。MaskGIT 只用了 8 次迭代就完成了解码，而顺序法需要 256 轮。 代表模型：MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis​ Diffusion Model​ 参考链接 深度理解变分自编码器(VAE) | 从入门到精通","s":"四种生成范式","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Image-and-Video-Generative-Foundation-Model","h":"#四种生成范式","p":1429},{"i":1435,"t":"数据：Re-caption 与 text encoder（T5） 结构：从 U-Net 到纯 Transformer，代表论文 Scalable Diffusion Models with Transformers 训练范式：使用 Rectified Flow 加速生成过程，参考链接 Diffusion学习笔记（十二）——Rectified Flow","s":"如何训练优秀的生成基座模型？","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Image-and-Video-Generative-Foundation-Model","h":"#如何训练优秀的生成基座模型","p":1429},{"i":1437,"t":"参考链接 原文 URL：Video and 3D Generation","s":"视频生成基座模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Image-and-Video-Generative-Foundation-Model","h":"#视频生成基座模型","p":1429},{"i":1439,"t":"important 这里记录着在学习过程中发现的理解或操作方面出现的错误，温故知新。","s":"查漏补缺","u":"/en/docs/Deep-Learning/Fill-The-Gaps","h":"","p":1438},{"i":1441,"t":"进行广播的条件：两个矩阵的后缘维度相同或其中一方的维度为1。 后缘维度相同：A为（3，4，5）的三维数据，B为（4，5）的二维数组。由于A和B的后缘维度都为（4，5），所以可以进行广播。同理，当A为（3，4）的二维数组，B为（4，）的数组，他们的后缘维度都是4，所以可以进行广播。 后缘维度中有一方维度为1：A为（4，5）的二维数组，B为（4，1）的二维数组，其中一方维度为1，可以进行广播。 广播的原理：在运算过程中，Python逐步对数组进行广播，并不进行实际的复制操作，节省内存。 以下是举出具体例子进行分析： import numpy as np import torch x = torch.tensor(np.arange(9), dtype=torch.float32) x = torch.reshape(x, (3, 3)) print(x) x1 = x[:, :, None] # (3, 3, 1) print(x1) \"\"\" tensor([[[0.], [1.], [2.]], [[3.], [4.], [5.]], [[6.], [7.], [8.]]]) \"\"\" x2 = x[:, None] # (3, 1, 3) print(x2) \"\"\" tensor([[[0., 1., 2.]], [[3., 4., 5.]], [[6., 7., 8.]]]) \"\"\" output = x1 + x2 print(output) \"\"\" tensor([[[ 0., 1., 2.], [ 1., 2., 3.], [ 2., 3., 4.]], [[ 6., 7., 8.], [ 7., 8., 9.], [ 8., 9., 10.]], [[12., 13., 14.], [13., 14., 15.], [14., 15., 16.]]]) \"\"\" 由于x1与x2在第0维度上维度相同，所以Python可以直接进行逐元素相加，即依次进行如下运算 x1[0,:,:]+x2[0,:,:](1)x1[0, :, :] + x2[0, :, :] \\tag{1}x1[0,:,:]+x2[0,:,:](1) x1[1,:,:]+x2[1,:,:](2)x1[1, :, :] + x2[1, :, :] \\tag{2}x1[1,:,:]+x2[1,:,:](2) x1[2,:,:]+x2[2,:,:](3)x1[2, :, :] + x2[2, :, :] \\tag{3}x1[2,:,:]+x2[2,:,:](3) 但在第0维度的相加过程中出现了shape为(3, 1)的矩阵与shape为(1, 3)的矩阵相加的情况，此时进行广播，将(3, 1)的每一列复制三次为(3, 3)，将(1, 3)的每一行复制三次为(3, 3)，再进行逐元素相加。 其实，上述的过程还可以再细分为，x1[0, 0, :]与x2[0, 0, :]相加时出现了第一次广播，将x1[0, 0, :]复制了三次与x2[0,0, :]完成相加，这里不再赘述，最终想表达的原理是广播机制是在运算过程当中进行的，并非一次性将二者全部复制为对应的最小公倍数形状后再进行运算。","s":"Python的广播机制","u":"/en/docs/Deep-Learning/Fill-The-Gaps","h":"#python的广播机制","p":1438},{"i":1443,"t":"点积在Python中对应的运算符为*，进行矩阵之间的逐元素乘法。在点积运算中，运算矩阵二者形状不一样时可能涉及到广播机制； 矩阵乘法在Python中对应的运算符为@，进行常规矩阵乘法。遵守左矩阵的列数必须等于右矩阵的行数，且输出矩阵的行数等于左矩阵的行数、输出矩阵的列数等于右矩阵的列数的规则。","s":"点积（dot product）与矩阵乘法（matmul product）","u":"/en/docs/Deep-Learning/Fill-The-Gaps","h":"#点积dot-product与矩阵乘法matmul-product","p":1438},{"i":1445,"t":"当你有多个列表（或其他可迭代对象）时，zip函数可以将它们逐个配对成元组。而*操作符用于解压元组，将元组中的元素分别作为参数传递给函数。 以下是一个简单的例子： # zip函数的例子 list1 = [1, 2, 3] list2 = ['a', 'b', 'c'] list3 = ['x', 'y', 'z'] # 使用zip将多个列表配对成元组 zipped_lists = zip(list1, list2, list3) # 打印配对后的元组 for item in zipped_lists: print(item) 输出： (1, 'a', 'x') (2, 'b', 'y') (3, 'c', 'z') 在这个例子中，zip将list1、list2和list3中相同位置的元素组合成元组。 接下来，我们可以使用 * 操作符解压这些元组： # *解压操作的例子 unzipped_lists = zip(*zipped_lists) # 打印解压后的列表 for item in unzipped_lists: print(item) 输出： (1, 2, 3) ('a', 'b', 'c') ('x', 'y', 'z') 在这个例子中，*操作符将先前由zip组合的元组解压，分别放回原始的列表。","s":"zip函数与解压操作*","u":"/en/docs/Deep-Learning/Fill-The-Gaps","h":"#zip函数与解压操作","p":1438},{"i":1447,"t":"参考资料 https://www.jianshu.com/p/41c15d301542 对于不同的网络层，输入的维度虽然不同，但是通常输入的第一个维度都是batch_size，比如torch.nn.Linear的输入(batch_size,in_features)，torch.nn.Conv2d的输入(batch_size, C, H, W)。 而RNN的输入是(seq_len, batch_size, input_size)，batch_size位于第二维度！虽然可以将batch_size和序列长度seq_len对换位置，此时只需令batch_first=True。 但是为什么RNN输入默认不是batch first=True？这是为了便于并行计算。 因为cuDNN中RNN的API就是batch_size在第二维度。进一步讲，batch first意味着模型的输入（一个Tensor）在内存中存储时，先存储第一个sequence，再存储第二个，而如果是seq_len first，模型的输入在内存中，先存储每一个sequence的第一个元素，然后是第二个元素，两种区别如下图所示： seq_len first意味着不同序列中同一个时刻对应的输入单元在内存中是毗邻的，这样才能做到真正的batch计算。","s":"对batch_first参数的理解","u":"/en/docs/Deep-Learning/Fill-The-Gaps","h":"#对batch_first参数的理解","p":1438},{"i":1449,"t":"参考资料： CVPR 2023 Tutorial: Denoising Diffusion Models: A Generative Learning Big Bang CVPR 2024 Tutorial: Diffusion-based Video Generative Models 【较真系列】讲人话- Diffusion Model 全解（原理+代码+公式）","s":"图像生成：扩散模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"","p":1448},{"i":1452,"t":"基于 U-Net 结构：被广泛用于 text-to-image Diffusion Model 中 Imagen Stable Diffusion eDiff-I 基于 Transformer 结构：将图像分割为 patch 后作为 tokens 输入至 Transformer 中 Scalable Diffusion Models with Transformers One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale Simple Diffusion: End-to-end Diffusion for High Resolution Images","s":"Diffusion Model 的结构","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"#diffusion-model-的结构","p":1448},{"i":1454,"t":"目前有三种常见的引导（Guidance）方法： RGB Pixel Guidance Text Guidance Reference Image Guidance RGB Pixel Guidance​ ICLR 2022, SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations​ Given an input image with user guide in a form of manipulating RGB pixels, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. 通过用户在原图上给出一些引导，比如 RGB 像素的涂鸦（stroke painting），甚至可以不给定原图，直接纯手工绘制一个涂鸦画作为输入，模型首先对输入添加噪声，最后通过随机微分方程的先验增加图片的真实性，最终根据输入的带有引导信息的图像生成对应的结果。 Synthesizing images from strokes with SDEdit. The blue dots illustrate the editing process of our method. The green and blue contour plots represent the distributions of images and stroke paintings, respectively. Given a stroke painting, we first perturb it with Gaussian noise and progressively remove the noise by simulating the reverse SDE. This process gradually projects an unrealistic stroke painting to the manifold of natural images. 下图中的蓝点代表本文的编辑过程，绿色分布代表真实图像的分布，蓝色分布代表 stroke paintings 的分布。 当模型首先被输入 stroke painting 后，使用高斯噪声进行扰动，然后通过模拟反向随机微分方程逐步移除噪声。这一过程逐步将不真实的 stroke painting 投影到自然、真实图像的分布中。 Text Guidance​ ICLR 2023, DiffEdit: Diffusion-based semantic image editing with mask guidance​ 论文引入一个掩码生成模块，该模块确定图像的哪一部分应该被编辑，然后只对掩码部分执行基于文本的扩散。 首先用户输入参考图像以及两个查询文本和参考文本，查询文本 Query 是参考图像的标题或用于描述图像，参考文本 R 用于描述想要替换的效果。 掩码生成模块首先为输入图像添加噪声，并进行两次去噪，一次通过参考文本 R 进行，一次通过查询文本 Q 进行，并根据去噪结果的差异推导出参考图像中的掩码区域。 CVPR 2023, Imagic: Text-Based Real Image Editing with Diffusion Models​ 模型接受真实图像（参考图像）和目标图像文本描述 TTT 作为输入。 模型首先对目标文本进行编码，得到初始嵌入表示 etgte_{tgt}etgt​，然后优化 etgte_{tgt}etgt​ 对原始图像进行重构，得到优化后的目标文本嵌入表示 eopte_{opt}eopt​。​ 目标文本首先通过一个文本编码器，该编码器输出其对应的文本嵌入 etgt∈RT×d\\mathbf{e}_{t g t} \\in \\mathbb{R}^{T \\times d}etgt​∈RT×d，其中 TTT 是给定目标文本中的 token 数量，而 ddd 是 token 的嵌入维度。固定生成扩散模型 f0f_0f0​ 的参数，并使用重构损失来优化目标文本嵌入 ： L(x,e,θ)=Et,ϵ[∥ϵ−fθ(xt,t,e)∥22]\\mathcal{L}(\\mathbf{x}, \\mathbf{e}, \\theta)=\\mathbb{E}_{t, \\epsilon}\\left[\\left\\|\\boldsymbol{\\epsilon}-f_\\theta\\left(\\mathbf{x}_t, t, \\mathbf{e}\\right)\\right\\|_2^2\\right]L(x,e,θ)=Et,ϵ​[∥ϵ−fθ​(xt​,t,e)∥22​] 其中 t∼Uniform⁡[1,T]t \\sim \\operatorname{Uniform}[1, T]t∼Uniform[1,T]，xt\\mathbf{x}_txt​ 是输入图像 x\\mathbf{x}x 的噪声版本，θ\\thetaθ 是预训练的扩散模型权重。这使输入的图像尽可能接近的文本嵌入表示，即只更新 Text Embedding 的参数，从而得到 eopt\\mathbf{e}_{\\text {opt}}eopt​​。 这种邻近性使得在嵌入空间中进行有意义的线性插值成为可能，而对于远距离的嵌入表示来说并不表现出线性行为。 此时使用的是低分辨率版本的 Diffusion Model，分辨率 64×6464 \\times 6464×64，训练设定为 100 次迭代。 由于第一步进行的 Text Embedding Optimization 步数较少，优化后的 eopte_{opt}eopt​ 并不能直接与输入图像对齐。 因此第二步固定优化后的目标文本嵌入表示 eopte_{opt}eopt​​，仍使用第一步中的 Reconstrcution Loss 对第一步使用的 64×6464 \\times 6464×64 分辨率的预训练 Diffusion Model 进行微调，设定为 1500 次迭代。 同时，为了捕获原始图像中的细节，保持高保真度，文章还会使用初始的 Text Embedding 层，即 etgte_{tgt}etgt​ 以及原始图像通过 Reconstruction Loss 再 fine-tune 一个从 64×6464 \\times 6464×64 到 256×256256 \\times 256256×256 的超分辨率 Diffusion Model，设定为 1500 次迭代。 使用初始目标文本嵌入表示 etgte_{tgt}etgt​ 插值优化后的目标文本表示 eopte_{opt}eopt​​​，插值方式如下： eˉ=η⋅etgt+(1−η)⋅eopt\\bar{\\mathbf{e}}=\\eta\\cdot\\mathbf{e}_{tgt}+(1-\\eta)\\cdot\\mathbf{e}_{opt}eˉ=η⋅etgt​+(1−η)⋅eopt​ 插值得到的文本嵌入表示分别通过 64×6464 \\times 6464×64 的基础 Diffusion Model、从 64×6464 \\times 6464×64 到 256×256256 \\times 256256×256 的超分辨率 Diffusion Model，以及最终的从 256×256256 \\times 256256×256 到 1024×10241024 \\times 10241024×1024 的超分辨率 Diffusion Model 得到最终的目标图像。 InstructPix2Pix: Learning to Follow Image Editing Instructions​ 主要方法​ Our method consists of two parts: generating an image editing dataset, and training a diffusion model on that dataset. (a) We first use a finetuned GPT-3 to generate instructions and edited captions. (b) We then use StableDiffusion [52] in combination with Prompt-to-Prompt [17] to generate pairs of images from pairs of captions. We use this procedure to create a dataset (c) of over 450,000 training examples. (d) Finally, our InstructPix2Pix diffusion model is trained on our generated data to edit images from instructions. At inference time, our model generalizes to edit real images from human-written instructions. 论文使用现有的 LLMs 工具生成训练数据集，再使用上述生成的数据微调 Stable Diffusion Model，最终得到可以根据指令 caption 进行图像编辑的 Diffusion Model。 训练数据生成阶段： 首先使用 Input Caption 描述一张图像，并使用 GPT-3 通过指令 Instruction 生成修改过后的图像描述 Edited Caption，如 Input Caption “一张女孩骑在马上的图片”通过指令 Instruction “让她骑在龙上”得到 Edited Caption “一张女孩骑在龙上的图片” 使用 Stable Diffusion 以及 Prompt2Prompt 通过 Input Caption 以及 Edited Caption 生成一组图像对 最终得到图像对以及指令 Instruction 组成一组训练数据 使用上述训练数据微调 Stable Diffusion： 首先，对于一张图像 xxx，我们将其通过编码器 E\\mathcal{E}E 转换为对应的潜变量 z=E(x)z=\\mathcal{E}(x)z=E(x) 。然后，通过一系列的时间步长 t∈Tt \\in \\mathrm{T}t∈T，我们将噪声逐步加入到潜变量 zzz 中，得到带有噪声的潜变量 ztz_tzt​ 。这里的噪声水平随着时间步长的增加而增大。 接下来，我们训练一个网络 ϵθ\\epsilon_\\thetaϵθ​ ，使其能够在给定图像条件 cIc_IcI​ 和文本指令条件 cTc_TcT​ 的情况下，预测出在当前时间步长 ttt，ztz_tzt​ 所加入的噪声。 为了达到这一目标，我们需要最小化以下目标函数 Latent Diffusion Objective： L=EE(x),E(cI),cT,ε∼N(0,1),t[∥ε−ϵθ(zt,t,E(cI),cT))∥22]\\left.L=\\mathbb{E}_{\\mathcal{E}(x), \\mathcal{E}\\left(c_I\\right), c_T, \\varepsilon \\sim N(0,1), t}\\left[\\| \\varepsilon-\\epsilon_\\theta\\left(z_t, t, \\mathcal{E}\\left(c_I\\right), c_T\\right)\\right) \\|_2^2\\right]L=EE(x),E(cI​),cT​,ε∼N(0,1),t​[∥ε−ϵθ​(zt​,t,E(cI​),cT​))∥22​] E\\mathbb{E}E 代表数学期望，训练网络 ϵθ\\epsilon_\\thetaϵθ​​​ 使其预测的噪声与真实噪声之间的差距尽可能小，即最小化它们之间的欧几里得距离。 To support image conditioning, we add additional input channels to the first convolutional layer, concatenating ztz_tzt​ and E(cI)\\mathcal{E}(c_I)E(cI​). 为了可以使 Stable Diffusion 支持以图像为条件，作者在第一个卷积层中将 ztz_tzt​ 与 E(cI)\\mathcal{E}(c_I)E(cI​)​​​ 进行 concat，同时扩充对应网络的 channel 数。 在微调之前，预训练扩散模型通过 Stable Diffusion v1.5 checkpoint 进行初始化，额外增加通道数的卷积参数都被初始化为 0。 训练设置与参数​ 模型在 256×256256 \\times 256256×256​ 分辨率上的 batch size 为 1024，Stable Diffusion 的扩散步数设置为 10, 000，在 8 张 40 GB 显存的 NVIDIA A100 GPU 上训练了 25.5 小时。 推理设置与参数​ 虽然训练过程在 256×256256 \\times 256256×256 分辨率上进行，但是在推理阶段直接生成 512×512512 \\times 512512×512 分辨率的图像效果仍然很好，推理过程扩散步数设置为 100，在 NVIDIA A100 GPU 上推理速度为 9 秒。 Reference Image Guidance​ CVPR 2023, DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation​ 通过接收一组（3~5 张）相同目标（文中称为主题 Subject）的图像，模型可以根据用户提供的 Text Guidance 生成统一主题但不同姿势、不同场景的语义上新的图像。 Text-to-Image Diffusion Model​ 本文首先使用了一个预训练的 Text-to-Image Diffusion Model x^θ\\hat{x}_\\thetax^θ​，该模型在给定初始噪声图 ϵ∼N(0,I)\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})ϵ∼N(0,I) 和由文本编码器 Γ\\GammaΓ 和文本提示 PPP 生成的条件向量 c=Γ(P)\\mathbf{c} = \\Gamma(P)c=Γ(P) 的情况下，生成图像 xgen=x^θ(ϵ,c)x_{gen} = \\hat{x}_\\theta(\\epsilon, \\mathbf{c})xgen​=x^θ​(ϵ,c)。使用平方误差损失计算重构损失进行训练，以对噪声图像 zt:=αtx+σtϵz_t := \\alpha_t \\mathbf{x} + \\sigma_t \\epsilonzt​:=αt​x+σt​ϵ 进行去噪，如下所示： Ex,c,ϵ,t[wt∥x~θ(αtx+σtϵ,c)−x∥22]\\mathbb{E}_{\\mathbf{x}, \\mathbf{c}, \\epsilon, t} \\left[ w_t \\| \\tilde{x}_\\theta (\\alpha_t \\mathbf{x} + \\sigma_t \\epsilon, \\mathbf{c}) - \\mathbf{x} \\|_2^2 \\right] \\quadEx,c,ϵ,t​[wt​∥x~θ​(αt​x+σt​ϵ,c)−x∥22​] 其中 x\\mathbf{x}x 是训练集中的 ground-truth 图像，c\\mathbf{c}c 是文本提示 PPP​ 生成的条件向量。 Unique Subject Identifier​ Our first task is to implant the subject instance into the output domain of the model such that we can query the model for varied novel images of the subject. 为了使生成模型生成更多主题不变的相关图像，首要的任务是将参考图像的主题植入到生成模型的输出域中。 一种自然的想法就是微调模型，然而在大规模语料库上预训练的 LLMs 在少量样本上进行 few-shot 微调时很容易产生**过拟合（overfitting）以及语言漂移（language drift）**问题，因此本文设计了一种新颖的微调方法，为输入的每组主题图像匹配 Subject Indentifier。 具体而言，首先模型为每一组特定主题的图像产生一个形如“A [identifier] [class noun]”的标签，其中： [identifier] 是与主题相关的唯一标识，也是模型可以保存主题特征的重要原因。 本文提出的 Subject Identifier 产生方法是从 Vocabulary 中找出罕见的 tokens，并将其映射到文字空间，以减少 identifier 中包含的先验。 For Imagen, we find that using uniform random sampling of tokens that correspond to 3 or fewer Unicode characters (without spaces) and using tokens in the T5-XXL tokenizer range of {5000, ..., 10000} works well. 对于 Imagen 作为生成模型的情况，作者发现 identifier 所包含的 token 数小于等于 3 时效果最好，当使用 T5-XXL Tokenizer 时，选择 {5000, ... , 10000} 的 token 效果较好。 [class noun] 是类别标签，可以通过用户提供或分类器生成，将模型对类别具有的先验融合到主题标识中。 Class-specific Prior Preservation Loss​ 在微调生成模型时会面临两个主要的问题：语言漂移（Language Drift）和多样性下降。 为了解决上述问题，作者提出了 Class-specific Prior Preservation Loss。 首先使用使用固定参数的 Text-to-Image Diffusion Model 产生 ground-truth 图像 xpr=x^θ(zt1,cpr)x_{pr} = \\hat{x}_\\theta(z_{t_1}, c_{pr})xpr​=x^θ​(zt1​​,cpr​)，其中随机初始噪声 zt1∼N(0,I)z_{t_1} \\sim \\mathcal{N}(0, \\mathbf{I})zt1​​∼N(0,I) 和条件向量 cpr:=Γ(f(\\mathbf{c}_{\\mathrm{pr}}:=\\Gamma(f(cpr​:=Γ(f(\"A [class noun]\" ))))))。损失变为： Ex,c,ϵ,ϵ′,t[wt∥x^θ(αtx+σtϵ,c)−x∥22+λwt′∥x^θ(αt′xpr+σt′ϵ′,cpr)−xpr∥22]\\mathbb{E}_{\\mathbf{x}, \\mathbf{c}, \\epsilon, \\epsilon', t} \\left[ w_t \\| \\hat{x}_\\theta (\\alpha_t \\mathbf{x} + \\sigma_t \\epsilon, \\mathbf{c}) - \\mathbf{x} \\|_2^2 + \\lambda w_{t'} \\| \\hat{x}_\\theta (\\alpha_{t'} x_{pr} + \\sigma_{t'} \\epsilon', c_{pr}) - x_{pr} \\|_2^2 \\right]Ex,c,ϵ,ϵ′,t​[wt​∥x^θ​(αt​x+σt​ϵ,c)−x∥22​+λwt′​∥x^θ​(αt′​xpr​+σt′​ϵ′,cpr​)−xpr​∥22​] 其中第二项是作者提出的用于保持模型对类别先验的损失函数，λ\\lambdaλ 控制该项的相对权重。 微调 Diffusion Model​ 在微调过程中，每次迭代输入 3~5 张主题图像，进行 1000 次迭代就可以达到比较好的效果。 当生成模型为 Imagen 时，在 TPUv4 上耗时 5 min；当生成模型为 Stable Diffusion 时，在 NVIDIA A100 GPU 上耗时 5 min。 其他细节​ 本文使用的 Tokenizer 是 SentencePiece，编码文本的语言模型是 T5-XXL。 训练数据来自于作者的搜集，共包含 30 个主题，其中 21 个主题是物体，其余 9 个主题是动物。","s":"使用 Diffusion Model 对图像进行编辑和定制","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"#使用-diffusion-model-对图像进行编辑和定制","p":1448},{"i":1457,"t":"NeurIPS 2022, Video Diffusion Models​ 2022, Imagen Video: High Definition Video Generation with Diffusion Models​ ICLR 2023, Make-A-Video: Text-to-Video Generation without Text-Video Data​ CVPR 203, Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models (Video LDM)​","s":"视频生成模型","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"#视频生成模型","p":1448},{"i":1459,"t":"arXiv 2023, Structure and Content-Guided Video Synthesis with Diffusion Models (Gen-1)​ arXiv 2023, Pix2Video: Video Editing Using Image Diffusion​","s":"视频的风格转换（Style Transfer）和编辑（editing）方法","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Diffusion-Models","h":"#视频的风格转换style-transfer和编辑editing方法","p":1448},{"i":1461,"t":"介绍 欢迎来到笔记本的深度学习部分 若更新不及时，欢迎通过 GitHub 联系交流 有关论文笔记的说明 为了节省写作时间，论文笔记部分的文章通常只讲述文章的方法论，Introduction 以及 Related Work 详情可以参考论文原文","s":"Welcome","u":"/en/docs/Deep-Learning/intro","h":"","p":1460},{"i":1463,"t":"如果可以帮到你的话就给个免费的 Star 吧！","s":"支持我！","u":"/en/docs/Deep-Learning/intro","h":"#支持我","p":1460},{"i":1465,"t":"按照官方文档将 Docusaurus 从 V2.4.3 升级至 V3.5.2，记录以下主要问题。","s":"更新至 Docusaurus V3","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","h":"","p":1464},{"i":1467,"t":"升级至 V3 版本需要升级 MDX 版本至 V3，升级后在渲染 mardown 文件时出现了大量报错，这是因为在新的 MDX 中默认将将 .md 文件也当作 .mdx 文件进行渲染了，在 docusaurus.config.js 中进行如下声明即可。 markdown:{ format: \"detect\" },","s":"MDX 升级后的大量渲染报错","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","h":"#mdx-升级后的大量渲染报错","p":1464},{"i":1469,"t":"If you use Docusaurus to render Math Equations, you should upgrade the MDX plugins. Make sure to use remark-math 6 and rehype-katex 7 for Docusaurus v3 (using MDX v3). We can't guarantee other versions will work. Docusaurus 默认使用 katex 进行 LaTex 公式渲染，需要同步升级 remark-math 以及 rehype-katex。","s":"升级后的数学公式渲染问题","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","h":"#升级后的数学公式渲染问题","p":1464},{"i":1471,"t":"Docusaurus now implements admonitions with Markdown Directives (implemented with remark-directive). Docusaurus 在升级后需要额外安装 remark-directive 来实现 Admonitions 的渲染，执行下面的命令安装插件。 npm install --save remark-directive","s":"Admonitions 无法正常渲染","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","h":"#admonitions-无�法正常渲染","p":1464},{"i":1473,"t":"按照官方文档的说明修改 package.json 文件中的各种依赖包的版本，在升级过程中最好首先删除原项目中的 node_modules 文件夹，在修改完所有版本后，直接执行 npm install 来避免更新过程中新版本包和其它旧版本包的依赖关系发生冲突。","s":"更新方式","u":"/en/docs/Others/博客搭建/upgrade_to_docusaurusv3","h":"#更新方式","p":1464},{"i":1475,"t":"告示栏的启用 在docusaurus.config.js的themeConfig中加入以下代码 announcementBar: { id: 'announcementBar-3', content: 'Welcome to my notebook!', isCloseable: false, }, 告示栏的背景个性化 在custom.css中加入以下代码 div[class^='announcementBar_'] { background: repeating-linear-gradient( -35deg, var(--ifm-color-primary-lighter), var(--ifm-color-primary-lighter) 20px, var(--ifm-color-primary-lightest) 10px, var(--ifm-color-primary-lightest) 40px ); font-weight: 700; }","s":"告示栏","u":"/en/docs/Others/博客搭建/announcement_bar","h":"","p":1474},{"i":1477,"t":"原文链接：https://arxiv.org/pdf/2404.02905","s":"自回归模型：VAR","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"","p":1476},{"i":1479,"t":"We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine “next-scale prediction” or “next-resolution prediction”, diverging from the standard raster-scan “next-token prediction”. This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and can generalize well: VAR, for the first time, makes GPT-style AR models surpass diffusion transformers in image generation. On ImageNet 256×256 benchmark, VAR significantly improve AR baseline by improving Fréchet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with 20× faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near −0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.","s":"摘要","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#摘要","p":1476},{"i":1481,"t":"通过回顾先前的视觉 Autoregressive 模型（即采用 next-token 范式进行生成），从 scaling laws 和性能的角度将其与 NLP 领域的 LLMs 进行比较，宏观上说明了传统视觉 AR 模型存在的问题。 传统视觉 AR 模型是否遵循 scaling laws 有待探索和验证 性能有待提升 上图展示了传统 Visual AR 模型在 scaling laws 方面的局限性，并以 FID 作为评价指标，展示了性能效果方面的不足。","s":"研究背景与动机","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#研究背景与动机","p":1476},{"i":1483,"t":"首先文章花费了大量篇幅来用数学语言描述以 next-token prediction 作为生成范式的传统视觉自回归模型的定义和流程，大致包括以下流程： 输入 raw image，Encoder 得到对应的 feature map。 f=E(im)f=\\mathcal{E}(im)f=E(im) 输入feature map，Quantizer 得到对应的量化 image token。 q=Q(f)q=\\mathcal{Q}(f)q=Q(f) 在量化步骤，会将每个特征向量映射到与其在欧几里得意义上最接近的 Codebook ZZZ 中的 code。 q(i,j)=(arg⁡min⁡v∈[V]∥lookup(Z,v)−f(i,j)∥2)∈[V]q^{(i,j)}=\\left(\\arg\\min_{v\\in[V]}\\|\\text{lookup}(Z,v)-f^{(i,j)}\\|_2\\right)\\in[V]q(i,j)=(argv∈[V]min​∥lookup(Z,v)−f(i,j)∥2​)∈[V] Decoder 通过接收在 Codebook 中查找得到的 f^\\hat{f}f^​ 生成重构的图像。 f^=lookup⁡(Z,q)\\hat{f}=\\operatorname{lookup}(Z,q)f^​=lookup(Z,q) im^=D(f^)\\hat{im}=\\mathcal{D}(\\hat{f})im^=D(f^​) L=∥im−im^∥2+∥f−f^∥2+λPLP(im^)+λGLG(im^)\\mathcal{L}=\\|im-i\\hat{m}\\|_2+\\|f-\\hat{f}\\|_2+\\lambda_\\text{P}\\mathcal{L}_\\text{P}(i\\hat{m})+\\lambda_\\text{G}\\mathcal{L}_\\text{G}(i\\hat{m})L=∥im−im^∥2​+∥f−f^​∥2​+λP​LP​(im^)+λG​LG​(im^) 下图展示了传统 VAR 方法与本文提出的 VAR 方法的对比。其中图（a）展示了 NLP 领域的 next-token prediction，图（b）展示了上述公式定义的 next-image-token prediction 的过程，包含量化与展平的步骤，图（c）展示了本文提出的 next-scale prediction。 文章发现了目前传统 VAR 模型存在的三个问题。 VQGAN 违反了 Autoregressive 的数学前提。Autoregressive 模型假设当前时间步的 token xtx_txt​ 只取决于其之前时间步的 token 前缀 (x1,x2,...,xt−1)(x_1, x_2, ..., x_{t-1})(x1​,x2​,...,xt−1​)，具有单向相关性（unidirectional）。而 VQGAN 中的 image encoder 直接从具有双向相关性（bidirectional）的 feature map 中进行量化和展平，因此得到的 image tokens 同样具有双向相关性，违反了数学假设。 作者在附录中检查了 VQGAN 模型在量化步骤之前的注意力层输出的图像注意力分数，证明了较强的双向相关性。 作者解释，这是因为在图像 VAE 以及其他类似的工作的自注意力层中，并没有使用任何注意力掩码机制，如 sequence mask 以及 causal attention 等，导致了双向相关性。 This is not surprising since the VQVAE model, trained to reconstruct images, leverages self-attention layers without any attention mask. Some work [67] has used causal attention in self-attention layers of a video VAE, but we did not find any image VAE work uses causal self-attention. image tokens 的空间结构性被破坏。由于先前的 VAR 工作均是采用类似于先列后行的一维顺序存储 image tokens 并进行 Autoregressive 生成，image tokens 的扁平化破坏了图像特征图固有的空间局部性。 时间复杂度过高，影响生成效率。使用传统的视觉 Autoregressive 方法生成 x=(x1,x2,…,xn×n)x=(x_1,x_2,\\ldots,x_{n\\times n})x=(x1​,x2​,…,xn×n​) 的 token 序列，需要 O(n2)\\mathcal{O}(n^{2})O(n2) 的注意力步骤以及 O(n6)\\mathcal{O}(n^{6})O(n6) 的计算复杂度。","s":"问题发现与提出","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#问题发现与提出","p":1476},{"i":1486,"t":"本文的工作重新考虑了以什么样的顺序生成图像。人类通常以分层的方式感知或创建图像，首先捕获全局结构，然后捕获局部细节。这种多尺度、由粗到精（coarse-to-fine）的方法很自然地给图像暗示了一种顺序。此外，受广泛使用的多尺度（multi-scale）设计工作的启发，本文将图像的自回归学习定义为下图展示的 next-scale prediction。 自回归过程从分辨率为 1×11\\times11×1​ 的 token map 开始，并逐步扩大分辨率：在每一步，Transformer 以之前生成的所有分辨率的 token maps 为条件预测下一大分辨率的 token map。 通过从 next-token prediction 策略转变为 next-scale prediction 策略，重新概念化了对图像的自回归建模。在这里，自回归单元是整个 token map，而不是传统方法的单个 token。先将特征图 f∈Rh×w×Cf \\in \\mathbb{R}^{h \\times w \\times C}f∈Rh×w×C 量化为 KKK 个多尺度标记图 (r1,r2,…,rK)(r_1, r_2, \\ldots, r_K)(r1​,r2​,…,rK​)，每个图的分辨率 hk×wkh_k \\times w_khk​×wk​ 逐步增加，最终达到 rKr_KrK​ 与原始特征图的分辨率 h×wh \\times wh×w 匹配。自回归似然性公式为： p(r1,r2,…,rK)=∏k=1Kp(rk∣r1,r2,…,rk−1)p(r_1, r_2, \\ldots, r_K) = \\prod_{k=1}^{K} p(r_k \\mid r_1, r_2, \\ldots, r_{k-1})p(r1​,r2​,…,rK​)=k=1∏K​p(rk​∣r1​,r2​,…,rk−1​) 其中每个自回归单元 rk∈[V]hk×wkr_k \\in [V]^{h_k \\times w_k}rk​∈[V]hk​×wk​ 是包含 hk×wkh_k \\times w_khk​×wk​ 个标记的第 kkk 个尺度的标记图，而序列 (r1,r2,…,rk−1)(r_1, r_2, \\ldots, r_{k-1})(r1​,r2​,…,rk−1​) 作为 rkr_krk​ 的“前缀”。在第 kkk 个自回归步骤中，所有 rkr_krk​ 中的 h_k \\times w_k 标记的分布将并行生成，并以 rkr_krk​ 的前缀和关联的第 kkk 个位置嵌入图为条件。如下图所示。 请注意，在 VAR 的训练中，使用逐块的因果注意力掩码，以确保每个 rkr_krk​ 只能关注其前缀 r≤kr_{\\leq k}r≤k​，从而满足 Autoregressive 模型的数学假设前提。 本文在结构上的主要贡献是开发了适用于 next-scale prediction 的 multi-scale VQ quantizer，同时结合新的 VQ quantizer 提出了新的图像自回归生成模型，并进一步论证了该方法展示出的与 LLMs 类似的 scaling laws 能力。","s":"主要方法","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#主要方法","p":1476},{"i":1488,"t":"Multi-scale VQ quantizer​ 首先，需要设计一个满足多尺度要求的 VQ tokenizer，作者使用了与 VQVAE 相同的框架，并采用了改进的多尺度量化层，并加入了对 feature map 的残差设计：对 encoder 输出的 feature map 进行 interpolate 构建不同尺度的 feature map，不同 feature map 之间通过计算残差的方式进行连接，并结合 quantizer 得到离散序列。VQ quantizer（VAR tokenizer）的具体算法如下图所示。 VAR Transformer​ 作者将重点放在了 VAR tokenizer 的理念和设计上，在 VAR Transformer 中保持了与 GPT-2 和 VQGAN 相同的简洁设计，在结构设计上只融合了 adaptive normalization（AdaLN）。 在训练完 VQ tokenizer 后，需要在离散化后的序列上训练生成模型，上图中 r1,...,rkr_1,...,r_kr1​,...,rk​ 分别表示不同尺度的离散序列。作者将传统的单向自回归模型修改为双向与单向混合的模式，同一个尺度的图片内部使用双向 attention，token 彼此可见，不同尺度的图片之间使用单向 attention ，具有从粗粒度到细粒度的 causal dependency，保证了满足 Autoregressive 假设的数学前提。 单个尺度的图片可以一步生成，生成所需的迭代步数取决于 VQ tokenizer 设计的尺度层数 KKK​。","s":"模型主要结构","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#模型主要结构","p":1476},{"i":1490,"t":"期待更好的 VQVAE tokenizer 可以提升 VAR 模型的能力。 本文只实现了 class-conditional generation，可以继续扩展为多模态任务，text-prompt generation。目前我认为比较常见的一个想法就是结合 T5 text-encoder，将文字 视频生成。可以将视频看作是 3D pyramids，尝试 3D next-scale prediction。 作者表示，与 Diffusion Model-based 的视频生成模型 SORA 相比，由于 VAR 模型在结构与上与 LLMs 更相似，可以更好地与 LLMs 的技术相结合。","s":"局限性与展望","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#局限性与展望","p":1476},{"i":1492,"t":"本文主要从视觉 Autoregressive 模型生成图像像素的顺序出发，重新思考，通过人类感知图像的方式设计了 next-scale 的生成范式，每一步迭代生成一张完整的图像，但生成图像的分辨率逐步提升，最终得到高像素目标图像。 对文章开篇提出的问题的解决： 使用 causal attention 对自注意力进行掩码，从而满足 Autoregressive 模型对时间序列的数学假设。 在 quantization 步骤使用二维的方式存储整个 image token map，保证了结构的完整性。 得益于多尺度思想的生成方式，时间复杂度和计算开销显著降低。以生成 n2n^2n2 个 image tokens 为例，传统的视觉 Autoregressive 生成需要 O(n2)\\mathcal{O}(n^2)O(n2) 次解码迭代和 O(n6)\\mathcal{O}(n^6)O(n6) 次总计算。相比之下，本文提出的 VAR 只需要 O(log(n))\\mathcal{O}(log(n))O(log(n)) 次迭代和 O(n4)\\mathcal{O}(n^4)O(n4)​ 次总计算量。 文章还论证了 VAR 模型展现出的与 LLMs 相同的 scaling laws 能力。","s":"总结","u":"/en/docs/Deep-Learning/图像生成与视频生成大模型/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction","h":"#总结","p":1476},{"i":1494,"t":"tip 欢迎来到笔记本的其他部分","s":"Welcome","u":"/en/docs/Others/intro","h":"","p":1493},{"i":1496,"t":"如果可以帮到你的话就给个免费的 Star 吧！","s":"支持我！","u":"/en/docs/Others/intro","h":"#支持我","p":1493},{"i":1498,"t":"通过编写脚本函数的方式，手动开启代理","s":"终端代理","u":"/en/docs/Others/Linux/实用工具/终端代理","h":"","p":1497},{"i":1500,"t":"新建脚本文件terminal_proxy.sh # 开启代理 function proxy_on(){ export ALL_PROXY=socks5://127.0.0.1:7890 export http_proxy=http://127.0.0.1:7890 export https_proxy=https://127.0.0.1:7890 echo -e \"已开启代理\" } # 关闭代理 function proxy_off(){ unset ALL_PROXY unset http_proxy unset https_proxy echo -e \"已关闭代理\" }","s":"一、编写脚本","u":"/en/docs/Others/Linux/实用工具/终端代理","h":"#一编写脚本","p":1497},{"i":1502,"t":"fish的配置文件：~/.config/fish/config.fish zsh的配置文件：~/.zshrc bash的配置文件：~/.bashrc 在配置文件末尾添加以下代码 source /path/terminal_proxy.sh","s":"二、关联终端配置文件","u":"/en/docs/Others/Linux/实用工具/终端代理","h":"#二关联终端配置文件","p":1497},{"i":1504,"t":"在终端中输入以下命令即可开启代理 proxy_on 在终端中输入以下命令即可关闭代理 proxy_off","s":"三、使用","u":"/en/docs/Others/Linux/实用工具/终端代理","h":"#三使用","p":1497},{"i":1507,"t":"未知，可能是由Windows休眠模式导致","s":"一、发生原因","u":"/en/docs/Others/Linux/问题解决/双系统挂载Windows磁盘为只读文件","h":"#一发生原因","p":1505},{"i":1509,"t":"使用ntfsfix修复ntfs磁盘 安装ntfsfix yay -S ntfsfix 查看问题分区 df -h 修复 sudo ntfsfix /dev/your_partition 重启 reboot","s":"二、解决方案","u":"/en/docs/Others/Linux/问题解决/双系统挂载Windows磁盘为只读文件","h":"#二解决方案","p":1505},{"i":1512,"t":"dock显示的图标是全局图标，程序启动器的desktop文件位于/usr/share/applications中，全局主题中图标主题的程序logo位于~/.local/share/icons/Mkos-Big-Sur-Night/128x128/apps(deppending on specific situation)中。在logo文件夹中挑选想要的logo，在desktop中的icon位置修改即可 应用更新的时候会同时更新.desktop文件，因此在更换图标是最好直接更换在主题文件中替换icon，而不是更改desktop的icon路径 Finder小组件中application title文字不能垂直居中，可以更换为Window title插件","s":"一、latte-dock","u":"/en/docs/Others/Linux/客制化/如何让你的KDE看起来更像macOS","h":"#一latte-dock","p":1510},{"i":1514,"t":"Finder栏中Plasmoids左半部分从左至右依次为： kpple menu application title/window titile(if the text of application title can't be centered vertically) global menu 右半部分从左至右依次为： resources monitor (fork) mcOS BS Inline Battery 网络 Control Center(replace the icon with search icon) Control Center(replace the icom with menu icon) Better Inline Clock 安装方法： plasmpkg2 -u xxx.plasmoid","s":"二、Kde Plasmoids","u":"/en/docs/Others/Linux/客制化/如何让你的KDE看起来更像macOS","h":"#二kde-plasmoids","p":1510},{"i":1517,"t":"树的性质： 一棵 N 个结点的树有 N-1 条边 树的总度数+1=树的结点数 树的度=树中度最大结点的度数 二叉树的性质： 叶子结点数等于度为 2 的结点数加 1，即n0 = n2 + 1 树转化为二叉树： 参考资料：知乎 **加线。**在所有的兄弟结点之间加一条线。 去线。树中的每个结点，只保留它与第一个孩子结点的连线，删除其他孩子结点之间的连线。 调整。每个结点的原来的孩子是结点的左孩子，由原来的兄弟结点转过来的孩子是结点的右孩子。 二叉排序树：每个结点的左子树上的所有结点值都更小，每个结点的右子树上的所有结点的值都更大。 平衡二叉排序树：要么是空树，要么左子树的高度与右子树的高度之差小于等于1。","s":"树","u":"/en/docs/Tui-Mian/计算机基础综合/数据结构","h":"#树","p":1515},{"i":1519,"t":"图的表示： 邻接矩阵 邻接表：每一行表示的是一个顶点所连接的顶点，链表不具有指向性 邻接表的搜索 最小生成树：在连通网的所有生成树中，所有边的代价和最小的生成树，称为最小生成树。 Kruskal算法 Prim算法 最短路径 ​ ​","s":"图","u":"/en/docs/Tui-Mian/计算机基础综合/数据结构","h":"#图","p":1515},{"i":1521,"t":"tip 大数除法是指被除数大小超出long long范围，而导致必须使用字符串存储的除法，属于简单模拟的范畴","s":"大数除法","u":"/en/docs/Tui-Mian/机试/大数除法","h":"","p":1520},{"i":1523,"t":"通过模拟列竖式手动计算除法，实现使用字符串存储被除数的大数除法","s":"思路","u":"/en/docs/Tui-Mian/机试/大数除法","h":"#思路","p":1520},{"i":1525,"t":"string division(string s, int divisor) { /* * 通过模拟列竖式手算除法完成字符串存储的大数除法 */ string quotient; // 商 int idx = 0; // 当前处理的数字在原始字符串中的位置 int remainder = 0; // 余数 int temp = 0; while (idx < s.size()) { // 一直循环处理到索引等于长度 temp = remainder * 10 + (s[idx] - '0'); // 当前进行除法运算的temp if (temp >= divisor) { // 如果能除的动，则将当前的商插入quotient，并更新余数 quotient.push_back(temp / divisor + '0'); remainder = temp % divisor; } else { // 除不动时分两种情况 if (!quotient.empty()) { // 商目前不为空，此时按照竖式方法，需要向商中加入0，再接着下一次循环 quotient.push_back('0'); } remainder = temp; // 商目前为空，按照竖式计算方法，只更新余数，商保持为空 } idx++; // 更新索引位置 } if (quotient.empty()) { // 如果一直除不动，循环结束商还为空，则赋值为0字符串 quotient.assign(\"0\"); } return quotient; // 返回商字符串 }","s":"参考代码","u":"/en/docs/Tui-Mian/机试/大数除法","h":"#参考代码","p":1520},{"i":1527,"t":"将大数除法与进制转换相结合。 tip 北京大学机试真题，N诺链接 完整代码如下： #include <bits/stdc++.h> using namespace std; string division(string s, int divisor) { /* * 通过模拟列竖式手算除法完成字符串存储的大数除法 */ string quotient; // 商 int idx = 0; // 当前处理的数字在原始字符串中的位置 int remainder = 0; // 余数 int temp = 0; while (idx < s.size()) { // 一直循环处理到索引等于长度 temp = remainder * 10 + (s[idx] - '0'); // 当前进行除法运算的temp if (temp >= divisor) { // 如果能除的动，则将当前的商插入quotient，并更新余数 quotient.push_back(temp / divisor + '0'); remainder = temp % divisor; } else { // 除不动时分两种情况 if (!quotient.empty()) { // 商目前不为空，此时按照竖式方法，需要向商中加入0，再接着下一次循环 quotient.push_back('0'); } remainder = temp; // 商目前为空，按照竖式计算方法，只更新余数，商保持为空 } idx++; // 更新索引位置 } if (quotient.empty()) { // 如果一直除不动，循环结束商还为空，则赋值为0字符串 quotient.assign(\"0\"); } return quotient; // 返回商字符串 } int main() { string s; while (cin >> s) { vector<int> vec; int len = s.size(); while (s != \"0\") { int remainder = (s[len - 1] - '0') % 2; vec.push_back(remainder); s = division(s, 2); len = s.size(); } if (vec.empty()) { cout << \"0\"; } else { for (auto it = vec.rbegin(); it != vec.rend(); it++) { cout << *it; } } cout << endl; } return 0; }","s":"扩展","u":"/en/docs/Tui-Mian/机试/大数除法","h":"#扩展","p":1520},{"i":1530,"t":"简述大数定理。 大数定理描述了大样本情况下随机变量的均值与其期望值之间的关系。对于独立同分布的随机变量序列，随着样本数量的增加，样本均值会以较高的概率接近其期望值。 简述中心极限定理。 当独立随机变量的数量足够大时，它们的和（或平均值）的分布会逐渐接近一个正态分布。即使原始随机变量不服从正态分布，但当样本容量足够大时，和（或平均值）的分布仍然呈现出正态分布的特征。 什么是全概率公式。 对于事件A而言，假设有一组互斥且穷尽的条件事件B，则事件A的概率等于事件A在每个条件事件下发生的概率与该条件事件发生概率的乘积和。 什么是最大似然估计。 基本思想是在已知观测数据的情况下，通过调整参数的取值，找到使得观测数据出现概率最大的参数值。 大致过程： 构建参数化的概率模型，即构建似然函数，表示在给定参数下观测数据出现的概率 取似然函数的对数，方便计算与优化 最大化似然函数，求解参数的最优值 简述贝叶斯定理。 贝叶斯定理描述了在给定观测数据的条件下，计算事件的后验概率的方法。 P(A∣B)=P(B∣A)∗P(A)P(B)P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}P(A∣B)=P(B)P(B∣A)∗P(A)​ 其中： P(A∣B)P(A|B)P(A∣B)表示在观测到事件 B 发生的条件下，事件 A 发生的概率，称为后验概率 P(B∣A)P(B|A)P(B∣A)表示在事件 A 发生的条件下，事件 B 发生的概率，称为似然； P(A)P(A)P(A)和P(B)P(B)P(B)分别是事件 A 和事件 B 独立发生的先验概率。 优点：它能够将主观先验知识与观测数据相结合，通过不断更新后验概率来进行推断和决策。 P问题、NP问题以及NP完全问题 tip P stands for Polynomial 意为多项式 P问题是可以在多项式时间内解决的问题 NP问题是可以在多项式时间内验证解的正确性的问题 NP完全问题是一类特殊的NP问题，没有已知的高效解决算法，并且可以在多项式时间内归约到任何其他的NP问题","s":"面试常考问题","u":"/en/docs/Tui-Mian/数学/概率论","h":"#面试常考问题","p":1528},{"i":1532,"t":"tip 参考链接： 线性代数极简入门 《线性代数》高清教学视频 “惊叹号”系列 宋浩老师","s":"线性代数","u":"/en/docs/Tui-Mian/数学/线性代数","h":"","p":1531},{"i":1534,"t":"**线性相关与线性无关：**向量组中的任一向量都不能被其它向量线性表示，就说向量组线性无关；否则就是线性相关。 **矩阵转置：**将矩阵的行和列互相交换 **矩阵求逆：**对于方阵A，若存在方阵B使得AB=BA=单位方阵I，则方阵B为方阵A的逆矩阵，记为A−1A^{-1}A−1","s":"一、基础知识","u":"/en/docs/Tui-Mian/数学/线性代数","h":"#一基础知识","p":1531},{"i":1536,"t":"线性代数中的初等行变换。 交换两行 用非零常数乘以某一行 用一行的倍数加到另一行上 如何理解矩阵的秩。 矩阵的秩是指矩阵的列空间（或行空间）的维数，简而言之是矩阵中所有非零行（或列）向量构成的集合所组成的最大线性无关组的向量个数。 tip 宋浩八字：非零子式的最高阶数 任意矩阵的行秩都等于列秩。 矩阵的秩与线性方程组解的关系。 对于n元线性方程组而言： 当系数矩阵的秩等于增广矩阵的秩且秩等于n时，有唯一解 当系数矩阵的秩等于增广矩阵的秩且秩大于n时，有无穷多解 当系数矩阵的秩不等于增广矩阵的秩时，无解 tip 当系数矩阵的秩小于增广矩阵的秩时，说明系数矩阵中的某一列向量（或行向量）可以被其他列向量（或行向量）线性表示，此时该行不能提供额外的线性独立信息 简述向量组线性无关的含义。 含义：若一个向量组是线性无关的，则该向量组中的每个向量都不能表示成其他向量的线性组合。 意义：如果一个向量组线性无关，那么该向量组所张成的空间就是一个最小维度的向量空间，并且该向量空间中的任何向量都可由这些向量线性组合表示。 判定方法：如果一个向量组中的所有向量都不可以由其他向量线性组合得到，则称该向量组为线性无关的。否则，如果存在某个向量可以表示成其他向量的线性组合，则该向量组就不是线性无关的。 解释正定矩阵以及半正定矩阵。 简述特征值的含义。 特征值描述了矩阵在特定方向（特征向量方向）上的缩放因子，特征向量表示矩阵在这个特定方向上的不变性。 简述矩阵分解的物理意义。 矩阵分解是将一个矩阵表示为一些特定形式的矩阵乘积的过程。 矩阵分解的种类以及物理意义： LU分解：将矩阵分解为一个下三角矩阵和一个上三角矩阵的乘积。物理意义包括解线性方程组、计算矩阵的行列式和逆矩阵等。 QR分解：将矩阵分解为一个正交矩阵和一个上三角矩阵的乘积。物理意义包括最小二乘问题、矩阵的特征值计算等。 特征值分解：将矩阵分解为一个特征向量矩阵和一个对角矩阵的乘积。物理意义包括矩阵的幂、指数和对称矩阵的对角化等。 奇异值分解（SVD）：将矩阵分解为一个正交矩阵、一个对角矩阵和一个正交矩阵的乘积。物理意义包括降维、矩阵逼近和图像压缩等。","s":"二、面试常考问题","u":"/en/docs/Tui-Mian/数学/线性代数","h":"#二面试常考问题","p":1531},{"i":1539,"t":"**线性相关与线性无关：**向量组中的任一向量都不能被其它向量线性表示，就说向量组线性无关；否则就是线性相关。 **矩阵转置：**将矩阵的行和列互相交换 **矩阵求逆：**对于方阵A，若存在方阵B使得AB=BA=单位方阵I，则方阵B为方阵A的逆矩阵，记为A−1A^{-1}A−1 线性代数中的初等行变换。 交换两行 用非零常数乘以某一行 用一行的倍数加到另一行上 如何理解矩阵的秩。 矩阵的秩是指矩阵的列空间（或行空间）的维数，简而言之是矩阵中所有非零行（或列）向量构成的集合所组成的最大线性无关组的向量个数。 tip 宋浩八字：非零子式的最高阶数 任意矩阵的行秩都等于列秩。 矩阵的秩与线性方程组解的关系。 对于n元线性方程组而言： 当系数矩阵的秩等于增广矩阵的秩且秩等于n时，有唯一解 当系数矩阵的秩等于增广矩阵的秩且秩大于n时，有无穷多解 当系数矩阵的秩不等于增广矩阵的秩时，无解 tip 当系数矩阵的秩小于增广矩阵的秩时，说明系数矩阵中的某一列向量（或行向量）可以被其他列向量（或行向量）线性表示，此时该行不能提供额外的线性独立信息 简述向量组线性无关的含义。 含义：若一个向量组是线性无关的，则该向量组中的每个向量都不能表示成其他向量的线性组合。 意义：如果一个向量组线性无关，那么该向量组所张成的空间就是一个最小维度的向量空间，并且该向量空间中的任何向量都可由这些向量线性组合表示。 判定方法：如果一个向量组中的所有向量都不可以由其他向量线性组合得到，则称该向量组为线性无关的。否则，如果存在某个向量可以表示成其他向量的线性组合，则该向量组就不是线性无关的。 解释正定矩阵以及半正定矩阵。 简述特征值的含义。 特征值描述了矩阵在特定方向（特征向量方向）上的缩放因子，特征向量表示矩阵在这个特定方向上的不变性。 简述矩阵分解的物理意义。 矩阵分解是将一个矩阵表示为一些特定形式的矩阵乘积的过程。 矩阵分解的种类以及物理意义： LU分解：将矩阵分解为一个下三角矩阵和一个上三角矩阵的乘积。物理意义包括解线性方程组、计算矩阵的行列式和逆矩阵等。 QR分解：将矩阵分解为一个正交矩阵和一个上三角矩阵的乘积。物理意义包括最小二乘问题、矩阵的特征值计算等。 特征值分解：将矩阵分解为一个特征向量矩阵和一个对角矩阵的乘积。物理意义包括矩阵的幂、指数和对称矩阵的对角化等。 奇异值分解（SVD）：将矩阵分解为一个正交矩阵、一个对角矩阵和一个正交矩阵的乘积。物理意义包括降维、矩阵逼近和图像压缩等。","s":"一、线性代数","u":"/en/docs/Tui-Mian/数学/夏令营面试数学部分复习","h":"#一线性代数","p":1537},{"i":1541,"t":"简述大数定理。 大数定理描述了大样本情况下随机变量的均值与其期望值之间的关系。对于独立同分布的随机变量序列，随着样本数量的增加，样本均值会以较高的概率接近其期望值。 简述中心极限定理。 当独立随机变量的数量足够大时，它们的和（或平均值）的分布会逐渐接近一个正态分布。即使原始随机变量不服从正态分布，但当样本容量足够大时，和（或平均值）的分布仍然呈现出正态分布的特征。 什么是全概率公式。 对于事件A而言，假设有一组互斥且穷尽的条件事件B，则事件A的概率等于事件A在每个条件事件下发生的概率与该条件事件发生概率的乘积和。 什么是最大似然估计。 基本思想是在已知观测数据的情况下，通过调整参数的取值，找到使得观测数据出现概率最大的参数值。 大致过程： 构建参数化的概率模型，即构建似然函数，表示在给定参数下观测数据出现的概率 取似然函数的对数，方便计算与优化 最大化似然函数，求解参数的最优值 简述贝叶斯定理。 贝叶斯定理描述了在给定观测数据的条件下，计算事件的后验概率的方法。 P(A∣B)=P(B∣A)∗P(A)P(B)P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}P(A∣B)=P(B)P(B∣A)∗P(A)​ 其中： P(A∣B)P(A|B)P(A∣B)表示在观测到事件 B 发生的条件下，事件 A 发生的概率，称为后验概率 P(B∣A)P(B|A)P(B∣A)表示在事件 A 发生的条件下，事件 B 发生的概率，称为似然； P(A)P(A)P(A)和P(B)P(B)P(B)分别是事件 A 和事件 B 独立发生的先验概率。 优点：它能够将主观先验知识与观测数据相结合，通过不断更新后验概率来进行推断和决策。 P问题、NP问题以及NP完全问题 tip P stands for Polynomial 意为多项式 P问题是可以在多项式时间内解决的问题 NP问题是可以在多项式时间内验证解的正确性的问题 NP完全问题是一类特殊的NP问题，没有已知的高效解决算法，并且可以在多项式时间内归约到任何其他的NP问题","s":"二、概率论","u":"/en/docs/Tui-Mian/数学/夏令营面试数学部分复习","h":"#二概率论","p":1537},{"i":1545,"t":"显著性目标检测Salient Object Detection，相当于语义分割中的二分类任务，只有前景和背景","s":"（一）SOD任务","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#一sod任务","p":1542},{"i":1547,"t":"下图为U-2-Net的整体结构 tip residual [rɪˈzɪdjuəl] 在encoder阶段，每个block之后使用maxpooling下采样两倍 在decoder阶段，每个block之后使用双线性插值上采样两倍 下图为Residual U-block的结构 tip 卷积是如何改变输出的通道数的？ 卷积核的通道数等于输入的通道数，卷积核的个数等于输出的通道数 图片来源知乎 在特征融合阶段，每一层的encoder-decoder输出，使用3x3卷积以及双线性插值上采样到原始分辨率得到该层的特征图，且卷积核的个数为1，输出的feature map通道数也为1。将每一层的feature map进行concat拼接，得到6通道的融合feature map，最后使用1x1卷积以及sigmoid激活函数得到最终的融合特征图输出","s":"（二）网络结构","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二网络结构","p":1542},{"i":1549,"t":"损失函数是7个损失项的加权求和 共有6层encoder-decoder结构，将每一层对应的feature map与ground truth做BCE Loss得到6个损失项 第7个损失项是最终融合得到的feature map与ground truth的BCE Loss 在论文中，每个损失项的权重都为1 canny边缘检测： 使用高斯滤波进行平滑 计算像素梯度 非极大值抑制 双阈值检测强边缘、弱边缘 边缘连接","s":"（三）损失函数","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#三损失函数","p":1542},{"i":1551,"t":"深度可分离卷积的优点是可以在大致保持卷积效果的情况下减少参数量 在实现原理上可分为两个步骤：深度卷积（depth wise）以及逐点（point wise）卷积 深度卷积是一种在每个输入通道上分别进行卷积操作的卷积方法，每个输入通道只与对应的卷积核进行卷积。 逐点卷积通过使用1×11 \\times 11×1卷积对深度卷积的结果再次卷积","s":"（四）深度可分离卷积","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#四深度可分离卷积","p":1542},{"i":1554,"t":"PR曲线所围成的面积即使该类的AP值","s":"（一）mAP","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#一map","p":1542},{"i":1556,"t":"tip 参考资料：【精读AI论文】YOLO V1目标检测，看我就够了 1.预测阶段​ 下图为YOLOv1的算法框架 下图为YOLOv1的网络结构 输入[448, 448, 3]图像，输出[7, 7, 30]的tensor（包含所有预测框的坐标、置信度和类别结果），通过解析输出的tensor得到预测结果 首先将输入图片划分为S×SS \\times SS×S个grid cell。在YOLOv1中S=7S=7S=7 每个grid cell预测出BBB个bounding box预测框（bbox），每个bbox的中心点都落在该grid cell中。在YOLOv1中B=2B=2B=2 每个bbox包含(x, y, h, w, c)五种信息，其中x, y为bbox左上角坐标，h, w为bbox的宽高，c为该bbox是否存在object的概率 同时每个grid cell预测出一组与数据集有关的条件类别概率。在YOLOv1论文使用的数据集Pascal VOC中，类别种类为20类，因此在预测阶段输出的[7, 7, 30]的tensor含义如下图所示 每个grid cell选出条件类别概率最大的类别，因此每个grid cell只能检测一个物体 tip 这也是YOLOv1小目标和密集目标识别能力差的原因 每个bbox的置信度与其父grid cell的类别概率相乘得到全概率，如下图所示 进行NMS后处理： 对某一特定类别，首先根据全概率置信度排序 将此时最大置信度的bbox与其他所有置信度更小的bbox做IoU判断，若IoU大于设置的阈值，则抹除置信度小的bbox 将剩余的次大的置信度重复步骤2，抹除所有置信度更小的其IoU超过阈值的bbox tip 非极大值抑制只在预测阶段进行 在训练阶段，所有bbox都会在Loss Function中起到更新的作用，因此不进行NMS 2. 训练过程的损失函数​","s":"（二）YOLOv1","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二yolov1","p":1542},{"i":1558,"t":"1. BN层​ 2. 高分辨率训练​ 3. Anchor​ YOLOv2引入了anchor机制代替bbox，将图像划分为13×1313 \\times 1313×13个grid cell，每个grid cell生成5个anchor anchor是通过k-means聚类在数据集上生成的不同尺寸的先验框 对数据集进行anchor宽高比的聚类，聚类数越大，覆盖的IoU越大，但同时模型也更复杂","s":"（二）YOLOv2","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二yolov2","p":1542},{"i":1560,"t":"1. 特征融合​ YOLOv5使用CSPNet实现特征融合，CSP模块由主干和分支构成，主干提取低维特征，分支提取高维特征 主干通过卷积和池化提取特征，形成不同尺寸的特征图 分支将主干输出的特征图作为输入，逐步卷积和上采样提取高级别语义特征 主干特征图通过卷积对通道数降维之后与分支在通道维度上concat tip 在特征提取以及融合阶段可以加入Canny边缘检测得到的特征图进行特征融合 2. 前处理​ 对填充黑色像素进行了改善，以填充更少的黑像素，提高了精度 3. 特征金字塔FCN​","s":"（三）YOLOv5","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#三yolov5","p":1542},{"i":1562,"t":"tip 有关CSP特征融合可以参考：https://blog.csdn.net/weixin_55073640/article/details/122614176 CBAM是通道+空间注意力机制（SENet是通道注意力机制）","s":"三、CBAM","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#三cbam","p":1542},{"i":1564,"t":"通道注意力：原始特征图[b,c,h,w][b, c, h, w][b,c,h,w]经过通道注意力机制算法得到[b,c,1,1][b, c, 1, 1][b,c,1,1]的tensor，代表不同通道之间的重要程度，将其与原始特征图相乘 空间注意力：经过通道注意力的特征图[b,c,h,w][b, c, h, w][b,c,h,w]经过空间注意力机制算法得到[b,1,h,w][b, 1, h, w][b,1,h,w]的tensor，代表宽高维度的像素之间的重要程度，将其与原始特征图相乘","s":"（一）总体结构","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#一总体结构","p":1542},{"i":1566,"t":"原始特征图[b,c,h,w][b, c, h, w][b,c,h,w]分别经过最大池化和平均池化来压缩空间维度、学习通道之间的特征，得到[b,c,1,1][b, c, 1, 1][b,c,1,1]的tensor，再送入共享的多层感知机网络进行降维再升维，最后将二者相加再经过sigmoid函数产生最终的通道注意力特征图","s":"（二）通道注意力","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二通道注意力","p":1542},{"i":1568,"t":"原始特征图[b,c,h,w][b, c, h, w][b,c,h,w]分别经过最大池化和平均池化（通过torch.max和torch.mean函数实现）得到[b,1,h,w][b, 1, h, w][b,1,h,w]的tensor，再将二者concat后通过7×77 \\times 77×7卷积学习特征并降维，最后送入sigmoid函数得到最终的空间注意力特征图","s":"（三）空间注意力","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#三空间注意力","p":1542},{"i":1570,"t":"作者分别对通道注意力以及空间注意力使用最大池化还是平均池化做了消融实验，结果反映二者都用最大池化以及平均池化再相加效果最好（且对于7×77 \\times 77×7卷积与3×33 \\times 33×3卷积的消融实验发现，7×77 \\times 77×7卷积效果更好） 作者对先通道注意力还是先空间注意力做了消融实验，结果发现先通道再空间效果更好","s":"（四）其他注意事项","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#四其他注意事项","p":1542},{"i":1572,"t":"Focal Loss通过引入修正项和样本关注度超参数，增加困难样本的关注度，来解决类别不均衡问题。 YOLO损失函数分为分类损失以及回归损失，可以在分类损失中引入Focal Loss代替原来的交叉熵损失","s":"四、Focal Loss","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#四focal-loss","p":1542},{"i":1574,"t":"Squeeze and Excitation Squeeze挤压操作就是将[b,c,h,w][b, c, h, w][b,c,h,w]的特征图通过池化挤压宽高维度，得到[b,c,1,1][b, c, 1, 1][b,c,1,1]的tensor，该tensor还要经过所示的全连接层-ReLU-全连接层结构 Excitation激励操作就是通过sigmoid函数得到每个通道之间的重要程度系数","s":"五、SENet","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#五senet","p":1542},{"i":1576,"t":"自注意力机制通过计算元素之间的相似度来确定它们之间的关联性，并对其进行加权处理以获得上下文信息。 自注意力机制通过对输入的元素进行线性变换来得到查询（Query）向量、键（Key）向量和值（Value）向量。 通过点积和缩放点积计算相似程度 通过自注意力机制，每个元素都可以通过与其他元素的相似度计算和加权求和，获取到与它们相关的上下文信息。相似度高的元素将获得更高的权重，因此更受到关注和影响，从而建立起元素之间的关联性。","s":"六、自注意力机制","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#六自注意力机制","p":1542},{"i":1579,"t":"This content has been encrypted.","s":"（一）英文自我介绍","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#一英文自我介绍","p":1542},{"i":1581,"t":"1. 英文自我介绍​ This content has been encrypted. 2. 中文自我介绍​ This content has been encrypted.","s":"（二）西电广研院自我介绍","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#二西电广研院自我介绍","p":1542},{"i":1583,"t":"1. 英文自我介绍​ This content has been encrypted. 2. 中文自我介绍​ This content has been encrypted.","s":"（三）电子科技大学自我介绍","u":"/en/docs/Tui-Mian/简历/简历面试准备","h":"#三电子科技大学自我介绍","p":1542},{"i":1585,"t":"tip 欢迎来到笔记本的推免复习部分","s":"Welcome","u":"/en/docs/Tui-Mian/intro","h":"","p":1584},{"i":1587,"t":"如果可以帮到你的话就给个免费的 Star 吧！","s":"支持我！","u":"/en/docs/Tui-Mian/intro","h":"#支持我","p":1584},{"i":1590,"t":"自己在准备夏令营和预推免期间浏览了很多经验贴，按照科协的传统，也支持一下互联网精神，还是给师弟师妹们留下点文字资料吧。 希望可以帮到大家，也感谢那些曾经帮助过我的经验贴和师兄师姐。 文章均为个人经历与想法，不代表任何单位的官方立场，仅供参考。","s":"前言","u":"/en/docs/Tui-Mian/Summary","h":"#前言","p":1588},{"i":1592,"t":"常见名词 含义解释 928/929 每年的公历9月28日（2023年是9月29日），是研招网全国推免系统开放的日子，也是已经拿到offer的推免生上岸的日子。 没offer的也不用着急，因为每个学生只能上一个学校，928/929当天就算手里一堆offer的学生也是只能选择最想去的学校而释放掉其他offer。稳住心态沉住气，一般10月20日推免系统才关闭，这段时间足够联系导师上岸了。 优营 优营就是夏令营的优秀营员。 不同学校优营的效力不同，有些学校的优营直接等于拟录取，928/929当天填报系统即可上岸；但有些学校的优营只能保证在后期的预推免或九推的初筛中不被刷掉，仍需参加后续考核才能获得拟录取资格。 wl/waiting list wl就是递补名单，有些学校的夏令营优营评选中设置递补名单。如果后期有其他优营学生放鸽子不来的话，就会按顺序补录到递补名单中的学生。","s":"扫盲","u":"/en/docs/Tui-Mian/Summary","h":"#扫盲","p":1588},{"i":1594,"t":"本科学校：理工类普通一本（四非） 专业：计算机科学与技术CS rank：前1%（非rank 1 or 2的top选手） 英语：四级594 & 六级586 竞赛：非ACMer，一个CV类比赛国一，其他都是更不上台面的奖项了 奖学金：国家奖学金（夏令营结束之后才拿的，基本没用到）、省人民政府奖学金、CASC奖学金 科研及论文：无正经科研经历，EI水会一篇 从某种程度上来说，本科出身和rank基本上决定了上限。","s":"个人情况","u":"/en/docs/Tui-Mian/Summary","h":"#个人情况","p":1588}],"index":{"version":"2.3.9","fields":["t"],"fieldVectors":[["t/801",[0,3.559,1,8.88]],["t/803",[2,2.937,3,3.646,4,2.42,5,3.646,6,3.646,7,5.149,8,3.478,9,0.624,10,3.339,11,4.565,12,5.82,13,3.478,14,6.015,15,3.44,16,4.546,17,3.856,18,3.856,19,3.219]],["t/805",[7,4.717,9,0.626,12,5.624,14,5.026,15,3.646,16,5.227,20,5.067,21,1.879,22,3.176,23,6.964,24,4.048,25,4.593]],["t/808",[9,0.587,26,6.705,27,8.395]],["t/810",[9,0.57]],["t/812",[0,3.19,21,2.952,28,7.959,29,9.573,30,7.959,31,7.959,32,7.959]],["t/814",[2,2.299,3,2.854,4,1.894,5,2.854,6,2.854,7,4.997,8,2.722,9,0.622,15,3.971,16,2.088,17,4.652,18,4.652,19,2.519,21,2.492,22,3.452,24,2.854,25,3.238,33,2.722,34,3.238,35,6.72,36,6.091,37,3.573,38,5.507,39,6.89,40,3.238,41,3.573,42,3.573]],["t/816",[9,0.582,43,4.018,44,7.444,45,7.444,46,9.191,47,7.444,48,7.444,49,7.444]],["t/818",[0,3.559,9,0.496]],["t/820",[9,0.564,50,6.638]],["t/822",[0,2.983,9,0.557,15,4.604,51,7.444,52,7.444,53,7.444,54,7.444]],["t/824",[2,2.23,3,2.768,4,1.837,5,2.768,6,2.768,7,5.323,8,2.64,9,0.624,15,3.764,19,2.443,21,2.444,55,3.141,56,3.465,57,3.852,58,4.362,59,5.378,60,4.543,61,5.378,62,3.465,63,3.465,64,3.465,65,6.591,66,6.276,67,5.661,68,3.465]],["t/826",[9,0.437,15,3.46,69,7.824,70,7.824,71,7.824,72,7.824,73,7.824,74,7.824,75,7.824]],["t/830",[9,0.588,76,6.498,77,5.89,78,6.498,79,6.986,80,6.498,81,6.498,82,6.498,83,6.498,84,5.89,85,6.498,86,4.301,87,5.89]],["t/832",[79,6.029,88,8.551,89,8.551,90,8.551]],["t/834",[9,0.557,79,5.249,84,6.748,86,4.927,87,6.748,91,7.444,92,6.289,93,7.444,94,7.444,95,7.444]],["t/836",[9,0.57]],["t/838",[9,0.575,22,4.52,79,6.354,96,5.494,97,7.612,98,9.011,99,7.211,100,6.536]],["t/840",[4,2.235,9,0.6,15,0.811,36,1.663,43,2.716,77,1.663,79,4.401,86,1.214,92,1.55,96,2.426,97,4.252,101,2.886,102,1.835,103,1.835,104,1.835,105,3.184,106,1.835,107,1.663,108,1.835,109,1.835,110,1.835,111,1.835,112,3.184,113,5.033,114,1.835,115,1.835,116,3.184,117,4.562,118,5.033,119,3.184,120,1.835,121,1.835,122,1.214,123,1.342,124,1.835,125,1.835,126,3.184,127,3.184,128,1.835,129,1.835,130,3.184,131,1.835,132,2.886,133,1.398,134,1.835,135,1.835,136,0.801,137,1.55,138,1.835,139,1.835,140,1.835,141,1.835,142,1.835,143,1.466,144,1.55,145,1.835,146,1.835,147,1.835,148,1.663,149,1.835,150,1.835,151,1.663,152,1.835,153,1.342,154,1.835,155,3.184,156,3.184,157,3.184,158,1.835,159,1.663,160,1.835,161,1.835,162,1.55,163,1.663,164,1.466,165,1.342,166,1.663,167,1.663,168,1.663,169,1.663,170,1.663,171,2.886,172,1.663,173,1.663,174,1.663,175,1.398,176,1.663,177,2.886,178,1.663,179,2.886,180,1.663,181,1.663,182,2.886,183,1.663,184,1.663,185,1.663,186,1.663,187,1.663,188,3.822,189,1.663,190,1.663,191,1.663,192,1.663,193,3.822,194,1.663,195,1.466,196,1.663,197,1.181,198,1.55,199,1.55,200,1.663,201,1.835]],["t/842",[15,3.927,202,8.88]],["t/844",[2,4.811,4,0.83,7,1.834,8,1.194,9,0.611,39,1.323,43,2.778,79,1.954,96,1.194,101,2.512,199,3.148,203,1.566,204,1.566,205,3.727,206,2.771,207,2.771,208,2.771,209,2.771,210,2.771,211,6.55,212,2.771,213,2.771,214,2.771,215,2.771,216,2.771,217,4.503,218,2.771,219,2.771,220,2.771,221,2.771,222,2.771,223,2.771,224,3.072,225,2.771,226,2.771,227,2.771,228,2.771,229,5.689,230,2.771,231,1.783,232,2.771,233,2.512,234,3.727,235,1.566,236,1.566,237,1.037,238,1.566,239,1.566,240,1.566,241,1.566,242,1.105,243,1.566,244,1.566,245,1.566,246,1.566,247,1.566,248,1.566,249,1.566,250,1.566,251,1.251,252,1.566,253,1.566,254,1.566,255,1.566,256,1.566,257,1.566,258,1.566,259,1.566,260,1.566,261,1.566,262,1.566,263,1.566,264,1.566,265,1.566,266,1.566,267,1.566,268,1.566,269,1.566,270,1.194,271,1.566,272,1.566,273,1.566,274,1.566,275,1.566,276,1.146,277,1.566,278,1.566,279,1.566,280,1.566,281,1.566,282,1.566,283,1.566,284,1.566,285,1.566,286,1.566,287,1.566,288,1.566,289,1.566,290,2.771,291,1.566,292,1.566,293,1.566,294,1.566,295,1.566,296,1.42,297,1.566,298,1.42,299,1.566,300,1.566,301,1.42,302,1.566]],["t/846",[4,1.894,9,0.584,43,1.928,79,2.519,86,3.645,96,2.722,97,6.379,132,3.238,162,3.018,163,3.238,164,2.854,165,2.613,166,3.238,167,3.238,168,3.238,169,3.238,170,3.238,171,4.992,172,3.238,173,3.238,174,3.238,175,2.722,176,3.238,177,4.992,178,3.238,179,4.992,180,3.238,181,3.238,182,4.992,183,3.238,184,3.238,185,3.238,186,3.238,187,3.238,188,6.091,189,3.238,190,3.238,191,3.238,192,3.238,193,6.091,194,3.238,195,2.854,196,3.238,197,2.299,198,3.018,199,3.018,200,3.238,303,3.573,304,3.018,305,3.573,306,3.573,307,2.519]],["t/850",[9,0.627]],["t/852",[9,0.609]],["t/854",[9,0.506]],["t/857",[22,5.761,308,8.331,309,7.444,310,6.289,311,7.444,312,6.289,313,6.748,314,6.748,315,7.444,316,7.444]],["t/859",[22,4.904,310,6.609,312,6.609,317,7.824,318,7.092,319,7.567,320,7.824,321,7.824]],["t/861",[9,0.513,15,2.757,21,2.312,251,4.979,308,5.651,310,5.267,312,5.267,313,5.651,314,7.439,318,5.651,319,4.979,322,6.234,323,6.234,324,8.207,325,6.234,326,3.643,327,6.234,328,6.234,329,6.234,330,6.234]],["t/863",[9,0.506]],["t/867",[0,3.702,9,0.583,21,2.344,331,8.885,332,6.32,333,6.32,334,6.32,335,6.32,336,6.32,337,6.32,338,6.32,339,6.32,340,6.32]],["t/871",[9,0.564,341,8.712]],["t/874",[0,3.19,9,0.535,342,8.673,343,7.959,344,7.959]],["t/876",[0,3.559,345,8.88]],["t/879",[0,3.559,9,0.496]],["t/881",[346,8.625]],["t/883",[9,0.623,347,9.475]],["t/886",[0,3.427,9,0.591]],["t/888",[9,0.591,348,7.75]],["t/891",[9,0.609]],["t/893",[9,0.596,331,8.7,349,3.855,350,5.991,351,5.991,352,5.43,353,5.991,354,5.991,355,5.43,356,5.991,357,5.991,358,5.43,359,5.991,360,5.991,361,5.991,362,5.991,363,5.991]],["t/896",[9,0.623]],["t/898",[0,2.901,9,0.633,22,5.23,349,3.334,352,6.562,355,6.562,358,4.697,364,5.182,365,5.182,366,2.797,367,5.516,368,3.535]],["t/900",[9,0.612,22,5.077,367,6.171]],["t/902",[9,0.609,22,6.001,367,6.065]],["t/904",[9,0.609]],["t/906",[0,3.559,9,0.496]],["t/908",[9,0.564,50,6.638]],["t/910",[9,0.478,369,8.551,370,6.515,371,8.551]],["t/913",[326,5.189,372,8.88]],["t/915",[9,0.506]],["t/917",[9,0.639,373,6.173]],["t/919",[9,0.437,136,3.415,326,4.572,370,5.961,373,5.517,374,7.092,375,7.824,376,4.904,377,6.609]],["t/921",[9,0.553,136,4.647,378,4.714,379,8.248,380,7.326,381,4.848]],["t/923",[9,0.587,377,7.091,378,5.402]],["t/925",[9,0.585,382,6.408,383,6.408,384,6.408,385,6.408,386,6.408,387,6.408,388,8.356,389,6.408,390,6.408,391,6.408,392,6.408,393,6.408,394,6.408,395,6.408,396,6.408]],["t/927",[9,0.588,397,6.145,398,3.646,399,6.973,400,6.973,401,5.626,402,6.973]],["t/929",[403,8.208]],["t/932",[398,4.052,404,7.75,405,7.75,406,4.997]],["t/934",[407,6.243,408,6.243,409,6.243,410,6.243,411,6.243,412,6.243,413,6.243,414,6.243,415,6.243,416,6.243,417,6.243,418,6.243,419,6.243,420,4.432,421,4.558,422,6.243,423,6.243]],["t/936",[424,6.392,425,6.858,426,6.858,427,5.335,428,4.627,429,6.858,430,6.858,431,6.858,432,6.858,433,6.858,434,6.858]],["t/938",[435,7.473,436,7.473,437,7.473,438,7.473,439,7.473,440,7.473]],["t/940",[0,1.848,9,0.437,23,3.895,66,3.895,67,3.513,427,6.043,428,5.559,441,3.895,442,4.179,443,4.179,444,2.89,445,4.179,446,4.179,447,2.694,448,4.179,449,4.179,450,3.895,451,4.179,452,2.755,453,5.178,454,4.147,455,4.179,456,4.179,457,6.039,458,4.179,459,4.179,460,4.179,461,4.179,462,4.179,463,4.179,464,4.179,465,4.179,466,2.967,467,4.179,468,4.179,469,4.179]],["t/944",[9,0.628,398,4.239,470,5.06,471,6.295,472,4.593,473,3.573,474,3.573]],["t/946",[475,7.341,476,7.341,477,6.469,478,7.341,479,7.341,480,7.341,481,7.341]],["t/948",[9,0.549,15,3.416,21,2.112,452,3.402,454,3.018,477,4.548,482,5.161,483,5.161,484,5.161,485,5.161,486,5.161,487,5.161,488,5.161,489,5.161,490,5.161,491,5.161,492,5.161,493,3.885,494,5.161,495,4.339,496,3.664,497,5.161,498,5.161,499,5.161,500,5.161,501,5.161]],["t/950",[0,2.604,502,6.498,503,5.89,504,6.498,505,8.433,506,6.498,507,6.498,508,6.498,509,5.89,510,6.498,511,5.49,512,6.498,513,6.498,514,6.498,515,6.498,516,6.498,517,6.498,518,6.498,519,6.498,520,6.498]],["t/953",[9,0.597,521,9.191,522,7.444,523,7.444,524,7.444,525,7.444,526,7.444]],["t/955",[9,0.599,12,5.55,14,5.736,527,6.151,528,6.151,529,6.151,530,6.151,531,6.151,532,6.151,533,6.151,534,6.151,535,6.151,536,6.151,537,6.151,538,6.151,539,6.151]],["t/957",[540,7.473,541,8.244,542,8.244,543,8.244,544,8.244,545,8.244]],["t/961",[9,0.613,381,2.962,546,4.476,547,2.416,548,4.476,549,4.476,550,7.693,551,4.476,552,4.476,553,4.476,554,4.476,555,4.476,556,6.521,557,4.476,558,4.476,559,4.476,560,4.476,561,4.476,562,4.476,563,4.476,564,4.476,565,4.476,566,4.476,567,4.476,568,4.476,569,4.476,570,4.476,571,4.476,572,4.476,573,4.476,574,4.476,575,4.476,576,4.476]],["t/964",[9,0.627]],["t/966",[9,0.564,577,8.712]],["t/968",[9,0.564,578,8.712]],["t/971",[0,2.89,9,0.638]],["t/973",[9,0.628,367,4.687,579,6.872,580,6.151,581,8.135,582,6.151,583,6.151,584,5.196,585,6.151,586,6.151]],["t/975",[9,0.63,21,4.005,547,2.765,587,4.649]],["t/977",[9,0.57]],["t/979",[0,2.174,9,0.54,588,7.474,589,4.584,590,5.426,591,5.426,592,5.426,593,5.426,594,5.426,595,4.918,596,5.426,597,5.426,598,5.426,599,5.426,600,5.426,601,5.426,602,5.426,603,5.426,604,5.426,605,5.426,606,5.426,607,5.426,608,5.426,609,5.426,610,4.918,611,4.918,612,5.426,613,4.584,614,5.426,615,5.426]],["t/981",[9,0.548,595,5.728,610,5.728,611,5.728,613,5.339,616,6.32,617,6.32,618,6.32,619,6.32,620,6.32,621,6.32,622,6.32,623,6.32,624,6.32,625,6.32,626,6.32,627,6.32,628,6.32,629,6.32,630,6.32]],["t/983",[9,0.597,613,7.395,631,6.887,632,6.887,633,6.887,634,6.887,635,6.887,636,6.887,637,6.887,638,6.887,639,6.887]],["t/986",[57,5.291]],["t/990",[9,0.639]],["t/992",[9,0.591,640,8.551]],["t/995",[9,0.546,641,8.244,642,8.244,643,8.244,644,8.244]],["t/997",[9,0.623]],["t/999",[9,0.634,645,6.408,646,6.408,647,6.408,648,5.808,649,6.408,650,5.413,651,5.808]],["t/1001",[9,0.645,57,3.287,648,6.944]],["t/1003",[0,3.491,652,8.712,653,3.452]],["t/1005",[9,0.432,276,4.165,470,2.869,653,2.256,654,7.725,655,5.694,656,5.694,657,4.339,658,4.548,659,5.161,660,5.161,661,3.402,662,5.161,663,5.694,664,5.694,665,5.694,666,4.81,667,4.81,668,4.81,669,5.694,670,5.694,671,5.694,672,5.694,673,5.694,674,5.694,675,2.623,676,4.548,677,5.694,678,5.694]],["t/1007",[9,0.391,657,5.328,658,5.585,679,6.992,680,6.992,681,6.992,682,6.992,683,6.992,684,6.992,685,6.992,686,6.992,687,6.992,688,6.992,689,6.992,690,6.992,691,6.338]],["t/1009",[9,0.397,406,4.149,692,7.1,693,7.1,694,7.1,695,7.1,696,7.1,697,7.1,698,5.998,699,7.1,700,7.1,701,7.1,702,7.1,703,7.1,704,7.1]],["t/1012",[705,9.055]],["t/1014",[398,3.772,653,3.153,668,6.724,706,7.959,707,7.959,708,7.959,709,7.959,710,5.122]],["t/1016",[711,9.055]],["t/1018",[406,4.572,547,4.223,659,7.092,660,7.092,661,4.674,712,7.824,713,7.824,714,7.824,715,7.824]],["t/1020",[675,4.012,716,6.143,717,8.712]],["t/1022",[9,0.407,136,1.781,376,2.557,470,3.068,471,2.557,547,2.202,653,2.413,675,2.805,716,4.294,718,4.079,719,4.531,720,4.079,721,2.125,722,4.079,723,4.079,724,4.64,725,4.079,726,4.079,727,4.079,728,4.079,729,4.079,730,4.079,731,4.079,732,4.079,733,4.079,734,4.079,735,3.933,736,7.305,737,4.864,738,5.552,739,5.82,740,3.446,741,2.983,742,2.983,743,4.079,744,3.258,745,4.079,746,4.079,747,1.736,748,4.079,749,2.877,750,3.698]],["t/1024",[493,5.36,496,5.056,747,2.485,749,4.117,751,5.839,752,5.839,753,4.449,754,4.449,755,5.839,756,5.839,757,5.839,758,5.839,759,5.839,760,5.839,761,5.839,762,5.839,763,5.839,764,6.495,765,5.839,766,5.292,767,3.274,768,5.839,769,5.839,770,5.839,771,5.839]],["t/1026",[9,0.45,136,4.048,376,5.5,471,3.042,653,1.923,658,3.877,724,6.135,772,4.854,773,4.854,774,4.854,775,4.854,776,4.854,777,4.854,778,4.854,779,4.854,780,4.854,781,4.854,782,4.854,783,4.854,784,4.854,785,4.854,786,4.1,787,4.854,788,4.854,789,4.854,790,4.854,791,4.854,792,4.854,793,4.854,794,4.854,795,4.854,796,4.854,797,4.854,798,4.854]],["t/1028",[675,4.09,799,8.88]],["t/1030",[0,3.135,406,4.572,470,3.942,666,6.609,667,6.609,668,6.609,800,7.824,801,7.824,802,7.824]],["t/1032",[470,4.474,803,8.88]],["t/1034",[804,9.055]],["t/1036",[276,5.274,658,5.76,675,4.15,805,7.211,806,7.211,807,7.211,808,7.211,809,7.211,810,7.211,811,7.211,812,7.211,813,7.211,814,7.211]],["t/1038",[9,0.561,136,3.303,653,2.998,815,5.819,816,5.765,817,5.335,818,5.162,819,6.392]],["t/1041",[224,5.43,815,6.001,820,7.646,821,6.724,822,7.214,823,7.214]],["t/1043",[9,0.506,43,4.352,820,4.848,824,6.812,825,5.502,826,7.352,827,6.9,828,5.502,829,5.502,830,5.502,831,6.812,832,5.502,833,5.502,834,5.502,835,5.502,836,5.502]],["t/1045",[9,0.592,837,5.961,838,5.961,839,7.092,840,7.092,841,7.092]],["t/1049",[9,0.624,721,3.042,842,6.276,843,5.292,844,6.112,845,7.122,846,7.122,847,5.292,848,5.292,849,5.292]],["t/1051",[7,2.133,9,0.6,21,2.882,721,2.647,764,3.716,844,4.048,850,2.013,851,4.606,852,4.606,853,3.716,854,2.921,855,8.118,856,4.606,857,2.357,858,4.606,859,4.606,860,6.473,861,5.702,862,4.606,863,4.606,864,4.606,865,4.606,866,4.606,867,2.921,868,2.357,869,2.921,870,2.921,871,2.921,872,2.921,873,2.921,874,2.574,875,2.921,876,2.921,877,3.716,878,4.606,879,2.921,880,2.921]],["t/1053",[9,0.61,721,4.314,844,5.329,850,3.281,857,6.755,868,4.622,881,3.776,882,5.728,883,5.728,884,5.339]],["t/1055",[9,0.623,15,1.107,21,0.929,326,2.421,376,1.569,378,2.666,420,1.611,421,1.657,447,1.463,721,4.056,877,5.379,881,2.476,885,2.27,886,2.27,887,2.27,888,2.27,889,2.27,890,2.115,891,3.03,892,2.27,893,2.27,894,7.286,895,5.115,896,4.392,897,1.766,898,2.421,899,2.27,900,3.756,901,3.756,902,2.27,903,2.27,904,2.27,905,3.756,906,2.27,907,2.27,908,2.27,909,2.27,910,1.708,911,2,912,2.27,913,2.27]],["t/1057",[9,0.62,15,1.396,406,1.844,420,3.217,421,3.308,447,1.844,587,1.496,653,1.25,721,1.644,747,1.343,764,2.308,767,1.77,818,3.41,895,4.588,896,3.217,897,2.226,910,2.153,914,2.861,915,2.861,916,2.666,917,5.625,918,2.861,919,2.861,920,2.861,921,5.625,922,2.861,923,2.861,924,2.226,925,2.861,926,2.861,927,2.861,928,4.531,929,1.978,930,2.861,931,2.226,932,2.861,933,2.666,934,1.736,935,2.521,936,2.861,937,2.861,938,4.531,939,3.993,940,2.861,941,2.861,942,2.521,943,2.861]],["t/1059",[9,0.613,10,2.586,16,2.066,58,1.73,373,1.034,378,0.944,444,0.919,496,2.763,653,0.581,747,0.624,753,5.722,754,1.118,815,3.965,837,1.118,838,1.118,881,1.562,891,3.14,896,2.275,898,0.857,929,0.919,944,4.952,945,1.329,946,1.001,947,2.824,948,1.329,949,2.369,950,1.239,951,3.892,952,1.329,953,1.329,954,1.329,955,1.329,956,1.171,957,1.329,958,1.329,959,2.208,960,2.586,961,2.369,962,3.892,963,2.369,964,1.329,965,1.329,966,1.329,967,1.329,968,2.824,969,1.329,970,1.329,971,1.329,972,3.516,973,2.208,974,2.369,975,1.329,976,1.329,977,1.329,978,1.239,979,1.329,980,1.329,981,2.369,982,1.329,983,1.171,984,1.073,985,1.329,986,1.329,987,1.329,988,1.329,989,7.192,990,2.369,991,1.329,992,1.329,993,1.329,994,2.369,995,3.271,996,1.329,997,1.329,998,1.329,999,2.369,1000,2.369,1001,1.329,1002,1.329,1003,1.329,1004,2.369,1005,1.992,1006,2.369,1007,2.369,1008,2.369,1009,1.329,1010,1.329,1011,1.329,1012,4.952,1013,2.369,1014,2.369,1015,2.369,1016,1.329,1017,1.329]],["t/1062",[9,0.627,231,4.569,376,4.45,470,3.577,741,2.857,895,3.981,896,2.514,1018,5.343,1019,7.168,1020,3.541,1021,3.541,1022,4.98,1023,3.541,1024,3.541,1025,3.541,1026,5.343,1027,5.343,1028,4.311,1029,3.12,1030,3.541,1031,3.541,1032,3.541,1033,3.541]],["t/1064",[9,0.631,470,2.766,826,4.184,896,4.849,1029,4.385,1034,4.977,1035,4.977,1036,4.977,1037,8.393,1038,4.977,1039,4.977,1040,4.977]],["t/1067",[9,0.607,398,3.608,406,2.121,447,2.121,470,1.829,721,1.89,877,2.654,881,3.33,895,2.035,939,2.899,1041,2.335,1042,3.289,1043,3.289,1044,3.289,1045,3.289,1046,3.066,1047,3.066,1048,5.052,1049,5.42,1050,3.289,1051,3.066,1052,3.289,1053,3.289,1054,3.289,1055,3.289,1056,3.289,1057,3.289,1058,3.289,1059,3.289,1060,3.289,1061,3.289,1062,3.289,1063,3.289,1064,3.289,1065,3.289,1066,3.289,1067,3.289,1068,5.052,1069,5.052,1070,5.052,1071,3.289,1072,3.289,1073,3.289,1074,3.289,1075,3.289,1076,3.289]],["t/1069",[9,0.627,21,1.632,420,2.832,421,2.913,587,2.086,661,2.629,675,1.241,716,1.9,721,5.084,747,2.741,815,2.759,842,6.421,844,2.832,857,1.971,881,4.552,910,3.003,934,2.42,942,2.152,1077,2.442,1078,5.839,1079,2.442,1080,3.934,1081,4.251,1082,2.442,1083,2.442,1084,2.442,1085,2.442,1086,1.971,1087,2.442]],["t/1071",[9,0.605,33,2.325,136,1.332,137,2.578,153,2.232,197,1.964,420,1.964,474,2.152,661,4.138,719,1.711,721,5.334,747,2.071,815,3.05,844,3.132,868,2.232,881,4.818,911,2.437,924,4.28,929,1.913,934,2.676,935,2.437,1005,2.325,1041,1.964,1080,3.432,1086,3.559,1088,2.766,1089,2.766,1090,2.766,1091,2.082,1092,2.766,1093,2.766,1094,2.766,1095,2.766,1096,2.766,1097,2.766,1098,2.766,1099,2.766,1100,3.559,1101,2.325,1102,2.766,1103,1.913,1104,2.766]],["t/1073",[0,3.559,9,0.496]],["t/1075",[9,0.57]],["t/1077",[1105,7.233]],["t/1079",[9,0.579,470,3.369,473,4.715,474,4.715,724,5.095,1091,4.562,1105,8,1106,6.687,1107,6.687,1108,6.687,1109,6.061,1110,5.341]],["t/1081",[9,0.379,450,5.732,452,4.054,493,4.629,749,4.785,786,5.732,1111,5.17,1112,6.786,1113,6.786,1114,6.786,1115,6.786,1116,6.786,1117,6.786,1118,6.786,1119,6.786,1120,6.786,1121,3.882,1122,3.965]],["t/1083",[9,0.507,749,4.294,750,3.698,766,3.698,1091,2.783,1105,4.864,1110,3.258,1123,4.079,1124,5.552,1125,6.09,1126,4.079,1127,4.079,1128,4.079,1129,4.079,1130,6.09,1131,3.446,1132,4.079,1133,5.145,1134,3.698,1135,2.877,1136,4.079,1137,6.09,1138,4.079,1139,4.079,1140,4.079,1141,3.446,1142,4.079,1143,4.079,1144,4.079,1145,4.079,1146,3.698,1147,4.079,1148,4.079,1149,4.079,1150,4.079,1151,6.09,1152,4.079,1153,4.079,1154,4.079,1155,4.079,1156,4.079,1157,4.079,1158,4.079,1159,4.079,1160,3.108,1161,4.079,1162,4.079,1163,4.079,1164,4.079]],["t/1085",[9,0.616,398,3.169,424,5.649,874,5.341,1105,8,1165,6.687,1166,6.687,1167,6.687]],["t/1087",[934,4.616,1168,8.395,1169,8.395,1170,8.395,1171,7.091]],["t/1089",[136,3.803,719,4.885,741,6.372]],["t/1091",[9,0.496,741,6.495]],["t/1093",[9,0.609]],["t/1095",[9,0.596]],["t/1097",[9,0.506]],["t/1099",[9,0.615,1172,8.511,1173,6.591,1174,6.591,1175,6.591,1176,5.568,1177,4.821,1178,6.591,1179,6.591,1180,6.591,1181,6.591]],["t/1101",[9,0.608,21,1.613,143,6.044,242,6.94,719,3.58,1182,7.502,1183,3.942,1184,3.942,1185,3.314,1186,3.942,1187,3.942,1188,6.386,1189,4.349,1190,4.349,1191,2.799,1192,4.349,1193,4.349,1194,4.349,1195,4.349,1196,4.349,1197,3.942,1198,3.942,1199,4.349,1200,4.349,1201,3.942]],["t/1103",[9,0.605,21,1.568,242,5.249,719,2.371,853,4.575,1182,3.573,1183,3.834,1184,5.67,1185,4.766,1186,3.834,1187,3.834,1191,2.722,1197,5.67,1198,3.834,1201,3.834,1202,4.229,1203,4.229,1204,4.229,1205,4.229,1206,4.229,1207,4.229,1208,4.229,1209,7.456,1210,4.229,1211,3.834,1212,6.255,1213,4.229,1214,4.229,1215,4.229,1216,4.229,1217,4.229,1218,4.229,1219,4.229]],["t/1105",[9,0.578,242,5.711,853,5.923,1182,8.172]],["t/1107",[0,3.491,1220,8.712,1221,8.712]],["t/1109",[827,6.638,831,7.36,1222,6.959]],["t/1112",[9,0.46,148,7.473,1100,6.029,1222,7.809,1223,8.244]],["t/1114",[9,0.6,15,2.582,547,3.151,934,3.211,1222,7.093,1224,5.292,1225,5.839,1226,5.839,1227,8.049,1228,5.839,1229,5.839,1230,5.292,1231,7.122,1232,5.839,1233,5.292,1234,5.839]],["t/1116",[9,0.527,15,3.764,547,3.557,1222,7.529,1224,5.975,1227,7.714,1230,5.975,1231,5.975,1235,6.591,1236,6.591,1237,6.591,1238,6.591,1239,6.591,1240,6.591]],["t/1118",[9,0.445,342,6.724,398,3.772,1241,7.959,1242,7.959,1243,6.357,1244,6.724,1245,7.959]],["t/1120",[4,4.078,398,4.444,929,4.822,1103,4.822,1246,6.145,1247,6.973,1248,6.973,1249,7.693,1250,6.145]],["t/1122",[9,0.457,96,3.193,346,3.54,767,2.35,826,3.193,837,3.193,838,3.193,1247,3.799,1251,3.54,1252,6.255,1253,6.213,1254,3.799,1255,4.191,1256,6.213,1257,3.347,1258,4.191,1259,6.213,1260,6.213,1261,4.191,1262,4.191,1263,4.191,1264,6.213,1265,5.27,1266,7.404,1267,4.191,1268,4.191,1269,4.191,1270,4.191,1271,4.191,1272,4.191,1273,3.799,1274,4.191,1275,4.191,1276,6.213,1277,5.249,1278,6.213,1279,3.347,1280,4.191,1281,3.347,1282,4.191,1283,4.191,1284,4.191]],["t/1124",[1285,8.551,1286,8.551,1287,8.551,1288,8.551]],["t/1126",[9,0.452,428,5.916,454,5.128,547,4.371,1289,7.341]],["t/1129",[9,0.632,21,3.039,307,6.277,366,4.804,547,2.705,1290,5.012,1291,5.012,1292,4.234,1293,5.012,1294,5.012,1295,4.543,1296,5.012]],["t/1131",[0,3.19,9,0.535,1297,8.087,1298,7.959,1299,7.959,1300,7.959]],["t/1134",[19,3.826,197,4.81,428,5.635,454,5.295,493,5.099,496,3.491,503,4.918,737,4.334,1101,4.134,1111,5.695,1301,7.474,1302,5.426,1303,5.426,1304,5.426,1305,4.584,1306,3.968,1307,7.474,1308,4.134,1309,4.584,1310,5.426,1311,5.426,1312,5.426]],["t/1136",[1313,8.208]],["t/1138",[4,0.877,9,0.576,15,4.045,21,3.45,86,0.588,197,2.207,198,0.751,224,1.129,231,0.572,270,0.677,298,4.835,301,0.806,346,1.398,349,1.065,366,3.824,378,0.572,406,0.967,428,2.097,444,1.038,454,1.818,495,2.217,496,3.783,547,1.851,587,1.845,735,0.893,767,0.498,1049,1.322,1111,4.663,1254,0.806,1257,1.322,1277,0.751,1308,0.677,1313,0.806,1314,0.806,1315,0.751,1316,1.5,1317,0.806,1318,0.889,1319,1.962,1320,1.655,1321,1.655,1322,4.246,1323,0.889,1324,0.889,1325,0.889,1326,0.889,1327,0.751,1328,2.897,1329,0.889,1330,1.655,1331,0.806,1332,1.655,1333,0.889,1334,0.806,1335,0.889,1336,0.889,1337,0.751,1338,0.889,1339,0.889,1340,5.093,1341,5.531,1342,2.909,1343,2.909,1344,2.909,1345,1.398,1346,2.909,1347,4.246,1348,1.655,1349,1.655,1350,1.655,1351,0.889,1352,0.889,1353,0.889,1354,1.655,1355,2.909,1356,1.655,1357,1.5,1358,3.893,1359,1.655,1360,1.5,1361,1.655,1362,1.855,1363,1.655,1364,0.889,1365,1.655,1366,1.655,1367,1.655,1368,1.655,1369,3.64,1370,3.288,1371,2.909,1372,1.655,1373,1.655,1374,0.889,1375,0.889,1376,1.655,1377,0.889,1378,0.889,1379,0.889,1380,1.655,1381,0.889,1382,0.71,1383,0.889,1384,0.889,1385,0.889,1386,1.655,1387,0.889,1388,0.806,1389,0.806,1390,0.889,1391,1.398,1392,0.806,1393,0.889,1394,0.889,1395,0.889,1396,0.889,1397,0.677,1398,0.677,1399,0.889,1400,0.71,1401,0.751,1402,0.677,1403,0.889,1404,0.806]],["t/1140",[9,0.541,427,4.93,428,4.276,454,3.706,493,4.77,1370,5.907,1405,6.992,1406,6.992,1407,6.992,1408,6.992,1409,6.992,1410,6.992,1411,6.992,1412,6.992]],["t/1142",[428,5.328,454,4.618,1413,8.712]],["t/1144",[9,0.59,398,2.915,427,4.337,428,4.975,453,4.071,454,4.312,547,3.32,584,5.196,1049,6.498,1305,5.196,1314,5.575,1315,5.196,1316,7.374,1317,5.575,1414,5.196]],["t/1146",[9,0.573,584,6.724,1414,6.724,1415,7.959,1416,4.01,1417,7.959]],["t/1148",[9,0.62,242,4.987,1315,5.975,1357,6.411,1369,4.234,1370,4.234,1416,4.129,1418,5.012,1419,5.012,1420,6.411,1421,5.012,1422,5.012,1423,5.012,1424,5.012,1425,5.012,1426,5.975,1427,5.012,1428,5.012,1429,5.012]],["t/1150",[427,6.029,453,5.659,454,4.532,1420,7.75]],["t/1152",[9,0.569,398,3.313,1416,4.883,1430,6.992,1431,6.992,1432,8.838,1433,8.838,1434,6.992,1435,6.992]],["t/1154",[0,3.135,9,0.569,454,4.147,471,4.904,1436,7.824,1437,7.824,1438,5.517]],["t/1156",[0,2.983,368,6.27,898,4.35,1416,3.751,1439,7.444,1440,7.444,1441,7.444,1442,7.444,1443,7.444,1444,7.444,1445,7.444]],["t/1158",[9,0.508,349,4.714,366,4.911,1416,4.988,1446,7.326,1447,7.326,1448,7.326,1449,7.326,1450,7.326]],["t/1160",[9,0.423,366,5.01,1416,5.06,1451,7.567,1452,7.567,1453,7.567,1454,7.567,1455,7.567]],["t/1162",[0,2.802,9,0.494,21,2.593,349,4.499,366,3.774,898,4.086,1416,5.13,1456,6.338,1457,6.992,1458,6.992,1459,6.338,1460,6.992]],["t/1164",[9,0.561,136,3.303,653,2.998,815,5.819,816,5.765,817,5.335,818,5.162,819,6.392]],["t/1167",[224,5.43,815,6.001,820,7.646,821,6.724,822,7.214,823,7.214]],["t/1169",[9,0.506,43,4.352,820,4.848,824,6.812,825,5.502,826,7.352,827,6.9,828,5.502,829,5.502,830,5.502,831,6.812,832,5.502,833,5.502,834,5.502,835,5.502,836,5.502]],["t/1171",[9,0.592,837,5.961,838,5.961,839,7.092,840,7.092,841,7.092]],["t/1175",[9,0.624,721,3.042,842,6.276,843,5.292,844,6.112,845,7.122,846,7.122,847,5.292,848,5.292,849,5.292]],["t/1177",[7,2.133,9,0.6,21,2.882,721,2.647,764,3.716,844,4.048,850,2.013,851,4.606,852,4.606,853,3.716,854,2.921,855,8.118,856,4.606,857,2.357,858,4.606,859,4.606,860,6.473,861,5.702,862,4.606,863,4.606,864,4.606,865,4.606,866,4.606,867,2.921,868,2.357,869,2.921,870,2.921,871,2.921,872,2.921,873,2.921,874,2.574,875,2.921,876,2.921,877,3.716,878,4.606,879,2.921,880,2.921]],["t/1179",[9,0.61,721,4.314,844,5.329,850,3.281,857,6.755,868,4.622,881,3.776,882,5.728,883,5.728,884,5.339]],["t/1181",[9,0.623,15,1.107,21,0.929,326,2.421,376,1.569,378,2.666,420,1.611,421,1.657,447,1.463,721,4.056,877,5.379,881,2.476,885,2.27,886,2.27,887,2.27,888,2.27,889,2.27,890,2.115,891,3.03,892,2.27,893,2.27,894,7.286,895,5.115,896,4.392,897,1.766,898,2.421,899,2.27,900,3.756,901,3.756,902,2.27,903,2.27,904,2.27,905,3.756,906,2.27,907,2.27,908,2.27,909,2.27,910,1.708,911,2,912,2.27,913,2.27]],["t/1183",[9,0.62,15,1.396,406,1.844,420,3.217,421,3.308,447,1.844,587,1.496,653,1.25,721,1.644,747,1.343,764,2.308,767,1.77,818,3.41,895,4.588,896,3.217,897,2.226,910,2.153,914,2.861,915,2.861,916,2.666,917,5.625,918,2.861,919,2.861,920,2.861,921,5.625,922,2.861,923,2.861,924,2.226,925,2.861,926,2.861,927,2.861,928,4.531,929,1.978,930,2.861,931,2.226,932,2.861,933,2.666,934,1.736,935,2.521,936,2.861,937,2.861,938,4.531,939,3.993,940,2.861,941,2.861,942,2.521,943,2.861]],["t/1185",[9,0.613,10,2.586,16,2.066,58,1.73,373,1.034,378,0.944,444,0.919,496,2.763,653,0.581,747,0.624,753,5.722,754,1.118,815,3.965,837,1.118,838,1.118,881,1.562,891,3.14,896,2.275,898,0.857,929,0.919,944,4.952,945,1.329,946,1.001,947,2.824,948,1.329,949,2.369,950,1.239,951,3.892,952,1.329,953,1.329,954,1.329,955,1.329,956,1.171,957,1.329,958,1.329,959,2.208,960,2.586,961,2.369,962,3.892,963,2.369,964,1.329,965,1.329,966,1.329,967,1.329,968,2.824,969,1.329,970,1.329,971,1.329,972,3.516,973,2.208,974,2.369,975,1.329,976,1.329,977,1.329,978,1.239,979,1.329,980,1.329,981,2.369,982,1.329,983,1.171,984,1.073,985,1.329,986,1.329,987,1.329,988,1.329,989,7.192,990,2.369,991,1.329,992,1.329,993,1.329,994,2.369,995,3.271,996,1.329,997,1.329,998,1.329,999,2.369,1000,2.369,1001,1.329,1002,1.329,1003,1.329,1004,2.369,1005,1.992,1006,2.369,1007,2.369,1008,2.369,1009,1.329,1010,1.329,1011,1.329,1012,4.952,1013,2.369,1014,2.369,1015,2.369,1016,1.329,1017,1.329]],["t/1188",[9,0.627,231,4.569,376,4.45,470,3.577,741,2.857,895,3.981,896,2.514,1018,5.343,1019,7.168,1020,3.541,1021,3.541,1022,4.98,1023,3.541,1024,3.541,1025,3.541,1026,5.343,1027,5.343,1028,4.311,1029,3.12,1030,3.541,1031,3.541,1032,3.541,1033,3.541]],["t/1190",[9,0.631,470,2.766,826,4.184,896,4.849,1029,4.385,1034,4.977,1035,4.977,1036,4.977,1037,8.393,1038,4.977,1039,4.977,1040,4.977]],["t/1193",[9,0.607,398,3.608,406,2.121,447,2.121,470,1.829,721,1.89,877,2.654,881,3.33,895,2.035,939,2.899,1041,2.335,1042,3.289,1043,3.289,1044,3.289,1045,3.289,1046,3.066,1047,3.066,1048,5.052,1049,5.42,1050,3.289,1051,3.066,1052,3.289,1053,3.289,1054,3.289,1055,3.289,1056,3.289,1057,3.289,1058,3.289,1059,3.289,1060,3.289,1061,3.289,1062,3.289,1063,3.289,1064,3.289,1065,3.289,1066,3.289,1067,3.289,1068,5.052,1069,5.052,1070,5.052,1071,3.289,1072,3.289,1073,3.289,1074,3.289,1075,3.289,1076,3.289]],["t/1195",[9,0.627,21,1.632,420,2.832,421,2.913,587,2.086,661,2.629,675,1.241,716,1.9,721,5.084,747,2.741,815,2.759,842,6.421,844,2.832,857,1.971,881,4.552,910,3.003,934,2.42,942,2.152,1077,2.442,1078,5.839,1079,2.442,1080,3.934,1081,4.251,1082,2.442,1083,2.442,1084,2.442,1085,2.442,1086,1.971,1087,2.442]],["t/1197",[9,0.605,33,2.325,136,1.332,137,2.578,153,2.232,197,1.964,420,1.964,474,2.152,661,4.138,719,1.711,721,5.334,747,2.071,815,3.05,844,3.132,868,2.232,881,4.818,911,2.437,924,4.28,929,1.913,934,2.676,935,2.437,1005,2.325,1041,1.964,1080,3.432,1086,3.559,1088,2.766,1089,2.766,1090,2.766,1091,2.082,1092,2.766,1093,2.766,1094,2.766,1095,2.766,1096,2.766,1097,2.766,1098,2.766,1099,2.766,1100,3.559,1101,2.325,1102,2.766,1103,1.913,1104,2.766]],["t/1201",[9,0.54,12,5.099,15,2.399,16,3.171,21,2.772,58,3.591,123,5.466,326,3.171,547,4.034,1398,4.134,1400,4.334,1461,5.426,1462,5.426,1463,5.426,1464,5.426,1465,5.426,1466,4.918,1467,6.314,1468,5.426,1469,5.426,1470,4.134,1471,4.334,1472,3.968,1473,4.918,1474,5.426,1475,5.426]],["t/1203",[9,0.573,12,6.531,15,4.233,16,2.778,21,1.763,123,4.981,547,3.676,719,2.665,1209,6.173,1398,3.622,1400,3.797,1466,4.309,1471,3.797,1472,3.476,1473,4.309,1476,4.753,1477,4.309,1478,6.811,1479,6.811,1480,4.753,1481,4.753,1482,4.753,1483,4.753,1484,4.753]],["t/1206",[9,0.545,14,5.006,1477,6.435,1485,7.1,1486,7.1,1487,7.1,1488,8.923,1489,7.1,1490,7.1,1491,7.1,1492,7.1,1493,7.1]],["t/1208",[9,0.6,13,2.827,21,1.787,197,2.387,326,4.848,366,3.059,466,1.412,547,1.184,587,1.758,589,1.853,956,3.85,1122,2.816,1360,1.988,1426,4.788,1471,1.752,1494,2.194,1495,2.194,1496,2.194,1497,1.853,1498,3.71,1499,2.194,1500,2.194,1501,2.194,1502,2.194,1503,2.194,1504,2.194,1505,2.194,1506,2.194,1507,5.668,1508,2.194,1509,2.194,1510,2.194,1511,2.194,1512,2.194,1513,2.194,1514,2.194,1515,2.194,1516,2.194,1517,2.194,1518,2.194,1519,2.194,1520,2.194,1521,2.194,1522,2.194,1523,2.194,1524,2.194,1525,2.194,1526,3.71,1527,2.194,1528,2.194,1529,2.194,1530,2.194,1531,2.194,1532,2.194,1533,2.194,1534,2.194,1535,2.194,1536,2.194,1537,2.194,1538,3.71,1539,2.194,1540,2.194,1541,2.194,1542,2.194,1543,2.194,1544,2.194,1545,2.194,1546,2.194,1547,2.194,1548,2.194,1549,2.194,1550,1.988,1551,3.71,1552,2.194,1553,3.71,1554,2.827,1555,2.194,1556,5.668,1557,3.134,1558,2.194,1559,3.71,1560,2.194,1561,2.194,1562,2.194,1563,2.194]],["t/1210",[0,3.559,9,0.496]],["t/1212",[1086,6.253,1564,8.551,1565,8.551,1566,8.551]],["t/1214",[57,3.744,326,5.434,895,3.593,1567,5.808,1568,8.356,1569,7.574,1570,6.408,1571,5.808,1572,6.408,1573,5.808,1574,5.808,1575,6.408,1576,6.674,1577,6.408,1578,6.408,1579,6.408,1580,5.808]],["t/1216",[9,0.557,57,4.101,895,5.242,1571,4.494,1573,6.362,1574,6.362,1581,4.958,1582,4.958,1583,7.019,1584,4.958,1585,7.384,1586,4.958,1587,4.958,1588,4.189,1589,7.019,1590,4.958,1591,4.958,1592,7.019,1593,4.958,1594,4.958,1595,4.958,1596,4.494,1597,4.958,1598,4.958,1599,4.958]],["t/1219",[9,0.469,1600,8.395,1601,8.395,1602,8.395,1603,8.395]],["t/1221",[1604,6.959,1605,7.897,1606,8.712]],["t/1223",[9,0.578,1265,5.212,1605,7.341,1607,8.099,1608,7.341]],["t/1226",[4,3.994,242,5.314,466,4.849,653,2.985,767,3.078,1101,4.184,1103,3.442,1252,4.638,1281,6.019,1416,3.797,1609,8.393,1610,5.491,1611,5.491,1612,4.385,1613,4.184,1614,5.491,1615,5.491,1616,5.491,1617,5.491,1618,4.987,1619,5.491,1620,6.019,1621,5.491]],["t/1228",[4,4.37,107,4.593,144,4.281,195,4.048,242,3.573,276,3.706,466,3.261,737,4.048,767,2.841,1101,6.282,1281,4.048,1327,6.964,1337,4.281,1416,2.553,1609,8.108,1613,3.861,1618,4.717,1620,5.693,1622,7.127,1623,5.067,1624,5.067,1625,5.067,1626,4.281,1627,5.067,1628,3.573,1629,5.067,1630,5.067,1631,5.067,1632,5.067]],["t/1230",[4,3.706,653,2.77,767,4.955,1250,5.585,1281,5.585,1327,8.186,1337,5.907,1620,7.059,1633,6.992,1634,6.992,1635,6.992,1636,6.338]],["t/1233",[1416,4.926,1637,8.244,1638,8.244,1639,8.244,1640,7.473]],["t/1235",[9,0.527,237,5.92,895,2.841,1416,5.474,1640,4.593,1641,5.067,1642,5.067,1643,5.067,1644,7.127,1645,5.067,1646,5.067,1647,5.067,1648,5.067,1649,5.067,1650,5.067,1651,5.067,1652,4.593,1653,4.593,1654,5.067,1655,5.067,1656,5.067]],["t/1238",[9,0.609]],["t/1240",[9,0.569,587,3.313,736,5.907,1657,6.992,1658,6.992,1659,6.992,1660,6.992,1661,6.992,1662,6.992,1663,6.992,1664,6.992,1665,6.992,1666,6.992]],["t/1242",[9,0.561,1652,6.858,1653,6.858,1667,7.567,1668,7.567,1669,7.567,1670,7.567,1671,7.567,1672,7.567]],["t/1245",[1673,8.712,1674,8.712,1675,8.712]],["t/1247",[9,0.487,540,7.897,1470,6.638]],["t/1249",[1676,9.055]],["t/1251",[9,0.545,15,1.727,21,1.449,307,4.157,366,3.832,368,5.79,587,4.022,1328,3.3,1331,3.541,1345,3.3,1557,3.3,1677,3.906,1678,3.906,1679,3.906,1680,3.906,1681,3.906,1682,3.541,1683,5.343,1684,3.541,1685,3.906,1686,3.906,1687,3.906,1688,3.906,1689,3.906,1690,3.541,1691,3.906,1692,5.343,1693,3.906,1694,3.906,1695,3.906,1696,3.906,1697,3.906,1698,3.906,1699,3.906,1700,3.541,1701,3.906,1702,3.906,1703,3.906,1704,3.906,1705,3.541,1706,3.906,1707,3.541,1708,3.906,1709,3.3,1710,3.906,1711,3.906,1712,3.906]],["t/1254",[9,0.57]],["t/1256",[9,0.618]],["t/1258",[9,0.57]],["t/1261",[9,0.506]],["t/1263",[9,0.601,15,3.624,57,4.789,67,3.819,270,3.819,895,4.595,1246,4.003,1580,4.543,1626,4.234,1713,5.012,1714,5.012,1715,5.012,1716,5.012,1717,5.012,1718,5.012,1719,5.012,1720,5.012,1721,5.012,1722,8.195,1723,5.012,1724,5.012,1725,5.012,1726,5.012]],["t/1265",[21,3.231,1727,7.897,1728,8.712]],["t/1268",[368,5.833,1729,8.551,1730,8.551,1731,7.75]],["t/1270",[9,0.571,12,4.385,14,4.532,15,0.602,16,1.43,19,2.867,21,1.934,307,0.961,348,3.02,367,1.038,368,2.273,495,2.538,547,3.063,767,1.372,890,2.814,891,0.996,924,0.961,934,1.346,973,1.151,1041,1.575,1046,5.132,1047,2.067,1051,2.067,1233,1.235,1398,2.538,1400,1.088,1401,1.151,1402,1.038,1470,1.038,1471,1.088,1472,0.996,1550,2.218,1620,1.955,1682,3.685,1684,1.235,1731,1.235,1732,2.447,1733,1.362,1734,1.362,1735,1.362,1736,1.362,1737,1.362,1738,1.362,1739,1.362,1740,1.362,1741,1.362,1742,1.362,1743,2.447,1744,1.362,1745,2.447,1746,1.362,1747,1.362,1748,1.362,1749,1.362,1750,1.362,1751,1.362,1752,2.447,1753,1.362,1754,3.331,1755,2.447,1756,2.447,1757,1.362,1758,1.088,1759,1.362,1760,1.362,1761,1.362,1762,1.362,1763,1.362,1764,1.362,1765,1.362,1766,2.447,1767,2.447,1768,1.362,1769,1.362,1770,1.362,1771,1.362,1772,1.362,1773,2.447,1774,1.362,1775,1.362,1776,3.331,1777,2.447,1778,1.362,1779,2.447,1780,2.447,1781,2.447,1782,2.447,1783,1.362,1784,1.362,1785,3.331,1786,4.066,1787,4.686,1788,1.362,1789,2.447,1790,1.362,1791,1.362,1792,1.362,1793,1.362,1794,1.362,1795,1.038,1796,1.362,1797,1.362,1798,1.362,1799,1.362,1800,1.362,1801,1.362,1802,1.362,1803,1.362,1804,1.362,1805,4.066,1806,1.362,1807,1.362,1808,1.362,1809,1.362,1810,1.362,1811,3.685,1812,1.362,1813,1.362,1814,1.362,1815,1.362,1816,1.362,1817,1.362,1818,1.362,1819,1.362,1820,1.362,1821,1.362,1822,1.362,1823,2.447,1824,2.447,1825,2.447,1826,2.447,1827,2.447,1828,1.362,1829,1.362,1830,1.362,1831,1.362,1832,1.362,1833,1.362,1834,1.362,1835,1.362,1836,1.362,1837,1.235,1838,1.362,1839,1.235,1840,1.362,1841,1.362,1842,1.362]],["t/1272",[15,3.927,1843,8.88]],["t/1275",[9,0.595,13,3.513,15,1.559,21,1.71,43,3.302,57,4.381,60,1.745,159,1.872,197,2.268,307,4.314,349,3.507,366,5.051,368,4.174,587,3.372,884,2.978,924,1.457,1081,1.574,1100,1.511,1345,1.745,1362,1.65,1557,1.745,1690,1.872,1692,3.195,1700,1.872,1705,1.872,1707,1.872,1839,3.195,1844,2.066,1845,2.066,1846,3.525,1847,2.066,1848,2.066,1849,2.066,1850,2.066,1851,2.066,1852,2.066,1853,2.066,1854,2.066,1855,3.525,1856,2.066,1857,2.066,1858,2.066,1859,2.066,1860,2.066,1861,2.066,1862,2.066,1863,2.066,1864,2.066,1865,2.066,1866,2.066,1867,2.066,1868,1.872,1869,2.066,1870,2.066,1871,2.066,1872,2.066,1873,3.525,1874,2.066,1875,3.525,1876,2.066,1877,2.066,1878,2.066,1879,2.066,1880,2.066,1881,2.066,1882,4.611,1883,3.195,1884,3.525,1885,2.066,1886,2.066,1887,2.066,1888,2.066,1889,2.066,1890,2.066,1891,2.066]],["t/1277",[136,3.535,747,3.447,931,5.711,1892,7.341,1893,3.838,1894,5.36,1895,4.371]],["t/1279",[2,2.031,4,3.29,122,3.308,136,3.081,143,3.993,164,2.521,326,2.921,401,2.308,444,3.133,452,1.886,466,2.031,653,3.396,661,1.886,675,2.302,747,2.127,850,2.796,931,3.525,934,2.749,1171,2.666,1279,2.521,1306,2.308,1319,2.666,1341,5.963,1472,2.308,1612,2.521,1628,2.226,1709,4.223,1893,3.878,1895,2.698,1896,2.861,1897,2.861,1898,2.861,1899,2.666,1900,2.861,1901,2.521,1902,4.531,1903,2.521,1904,4.531,1905,2.861,1906,2.861,1907,4.531,1908,2.666,1909,2.405,1910,2.861,1911,2.861,1912,2.521,1913,2.861,1914,2.405,1915,2.861,1916,2.521,1917,2.521,1918,2.521,1919,2.666,1920,2.405,1921,2.861,1922,2.861,1923,2.308,1924,2.861,1925,2.861,1926,2.861,1927,2.521]],["t/1281",[9,0.613,850,3.894,1893,3.417,1895,3.892,1928,6.536,1929,6.536]],["t/1283",[9,0.606,381,5.178,850,3.753,1930,7.092]],["t/1285",[9,0.614,10,2.654,123,2.654,657,5.17,675,2.567,850,1.438,934,3.065,946,2.476,1177,4.076,1191,4.367,1576,2.899,1931,3.289,1932,3.289,1933,3.289,1934,3.289,1935,4.247,1936,2.899,1937,2.899,1938,3.289,1939,3.289,1940,3.289,1941,3.289,1942,4.247,1943,3.289,1944,3.289,1945,3.289,1946,2.899,1947,3.289,1948,3.289,1949,3.289,1950,2.899,1951,3.289,1952,3.289,1953,6.151,1954,3.289,1955,5.052,1956,3.289,1957,3.289,1958,2.654,1959,3.289,1960,2.899]],["t/1287",[9,0.588,698,4.478,1177,3.877,1191,5.435,1936,4.234,1937,4.234,1942,4.039,1946,4.234,1950,4.234,1961,4.805,1962,4.805,1963,4.805,1964,4.805,1965,4.805,1966,4.805,1967,4.805,1968,4.805,1969,4.805,1970,4.805,1971,6.213,1972,4.478,1973,4.805,1974,4.805,1975,4.805,1976,4.805,1977,4.805]],["t/1290",[9,0.623,43,3.662,57,3.965,471,4.253,850,3.435,1978,6.151,1979,5.17,1980,5.17]],["t/1297",[9,0.603,710,4.95,850,3.048,1917,6.145,1980,5.862,1981,6.973]],["t/1299",[9,0.588,397,6.145,398,3.646,399,6.973,400,6.973,401,5.626,402,6.973]],["t/1301",[403,8.208]],["t/1304",[398,4.052,404,7.75,405,7.75,406,4.997]],["t/1306",[407,6.243,408,6.243,409,6.243,410,6.243,411,6.243,412,6.243,413,6.243,414,6.243,415,6.243,416,6.243,417,6.243,418,6.243,419,6.243,420,4.432,421,4.558,422,6.243,423,6.243]],["t/1308",[424,6.392,425,6.858,426,6.858,427,5.335,428,4.627,429,6.858,430,6.858,431,6.858,432,6.858,433,6.858,434,6.858]],["t/1310",[435,7.473,436,7.473,437,7.473,438,7.473,439,7.473,440,7.473]],["t/1312",[0,1.848,9,0.437,23,3.895,66,3.895,67,3.513,427,6.043,428,5.559,441,3.895,442,4.179,443,4.179,444,2.89,445,4.179,446,4.179,447,2.694,448,4.179,449,4.179,450,3.895,451,4.179,452,2.755,453,5.178,454,4.147,455,4.179,456,4.179,457,6.039,458,4.179,459,4.179,460,4.179,461,4.179,462,4.179,463,4.179,464,4.179,465,4.179,466,2.967,467,4.179,468,4.179,469,4.179]],["t/1316",[9,0.628,398,4.239,470,5.06,471,6.295,472,4.593,473,3.573,474,3.573]],["t/1318",[475,7.341,476,7.341,477,6.469,478,7.341,479,7.341,480,7.341,481,7.341]],["t/1320",[9,0.549,15,3.416,21,2.112,452,3.402,454,3.018,477,4.548,482,5.161,483,5.161,484,5.161,485,5.161,486,5.161,487,5.161,488,5.161,489,5.161,490,5.161,491,5.161,492,5.161,493,3.885,494,5.161,495,4.339,496,3.664,497,5.161,498,5.161,499,5.161,500,5.161,501,5.161]],["t/1322",[9,0.524,653,3.048,675,4.659,716,6.613,1982,7.693,1983,7.693]],["t/1324",[2,1.482,122,1.524,133,2.945,136,2.553,233,2.088,276,2.827,326,2.259,376,2.423,379,2.088,381,2.558,447,1.346,454,1.221,466,1.482,496,1.482,653,2.795,675,2.693,719,2.167,724,1.755,742,1.685,744,1.84,747,3.19,767,2.167,1041,1.482,1091,1.571,1109,2.088,1111,1.755,1135,1.624,1160,1.755,1243,1.84,1265,1.482,1273,2.088,1292,1.946,1305,1.946,1306,2.827,1347,3.504,1389,2.088,1392,2.088,1416,1.161,1612,1.84,1613,1.755,1628,1.624,1795,2.945,1868,2.088,1894,1.524,1899,1.946,1909,1.755,1920,2.945,1927,1.84,1984,2.303,1985,2.088,1986,3.865,1987,1.946,1988,3.504,1989,4.94,1990,3.265,1991,3.265,1992,4.456,1993,2.303,1994,2.303,1995,2.303,1996,2.088,1997,2.303,1998,2.303,1999,2.303,2000,3.504,2001,4.219,2002,2.303,2003,2.303,2004,2.088,2005,2.303,2006,2.303,2007,2.303,2008,2.303,2009,2.303,2010,2.088,2011,2.303,2012,3.504,2013,2.303,2014,2.303,2015,1.84,2016,2.303,2017,1.946,2018,2.303,2019,2.303,2020,2.303,2021,2.088,2022,2.088,2023,2.303,2024,1.571,2025,2.303,2026,1.946,2027,2.303,2028,2.303,2029,2.303,2030,1.755,2031,2.827,2032,2.303,2033,2.303,2034,1.84,2035,1.755,2036,2.303,2037,2.303,2038,1.946,2039,2.303,2040,2.303,2041,2.303,2042,2.303]],["t/1327",[9,0.524,397,6.145,653,3.048,675,4.319,1244,6.499,2043,7.693,2044,7.693,2045,7.693]],["t/1330",[9,0.596,57,3.287,2046,5.625,2047,5.625,2048,5.625,2049,5.625,2050,5.625,2051,5.625,2052,5.625,2053,5.625,2054,5.625,2055,7.661,2056,5.625,2057,5.625,2058,5.625,2059,5.625,2060,5.625,2061,5.625,2062,5.625,2063,5.625,2064,5.625,2065,5.625,2066,5.625]],["t/1332",[9,0.559,956,4.003,1191,6.041,2067,5.012,2068,6.411,2069,4.543,2070,4.543,2071,6.411,2072,5.012,2073,4.543,2074,4.543,2075,5.012,2076,5.012,2077,5.012,2078,5.012,2079,5.012,2080,7.072,2081,5.975,2082,7.072,2083,5.012,2084,5.012,2085,5.012,2086,5.012,2087,5.012,2088,5.012,2089,5.012,2090,5.012]],["t/1334",[2068,6.435,2069,6.435,2070,6.435,2071,8.088,2073,6.435,2074,8.088,2091,7.1,2092,7.1,2093,7.1,2094,7.1,2095,7.1,2096,7.1,2097,7.1]],["t/1336",[9,0.613,894,6.145,2098,7.693,2099,7.693,2100,7.693]],["t/1338",[2101,8.208]],["t/1340",[21,0.867,133,1.782,136,3.455,153,1.71,231,1.505,326,1.366,349,1.505,366,1.262,381,2.591,587,1.108,653,3.366,661,1.397,675,1.077,719,2.195,735,1.262,742,4.318,747,3.501,749,1.649,767,2.195,850,2,910,1.595,960,2.863,972,1.505,984,1.71,1028,1.71,1041,1.505,1121,1.338,1122,1.366,1124,2.983,1133,1.975,1135,1.649,1250,3.127,1251,1.975,1265,1.505,1391,1.975,1613,2.983,1758,1.868,1795,1.782,1893,1.108,1894,1.548,1903,1.868,1909,1.782,1912,1.868,1914,1.782,1920,3.847,1923,1.71,1990,1.975,1991,1.975,2024,4.028,2026,1.975,2030,1.782,2034,1.868,2035,1.782,2038,1.975,2102,1.868,2103,2.12,2104,1.868,2105,2.12,2106,2.12,2107,2.12,2108,2.12,2109,1.71,2110,2.12,2111,2.12,2112,2.12,2113,2.12,2114,2.12,2115,2.12,2116,1.595,2117,1.868,2118,2.12,2119,1.868,2120,2.12,2121,2.12,2122,1.782,2123,2.12,2124,2.12,2125,1.548,2126,2.12,2127,1.868,2128,2.12,2129,2.12,2130,2.12,2131,2.12,2132,1.868,2133,1.868,2134,2.12,2135,1.868,2136,2.12,2137,1.782,2138,1.868,2139,1.868,2140,2.12,2141,2.12,2142,1.868,2143,1.311,2144,1.868,2145,1.868,2146,2.12,2147,2.12,2148,2.12,2149,1.868,2150,1.975,2151,2.12,2152,2.12,2153,1.975]],["t/1342",[9,0.627,16,2.072,58,2.346,136,1.547,452,2.118,653,2.649,675,1.633,710,4.839,747,3.2,850,1.405,1122,2.072,1893,2.594,1979,2.701,1989,2.995,2109,2.593,2116,2.419,2125,2.346,2127,2.832,2138,2.832,2139,2.832,2143,4.216,2144,2.832,2145,2.832,2154,4.004,2155,3.213,2156,2.995,2157,2.028,2158,3.213,2159,3.213,2160,2.701,2161,3.213,2162,4.962,2163,3.213]],["t/1345",[9,0.619,406,3.247,470,2.8,471,3.483,747,3.233,850,3.01,1618,3.678,1895,2.999,2125,6.817,2164,3.791,2165,5.037,2166,5.037]],["t/1347",[9,0.623,15,0.956,251,2.928,319,1.727,406,1.264,447,2.142,493,1.475,511,1.827,653,0.857,850,0.857,946,3.831,972,1.392,1176,3.096,1246,1.727,1467,1.827,1470,1.648,1604,1.727,1893,1.025,1960,3.81,2081,4.029,2102,2.928,2109,1.582,2143,2.055,2160,1.648,2167,5.697,2168,4.323,2169,1.827,2170,1.827,2171,1.96,2172,1.96,2173,1.96,2174,4.323,2175,1.96,2176,1.648,2177,1.96,2178,3.322,2179,1.96,2180,1.96,2181,1.96,2182,1.96,2183,1.96,2184,1.96,2185,1.96,2186,1.96,2187,3.322,2188,1.96,2189,1.96,2190,1.96,2191,1.96,2192,1.96,2193,1.96,2194,1.96,2195,1.96,2196,1.96,2197,3.322,2198,1.96,2199,1.96,2200,1.96,2201,1.96,2202,1.96,2203,3.322,2204,3.322,2205,1.827,2206,1.96,2207,1.827,2208,1.96,2209,1.96,2210,1.96,2211,1.96,2212,1.96,2213,1.96,2214,1.96,2215,1.96,2216,1.96,2217,1.96,2218,1.96,2219,1.96,2220,1.96,2221,1.96]],["t/1349",[9,0.374,136,2.919,653,2.649,817,4.715,818,4.562,983,5.341,1028,4.891,1146,6.061,1727,6.061,2150,5.649,2222,6.546,2223,5.095,2224,6.687,2225,6.687,2226,6.687,2227,6.061,2228,6.687,2229,6.687]],["t/1351",[117,5.728,136,2.759,195,5.048,466,4.067,661,4.948,721,3.292,737,5.048,747,3.524,818,4.311,1080,4.456,1131,5.339,2012,5.728,2022,5.728,2230,6.32,2231,6.32,2232,6.32,2233,5.339,2234,6.32,2235,6.32,2236,6.32,2237,6.32]],["t/1354",[0,1.265,9,0.176,136,2.709,370,2.405,373,3.525,377,2.666,378,2.031,466,2.031,473,3.525,474,4.376,509,2.861,653,1.25,661,1.886,747,1.343,767,1.77,816,2.405,817,4.376,916,2.666,978,2.666,984,2.308,1086,2.308,1265,2.031,1397,2.405,1608,2.861,2222,4.729,2227,2.861,2238,3.156,2239,3.156,2240,4.531,2241,3.156,2242,3.156,2243,6.206,2244,3.156,2245,3.156,2246,3.156,2247,3.156,2248,4.999,2249,2.666,2250,3.156,2251,3.156,2252,3.156,2253,3.156,2254,3.156,2255,3.156,2256,2.666,2257,3.156,2258,3.156,2259,3.156,2260,3.156,2261,3.156,2262,3.156,2263,2.666,2264,3.156,2265,3.156,2266,3.156,2267,3.156,2268,3.156,2269,3.156,2270,3.156,2271,3.156,2272,2.861,2273,4.999,2274,4.999,2275,4.999,2276,4.999,2277,3.156,2278,3.156,2279,3.156,2280,4.999,2281,3.156,2282,3.156,2283,3.156,2284,3.156,2285,3.156,2286,3.156,2287,3.156,2288,3.156]],["t/1356",[9,0.437,1103,5.939,2289,7.824,2290,7.824,2291,7.824,2292,7.824,2293,7.824,2294,7.824]],["t/1359",[376,5.985,764,5.796,815,3.707,881,4.735,934,3.252,1103,3.707,2233,4.996,2295,5.914,2296,5.914,2297,5.914,2298,5.914,2299,5.914,2300,5.914,2301,5.914,2302,5.914,2303,5.914,2304,5.914,2305,5.914,2306,5.914,2307,5.914,2308,5.914,2309,5.914,2310,5.914]],["t/1362",[136,3.535,747,3.447,931,5.711,1892,7.341,1893,3.838,1894,5.36,1895,4.371]],["t/1364",[2,2.031,4,3.29,122,3.308,136,3.081,143,3.993,164,2.521,326,2.921,401,2.308,444,3.133,452,1.886,466,2.031,653,3.396,661,1.886,675,2.302,747,2.127,850,2.796,931,3.525,934,2.749,1171,2.666,1279,2.521,1306,2.308,1319,2.666,1341,5.963,1472,2.308,1612,2.521,1628,2.226,1709,4.223,1893,3.878,1895,2.698,1896,2.861,1897,2.861,1898,2.861,1899,2.666,1900,2.861,1901,2.521,1902,4.531,1903,2.521,1904,4.531,1905,2.861,1906,2.861,1907,4.531,1908,2.666,1909,2.405,1910,2.861,1911,2.861,1912,2.521,1913,2.861,1914,2.405,1915,2.861,1916,2.521,1917,2.521,1918,2.521,1919,2.666,1920,2.405,1921,2.861,1922,2.861,1923,2.308,1924,2.861,1925,2.861,1926,2.861,1927,2.521]],["t/1366",[9,0.613,850,3.894,1893,3.417,1895,3.892,1928,6.536,1929,6.536]],["t/1368",[9,0.606,381,5.178,850,3.753,1930,7.092]],["t/1370",[9,0.614,10,2.654,123,2.654,657,5.17,675,2.567,850,1.438,934,3.065,946,2.476,1177,4.076,1191,4.367,1576,2.899,1931,3.289,1932,3.289,1933,3.289,1934,3.289,1935,4.247,1936,2.899,1937,2.899,1938,3.289,1939,3.289,1940,3.289,1941,3.289,1942,4.247,1943,3.289,1944,3.289,1945,3.289,1946,2.899,1947,3.289,1948,3.289,1949,3.289,1950,2.899,1951,3.289,1952,3.289,1953,6.151,1954,3.289,1955,5.052,1956,3.289,1957,3.289,1958,2.654,1959,3.289,1960,2.899]],["t/1372",[9,0.588,698,4.478,1177,3.877,1191,5.435,1936,4.234,1937,4.234,1942,4.039,1946,4.234,1950,4.234,1961,4.805,1962,4.805,1963,4.805,1964,4.805,1965,4.805,1966,4.805,1967,4.805,1968,4.805,1969,4.805,1970,4.805,1971,6.213,1972,4.478,1973,4.805,1974,4.805,1975,4.805,1976,4.805,1977,4.805]],["t/1375",[9,0.623,43,3.662,57,3.965,471,4.253,850,3.435,1978,6.151,1979,5.17,1980,5.17]],["t/1382",[9,0.603,710,4.95,850,3.048,1917,6.145,1980,5.862,1981,6.973]],["t/1384",[653,3.518,1893,4.208]],["t/1386",[9,0.6,946,3.575,1185,5.56,1191,5.402,1958,3.833,2311,5.241,2312,5.241,2313,6.164,2314,5.241,2315,5.241,2316,5.241,2317,5.241,2318,4.427,2319,5.241,2320,5.241,2321,5.241,2322,5.241,2323,5.241,2324,5.241,2325,5.241,2326,5.241,2327,5.241,2328,5.241,2329,5.241]],["t/1389",[9,0.469,2330,8.347,2331,9.881]],["t/1391",[9,0.478,1297,7.223,2330,7.223,2332,7.75]],["t/1393",[9,0.611,666,6.021,1895,3.847,1918,4.048,1988,6.46,2330,6.021,2332,6.46,2333,5.067,2334,3.573,2335,5.067,2336,6.238,2337,6.307,2338,7.127,2339,5.067,2340,5.067]],["t/1395",[9,0.615,587,4.467,2334,4.648,2336,6.238,2337,6.647,2341,6.591]],["t/1397",[9,0.607,136,2.425,370,5.789,373,6.105,374,5.037,378,4.889,1895,2.999,2164,5.183,2336,6.159,2337,6.563,2342,5.557,2343,5.557]],["t/1399",[9,0.617,398,4.67,1895,3.458,2336,4.241,2337,4.518,2344,8.356,2345,6.408,2346,6.408,2347,6.408]],["t/1401",[9,0.624,58,5.793,398,4.148,2348,7.935,2349,6.887]],["t/1403",[9,0.621,58,4.627,398,3.313,2156,8.601,2348,6.338]],["t/1406",[2,2.446,86,1.495,136,3.048,165,1.652,231,1.453,270,1.721,398,2.333,444,1.415,473,1.592,496,1.453,547,1.219,653,2.553,675,1.751,676,1.804,710,3.168,735,3.117,740,3.211,742,1.652,744,1.804,747,2.457,753,2.897,754,1.721,816,2.897,817,2.681,850,0.895,897,1.592,960,1.652,968,1.804,972,1.453,995,2.897,1091,1.541,1103,3.086,1110,1.804,1121,2.817,1122,2.877,1124,1.721,1135,2.681,1141,1.908,1265,1.453,1306,1.652,1309,1.908,1362,1.804,1382,1.804,1397,1.721,1416,1.138,1628,1.592,1893,2.333,1901,1.804,1916,1.804,1923,2.78,2001,1.908,2015,1.804,2017,1.908,2024,3.358,2031,1.652,2035,1.721,2104,1.804,2116,1.541,2117,1.804,2119,1.804,2132,1.804,2133,1.804,2135,1.804,2137,1.721,2142,1.804,2143,2.131,2149,1.804,2157,4.249,2176,1.721,2256,1.908,2350,2.047,2351,2.047,2352,1.804,2353,2.047,2354,2.047,2355,2.047,2356,2.047,2357,2.047,2358,2.047,2359,1.721,2360,2.047,2361,2.047,2362,2.047,2363,2.047,2364,2.047,2365,2.047,2366,2.047,2367,2.047,2368,2.047,2369,2.047,2370,2.047,2371,2.047,2372,2.047,2373,3.358,2374,2.047,2375,2.047,2376,2.047,2377,2.047,2378,2.047,2379,2.047,2380,2.047,2381,2.047,2382,2.047,2383,2.047,2384,2.047,2385,2.047,2386,2.047]],["t/1408",[9,0.625,710,5.642,735,4.732,850,2.256,972,3.664,1121,3.258,1893,2.698,2024,3.885,2143,3.193,2154,4.165,2373,5.981]],["t/1410",[4,2.058,9,0.606,43,1.249,122,2.569,231,1.49,401,1.693,428,3.065,447,2.268,452,2.319,454,3.749,470,1.166,471,1.451,653,0.917,735,1.249,747,3.487,767,1.298,850,3.446,898,2.268,929,3.142,1121,3.357,1122,2.929,1257,1.849,1308,1.764,1438,3.534,1497,1.956,1893,2.78,1894,1.532,1935,1.764,1958,1.693,2122,1.764,2125,2.569,2154,1.693,2157,3.357,2164,3.419,2313,1.956,2318,1.956,2334,1.632,2337,3.534,2387,2.098,2388,2.098,2389,2.098,2390,2.098,2391,2.098,2392,2.098,2393,2.098,2394,2.098,2395,2.098,2396,2.098,2397,2.098,2398,2.098,2399,2.098,2400,2.098,2401,2.098,2402,2.098,2403,2.098,2404,2.098,2405,1.764,2406,2.098,2407,2.098,2408,2.098,2409,2.098]],["t/1413",[9,0.606,398,1.255,453,1.752,579,5.943,735,4.078,850,2.993,895,1.485,898,3.725,1121,3.647,1122,3.725,1185,4.857,1191,1.704,1416,4.02,1618,4.219,1893,1.255,1895,2.342,2143,1.485,2157,1.515,2336,2.872,2352,2.115,2373,1.806,2410,2.4,2411,2.4,2412,2.4,2413,2.4,2414,2.4,2415,2.4,2416,5.778,2417,2.4,2418,3.933,2419,3.933,2420,2.4,2421,2.4,2422,2.4,2423,2.4,2424,2.4,2425,2.4,2426,2.4,2427,3.933,2428,2.4,2429,2.4,2430,2.4,2431,2.4,2432,2.4,2433,2.4,2434,2.4,2435,2.4,2436,2.4,2437,2.4]],["t/1415",[9,0.611,304,3.446,398,2.886,453,2.7,454,2.162,470,2.055,587,1.933,735,2.202,818,2.783,850,3.426,898,4.722,929,5.066,1438,2.877,1618,2.7,1893,1.933,1895,3.287,2157,4.169,2164,2.783,2334,2.877,2336,5.723,2359,3.108,2438,3.698,2439,3.698,2440,3.698,2441,3.698,2442,3.698]],["t/1417",[9,0.629,454,2.573,735,3.731,747,2.942,850,2.739,898,2.836,1121,2.777,1438,3.423,1893,3.816,1895,2.62,2143,2.721,2157,3.955,2373,3.311,2443,4.399,2444,4.399,2445,4.399,2446,4.399,2447,4.399]],["t/1419",[2101,8.208]],["t/1421",[21,0.867,133,1.782,136,3.455,153,1.71,231,1.505,326,1.366,349,1.505,366,1.262,381,2.591,587,1.108,653,3.366,661,1.397,675,1.077,719,2.195,735,1.262,742,4.318,747,3.501,749,1.649,767,2.195,850,2,910,1.595,960,2.863,972,1.505,984,1.71,1028,1.71,1041,1.505,1121,1.338,1122,1.366,1124,2.983,1133,1.975,1135,1.649,1250,3.127,1251,1.975,1265,1.505,1391,1.975,1613,2.983,1758,1.868,1795,1.782,1893,1.108,1894,1.548,1903,1.868,1909,1.782,1912,1.868,1914,1.782,1920,3.847,1923,1.71,1990,1.975,1991,1.975,2024,4.028,2026,1.975,2030,1.782,2034,1.868,2035,1.782,2038,1.975,2102,1.868,2103,2.12,2104,1.868,2105,2.12,2106,2.12,2107,2.12,2108,2.12,2109,1.71,2110,2.12,2111,2.12,2112,2.12,2113,2.12,2114,2.12,2115,2.12,2116,1.595,2117,1.868,2118,2.12,2119,1.868,2120,2.12,2121,2.12,2122,1.782,2123,2.12,2124,2.12,2125,1.548,2126,2.12,2127,1.868,2128,2.12,2129,2.12,2130,2.12,2131,2.12,2132,1.868,2133,1.868,2134,2.12,2135,1.868,2136,2.12,2137,1.782,2138,1.868,2139,1.868,2140,2.12,2141,2.12,2142,1.868,2143,1.311,2144,1.868,2145,1.868,2146,2.12,2147,2.12,2148,2.12,2149,1.868,2150,1.975,2151,2.12,2152,2.12,2153,1.975]],["t/1423",[9,0.626,16,2.088,58,2.365,136,1.559,452,2.135,653,2.662,675,1.645,710,4.859,747,3.213,850,1.415,1122,2.088,1893,2.61,1979,2.722,1989,3.018,2109,2.613,2116,2.437,2125,2.365,2127,2.854,2138,2.854,2139,2.854,2143,4.234,2144,2.854,2145,2.854,2154,4.028,2155,3.238,2156,3.018,2157,2.044,2158,3.238,2159,3.238,2160,2.722,2161,3.238,2162,4.992,2163,3.238]],["t/1426",[9,0.619,406,3.247,470,2.8,471,3.483,747,3.233,850,3.01,1618,3.678,1895,2.999,2125,6.817,2164,3.791,2165,5.037,2166,5.037]],["t/1428",[9,0.624,15,0.952,251,2.916,319,1.719,406,1.258,447,2.133,493,1.468,511,1.818,653,0.853,850,0.853,946,3.82,972,1.385,1176,3.084,1246,1.719,1467,1.818,1470,1.64,1604,1.719,1893,1.02,1960,3.797,2081,4.016,2102,2.916,2109,1.574,2143,2.047,2160,1.64,2167,5.682,2168,4.309,2169,1.818,2170,1.818,2171,1.951,2172,1.951,2173,1.951,2174,4.309,2175,1.951,2176,1.64,2177,1.951,2178,3.309,2179,1.951,2180,1.951,2181,1.951,2182,1.951,2183,1.951,2184,1.951,2185,1.951,2186,1.951,2187,3.309,2188,1.951,2189,1.951,2190,1.951,2191,1.951,2192,1.951,2193,1.951,2194,1.951,2195,1.951,2196,1.951,2197,3.309,2198,1.951,2199,1.951,2200,1.951,2201,1.951,2202,1.951,2203,3.309,2204,3.309,2205,1.818,2206,1.951,2207,1.818,2208,1.951,2209,1.951,2210,1.951,2211,1.951,2212,1.951,2213,1.951,2214,1.951,2215,1.951,2216,1.951,2217,1.951,2218,1.951,2219,1.951,2220,1.951,2221,1.951]],["t/1431",[9,0.535,136,3.474,653,3.153,747,3.387,2153,6.724,2448,7.959,2449,7.959]],["t/1433",[9,0.62,21,0.724,136,1.929,175,2.558,349,1.256,366,1.053,370,1.487,373,3.701,378,1.256,398,1.591,452,4.132,470,0.983,587,0.925,653,0.773,675,0.899,676,1.559,710,2.161,724,1.487,747,1.88,850,3.477,897,1.376,931,1.376,1103,1.223,1121,1.117,1122,1.962,1297,2.837,1618,2.924,1837,1.769,1893,0.925,1895,1.053,1918,1.559,1958,1.427,1979,4.922,1992,1.487,2024,3.58,2031,1.427,2125,2.222,2157,1.921,2160,1.487,2164,2.291,2336,5.412,2337,6.302,2450,1.952,2451,1.952,2452,1.952,2453,1.952,2454,1.952,2455,1.952,2456,3.358,2457,1.952,2458,1.952,2459,1.952,2460,1.952,2461,1.952,2462,1.952]],["t/1435",[9,0.576,398,4.033,653,2.611,675,3.036,719,3.696,738,5.022,739,5.265,2116,4.497,2169,5.568,2463,5.975,2464,6.591,2465,6.591,2466,8.511,2467,6.591]],["t/1437",[9,0.552,136,3.664,2468,8.395,2469,7.609]],["t/1439",[9,0.496,547,4.793]],["t/1441",[9,0.605,12,4.094,13,3.048,15,2.654,16,1.402,21,2.833,175,5.081,197,1.544,307,3.626,349,3.309,366,4.751,368,2.729,495,3.048,547,2.159,587,3.16,1100,1.755,1322,2.175,1398,1.828,1401,2.027,1402,1.828,1459,2.175,1554,4.572,1758,1.916,2470,2.399,2471,2.399,2472,2.399,2473,2.399,2474,2.399,2475,4,2476,2.399,2477,4,2478,4,2479,2.399,2480,2.399,2481,2.399,2482,4,2483,5.143,2484,4,2485,2.399,2486,2.399,2487,4,2488,2.399,2489,2.399,2490,2.399,2491,2.399,2492,2.399,2493,2.399,2494,2.399,2495,2.399,2496,2.399,2497,2.399,2498,2.399,2499,2.399,2500,2.399,2501,4,2502,2.399]],["t/1443",[1100,7.467]],["t/1445",[9,0.608,12,4.971,14,5.138,21,2.702,43,3.933,57,4.258,366,3.933,587,3.453,894,5.82,1883,7.325,2503,4.079,2504,6.09,2505,6.09,2506,6.09,2507,4.079,2508,6.09,2509,6.09,2510,4.079,2511,6.09,2512,4.079]],["t/1447",[9,0.391,43,3.774,237,4.627,891,5.114,895,3.92,2513,6.992,2514,6.992,2515,6.992,2516,6.992,2517,6.992,2518,6.992,2519,6.992,2520,6.992,2521,6.992,2522,6.992,2523,6.992]],["t/1449",[9,0.516,136,3.615,653,3.659,675,4.254,1103,3.961,1160,4.815,1588,5.339,1987,5.339,2222,6.31,2223,4.815,2405,4.815,2524,8.281,2525,6.32,2526,6.32]],["t/1452",[9,0.571,67,5.389,144,4.234,398,4.218,444,3.142,453,3.317,653,2.802,675,4.487,716,3.534,719,2.81,735,2.705,738,3.819,739,4.003,747,3.01,850,1.986,1397,3.819,1980,3.819,2030,3.819,2031,3.666,2116,3.419,2527,4.234,2528,5.012,2529,5.012,2530,5.012]],["t/1454",[4,1.237,9,0.601,10,0.404,13,0.805,15,0.244,16,0.323,40,0.501,43,0.298,57,0.323,60,0.467,92,0.467,122,0.366,123,0.773,133,0.421,136,1.177,151,0.958,162,0.467,165,3.22,175,0.421,197,0.356,276,0.773,326,0.323,342,0.467,366,1.048,368,0.721,376,0.346,381,0.366,441,0.467,444,0.346,447,0.323,452,0.33,473,0.745,474,0.745,653,2.275,657,1.157,661,0.33,662,0.501,675,2.967,691,0.958,716,1.901,719,2.334,721,0.55,736,0.467,741,0.404,747,2.631,749,0.39,767,0.851,786,0.893,817,0.39,821,0.467,824,1.283,850,1.548,874,1.213,933,0.467,934,1.068,946,0.377,950,0.893,959,0.893,984,0.404,995,2.31,1005,0.421,1022,0.467,1041,1.502,1080,0.745,1081,0.421,1091,0.377,1103,0.346,1111,2.054,1131,0.893,1134,0.501,1135,0.39,1160,0.805,1177,0.773,1211,0.958,1243,0.844,1248,0.958,1252,1.283,1265,0.356,1277,0.467,1279,0.441,1289,0.501,1292,0.467,1295,1.761,1308,0.421,1334,0.501,1340,0.958,1369,0.467,1382,0.441,1404,0.501,1414,0.467,1416,2.439,1426,0.467,1567,0.958,1569,2.748,1576,0.441,1585,0.958,1596,0.958,1628,0.39,1636,0.501,1795,0.421,1811,0.501,1894,0.366,1908,0.467,1914,0.421,1919,0.467,1935,0.421,1942,1.778,1971,1.641,1972,0.893,1987,0.467,1992,0.421,2000,1.376,2004,1.376,2015,1.864,2021,0.958,2122,0.421,2137,0.421,2143,0.592,2205,0.467,2207,3.071,2222,0.805,2223,1.157,2240,0.501,2249,0.893,2272,0.501,2359,0.805,2463,3.295,2527,1.283,2531,0.501,2532,1.518,2533,0.553,2534,0.553,2535,0.501,2536,0.553,2537,0.553,2538,0.553,2539,1.057,2540,0.553,2541,0.553,2542,1.057,2543,1.518,2544,0.553,2545,0.553,2546,3.344,2547,0.553,2548,1.057,2549,1.057,2550,0.553,2551,0.553,2552,0.553,2553,0.553,2554,0.553,2555,0.553,2556,0.553,2557,0.553,2558,0.553,2559,0.553,2560,0.553,2561,1.057,2562,1.057,2563,1.942,2564,1.942,2565,0.553,2566,0.553,2567,0.553,2568,0.553,2569,0.553,2570,0.553,2571,0.553,2572,0.553,2573,0.553,2574,0.553,2575,0.553,2576,0.553,2577,0.553,2578,0.553,2579,1.057,2580,0.553,2581,0.553,2582,2.334,2583,2.334,2584,0.553,2585,1.057,2586,2.334,2587,2.334,2588,0.553,2589,0.553,2590,0.553,2591,0.553,2592,1.057,2593,3.032,2594,0.553,2595,0.553,2596,0.553,2597,0.553,2598,0.553,2599,0.553,2600,0.553,2601,0.553,2602,0.553,2603,0.553,2604,0.553,2605,0.553,2606,0.553,2607,0.553,2608,0.553,2609,1.518,2610,0.553,2611,0.553,2612,0.553,2613,0.501,2614,0.553,2615,1.057,2616,0.553,2617,1.057,2618,0.553,2619,0.553,2620,0.553,2621,0.553,2622,0.553,2623,0.553,2624,0.553,2625,0.553,2626,1.057,2627,0.553,2628,1.057,2629,0.553,2630,0.553,2631,0.553,2632,0.553,2633,0.553,2634,1.518,2635,0.553,2636,0.553,2637,0.553,2638,0.553,2639,0.553,2640,1.057,2641,0.553,2642,0.553,2643,0.553,2644,0.553,2645,0.553,2646,0.553,2647,0.553,2648,0.553,2649,0.553,2650,1.518,2651,2.334,2652,1.518,2653,0.553,2654,0.553,2655,1.942,2656,2.696,2657,1.057,2658,0.553,2659,1.518,2660,0.553,2661,0.553,2662,1.057,2663,0.553,2664,0.553,2665,2.696,2666,0.553,2667,0.553,2668,0.553,2669,0.553,2670,1.057,2671,0.553,2672,1.518,2673,0.553,2674,0.553,2675,0.553,2676,1.376,2677,1.518,2678,0.553,2679,0.553,2680,0.553,2681,0.553,2682,0.553,2683,0.553,2684,0.553,2685,1.057,2686,0.553,2687,0.553,2688,1.057,2689,0.553,2690,0.553,2691,0.553,2692,0.553,2693,0.553,2694,0.553]],["t/1457",[86,3.468,136,3.185,376,4.574,653,3.326,675,3.866,719,4.091,1028,3.833,1265,3.372,1894,3.468,1992,3.993,2030,5.56,2031,3.833,2034,4.186,2222,3.993,2223,3.993,2249,4.427,2405,7.878,2527,4.427,2531,6.614,2695,5.241,2696,5.241,2697,5.241]],["t/1459",[4,3.545,21,2.48,165,4.891,397,6.861,653,2.649,675,3.956,747,2.846,983,5.341,1243,5.341,1992,5.095,2223,6.546,2405,6.546,2698,5.095,2699,6.687,2700,6.687]],["t/1461",[9,0.619,122,5.008,1244,6.392,2701,7.567,2702,7.567]],["t/1463",[9,0.564,50,6.638]],["t/1465",[9,0.582,26,6.585,2703,8.244,2704,8.244]],["t/1467",[9,0.615,947,6.798,1985,5.975,2705,8.544,2706,6.591,2707,6.591,2708,6.591,2709,5.975,2710,5.975]],["t/1469",[4,4.477,9,0.51,26,6.746,86,3.508,122,3.508,307,3.738,947,5.875,1388,4.805,1554,4.039,2535,4.805,2705,6.667,2711,5.301,2712,8.446,2713,5.301,2714,5.301,2715,6.667,2716,7.355,2717,8.446,2718,4.805,2719,5.301,2720,5.301,2721,5.301]],["t/1471",[9,0.523,26,6.736,39,5.49,827,4.951,2233,8.37,2709,5.89,2715,8.486,2722,8.433,2723,8.433,2724,5.89,2725,6.498]],["t/1473",[9,0.595,827,6.065,2724,7.214,2726,7.959,2727,7.959]],["t/1475",[9,0.546,366,2.488,1328,3.895,1626,3.895,1683,4.179,2157,4.903,2176,3.513,2698,3.513,2728,4.611,2729,6.663,2730,4.611,2731,4.611,2732,4.611,2733,4.611,2734,4.611,2735,4.611,2736,4.611,2737,4.611,2738,4.611,2739,8.57,2740,8.57,2741,8.57,2742,6.663,2743,4.611,2744,6.663,2745,4.611,2746,4.611,2747,4.611,2748,4.611]],["t/1477",[2749,9.055]],["t/1479",[2,2.446,86,1.495,136,3.048,165,1.652,231,1.453,270,1.721,398,2.333,444,1.415,473,1.592,496,1.453,547,1.219,653,2.553,675,1.751,676,1.804,710,3.168,735,3.117,740,3.211,742,1.652,744,1.804,747,2.457,753,2.897,754,1.721,816,2.897,817,2.681,850,0.895,897,1.592,960,1.652,968,1.804,972,1.453,995,2.897,1091,1.541,1103,3.086,1110,1.804,1121,2.817,1122,2.877,1124,1.721,1135,2.681,1141,1.908,1265,1.453,1306,1.652,1309,1.908,1362,1.804,1382,1.804,1397,1.721,1416,1.138,1628,1.592,1893,2.333,1901,1.804,1916,1.804,1923,2.78,2001,1.908,2015,1.804,2017,1.908,2024,3.358,2031,1.652,2035,1.721,2104,1.804,2116,1.541,2117,1.804,2119,1.804,2132,1.804,2133,1.804,2135,1.804,2137,1.721,2142,1.804,2143,2.131,2149,1.804,2157,4.249,2176,1.721,2256,1.908,2350,2.047,2351,2.047,2352,1.804,2353,2.047,2354,2.047,2355,2.047,2356,2.047,2357,2.047,2358,2.047,2359,1.721,2360,2.047,2361,2.047,2362,2.047,2363,2.047,2364,2.047,2365,2.047,2366,2.047,2367,2.047,2368,2.047,2369,2.047,2370,2.047,2371,2.047,2372,2.047,2373,3.358,2374,2.047,2375,2.047,2376,2.047,2377,2.047,2378,2.047,2379,2.047,2380,2.047,2381,2.047,2382,2.047,2383,2.047,2384,2.047,2385,2.047,2386,2.047]],["t/1481",[9,0.625,710,5.642,735,4.732,850,2.256,972,3.664,1121,3.258,1893,2.698,2024,3.885,2143,3.193,2154,4.165,2373,5.981]],["t/1483",[4,2.058,9,0.606,43,1.249,122,2.569,231,1.49,401,1.693,428,3.065,447,2.268,452,2.319,454,3.749,470,1.166,471,1.451,653,0.917,735,1.249,747,3.487,767,1.298,850,3.446,898,2.268,929,3.142,1121,3.357,1122,2.929,1257,1.849,1308,1.764,1438,3.534,1497,1.956,1893,2.78,1894,1.532,1935,1.764,1958,1.693,2122,1.764,2125,2.569,2154,1.693,2157,3.357,2164,3.419,2313,1.956,2318,1.956,2334,1.632,2337,3.534,2387,2.098,2388,2.098,2389,2.098,2390,2.098,2391,2.098,2392,2.098,2393,2.098,2394,2.098,2395,2.098,2396,2.098,2397,2.098,2398,2.098,2399,2.098,2400,2.098,2401,2.098,2402,2.098,2403,2.098,2404,2.098,2405,1.764,2406,2.098,2407,2.098,2408,2.098,2409,2.098]],["t/1486",[9,0.606,398,1.255,453,1.752,579,5.943,735,4.078,850,2.993,895,1.485,898,3.725,1121,3.647,1122,3.725,1185,4.857,1191,1.704,1416,4.02,1618,4.219,1893,1.255,1895,2.342,2143,1.485,2157,1.515,2336,2.872,2352,2.115,2373,1.806,2410,2.4,2411,2.4,2412,2.4,2413,2.4,2414,2.4,2415,2.4,2416,5.778,2417,2.4,2418,3.933,2419,3.933,2420,2.4,2421,2.4,2422,2.4,2423,2.4,2424,2.4,2425,2.4,2426,2.4,2427,3.933,2428,2.4,2429,2.4,2430,2.4,2431,2.4,2432,2.4,2433,2.4,2434,2.4,2435,2.4,2436,2.4,2437,2.4]],["t/1488",[9,0.611,304,3.446,398,2.886,453,2.7,454,2.162,470,2.055,587,1.933,735,2.202,818,2.783,850,3.426,898,4.722,929,5.066,1438,2.877,1618,2.7,1893,1.933,1895,3.287,2157,4.169,2164,2.783,2334,2.877,2336,5.723,2359,3.108,2438,3.698,2439,3.698,2440,3.698,2441,3.698,2442,3.698]],["t/1490",[9,0.604,136,2.368,381,3.591,470,2.734,653,2.15,675,2.499,719,3.042,721,2.826,735,2.928,850,2.15,1041,3.491,1121,3.104,1122,3.171,1160,4.134,2143,4.191,2157,4.276,2334,3.826,2469,6.775,2676,4.918,2750,5.426,2751,5.426,2752,5.426]],["t/1492",[9,0.629,454,2.573,735,3.731,747,2.942,850,2.739,898,2.836,1121,2.777,1438,3.423,1893,3.816,1895,2.62,2143,2.721,2157,3.955,2373,3.311,2443,4.399,2444,4.399,2445,4.399,2446,4.399,2447,4.399]],["t/1494",[0,3.559,9,0.496]],["t/1496",[9,0.564,50,6.638]],["t/1498",[9,0.506]],["t/1500",[9,0.592,58,5.2,296,7.122,1472,5.746,2753,5.839,2754,5.292,2755,8.88,2756,5.839,2757,5.839,2758,5.839,2759,5.292,2760,8.88,2761,5.839,2762,5.839,2763,5.839]],["t/1502",[9,0.46,153,6.029,2764,8.244,2765,8.244,2766,8.244,2767,8.244]],["t/1504",[9,0.558,2754,7.75,2759,7.75]],["t/1507",[2768,9.055]],["t/1509",[9,0.549,22,4.52,100,6.536,237,4.772,2769,7.211,2770,9.829,2771,7.211,2772,7.211,2773,7.211,2774,7.211]],["t/1512",[1080,5.425,1588,6.499,2775,7.693,2776,7.693,2777,7.693,2778,7.693,2779,7.693,2780,7.693,2781,7.693,2782,6.973]],["t/1514",[9,0.463,719,2.873,738,3.904,819,4.328,1927,5.737,1996,6.511,2010,4.644,2170,4.328,2718,4.644,2782,4.644,2783,5.124,2784,5.124,2785,8.294,2786,5.124,2787,5.124,2788,5.124,2789,5.124,2790,5.124,2791,5.124,2792,5.124,2793,7.183,2794,5.124,2795,7.183,2796,8.294,2797,5.124,2798,5.124,2799,5.124,2800,5.124,2801,5.124,2802,5.124]],["t/1517",[9,0.633,21,3.597,224,5.55,587,2.915,2803,6.151,2804,6.151]],["t/1519",[9,0.627,2805,7.693,2806,7.693]],["t/1521",[0,3.491,2807,10.098]],["t/1523",[9,0.506]],["t/1525",[7,4.726,9,0.635,15,3.822,16,1.883,19,2.272,22,2.02,24,2.574,33,3.872,2808,2.921,2809,6.473,2810,5.702,2811,5.702,2812,6.473,2813,7.484,2814,2.921,2815,2.921,2816,4.606,2817,2.921,2818,2.921]],["t/1527",[0,0.871,2,1.398,3,1.736,4,1.152,5,1.736,6,1.736,7,4.826,8,1.656,9,0.633,15,3.762,16,1.27,17,4.043,18,1.836,19,2.595,21,0.806,22,3.951,24,3.823,33,3.647,34,1.97,55,1.97,224,1.482,587,1.744,667,1.836,2808,1.97,2809,5.106,2810,4.338,2811,4.338,2812,5.713,2813,6.204,2814,1.97,2815,1.97,2816,3.335,2817,1.97,2818,1.97,2819,2.173,2820,3.68,2821,2.173,2822,2.173,2823,2.173,2824,2.173,2825,2.173,2826,2.173]],["t/1530",[0,2.009,9,0.634,57,4.789,853,5.172,1402,3.819,2827,4.543,2828,4.543,2829,4.543,2830,4.543,2831,4.543,2832,4.543,2833,4.543,2834,4.543,2835,4.543,2836,4.543,2837,4.543]],["t/1532",[0,3.304,9,0.615]],["t/1534",[9,0.558,2838,7.75,2839,7.75]],["t/1536",[0,3.122,9,0.639,224,6.02,2263,4.871,2840,5.226,2841,5.226]],["t/1539",[0,3.02,9,0.639,224,5.869,2263,4.638,2838,4.977,2839,4.977,2840,4.977,2841,4.977]],["t/1541",[0,2.009,9,0.634,57,4.789,853,5.172,1402,3.819,2827,4.543,2828,4.543,2829,4.543,2830,4.543,2831,4.543,2832,4.543,2833,4.543,2834,4.543,2835,4.543,2836,4.543,2837,4.543]],["t/1545",[2613,7.897,2710,7.897,2842,8.712]],["t/1547",[0,3.411,9,0.527,470,3.321,587,3.124,738,6.485,739,5.265,2843,8.511,2844,6.591,2845,6.591,2846,6.591,2847,6.591,2848,6.591,2849,6.591,2850,6.591,2851,6.591]],["t/1549",[9,0.584,21,2.554,650,5.818,934,3.787,1554,5.248,2852,6.887,2853,6.887,2854,8.754,2855,6.887,2856,6.887,2857,6.887,2858,6.887]],["t/1551",[9,0.535,421,5.268,1416,4.01,1456,7.214,2859,7.959,2860,7.959,2861,7.959]],["t/1554",[2862,9.055]],["t/1556",[0,3.171,9,0.442,14,3.318,21,1.745,237,3.114,587,2.23,651,4.265,895,2.638,1416,2.371,1554,5.151,2863,4.705,2864,4.705,2865,7.913,2866,6.761,2867,4.705,2868,4.705,2869,4.705,2870,4.705,2871,4.705,2872,7.913,2873,4.705,2874,4.705,2875,4.705,2876,4.705,2877,4.705,2878,4.705,2879,4.705,2880,4.705,2881,4.705,2882,4.705,2883,4.705,2884,6.761,2885,4.705,2886,4.705,2887,4.705,2888,4.705,2889,4.705]],["t/1558",[9,0.403,21,2.674,366,3.892,587,3.417,589,6.092,1416,3.633,2890,7.211,2891,7.211,2892,7.211,2893,7.211,2894,7.211,2895,7.211,2896,7.211,2897,7.211]],["t/1560",[0,2.936,9,0.595,21,2.717,366,3.954,587,3.471,650,6.189,1081,5.582,2898,7.326,2899,7.326]],["t/1562",[0,3.491,2900,8.712,2901,8.712]],["t/1564",[21,3.342,43,5.305,237,6.505,2902,7.612,2903,7.211,2904,7.211,2905,7.211,2906,7.211]],["t/1566",[21,3.004,43,5.221,237,5.36,2902,6.842,2907,8.099,2908,8.099]],["t/1568",[21,2.901,43,4.223,237,6.27,1416,3.942,2902,6.609,2909,7.824,2910,7.824,2911,7.824]],["t/1570",[9,0.445,1416,5.173,2912,7.959,2913,7.959,2914,7.959,2915,7.959]],["t/1572",[934,5.493,2916,8.551,2917,8.551]],["t/1574",[9,0.416,21,2.761,43,4.96,237,4.927,1604,5.946,2918,7.444,2919,7.444,2920,7.444,2921,7.444,2922,7.444,2923,7.444]],["t/1576",[9,0.591,2924,8.551]],["t/1579",[2698,6.767,2925,7.502]],["t/1581",[9,0.535,21,2.952,587,3.772,2698,7.294,2925,8.087]],["t/1583",[9,0.535,21,2.952,587,3.772,2698,7.294,2925,8.087]],["t/1585",[0,3.559,9,0.496]],["t/1587",[9,0.564,50,6.638]],["t/1590",[9,0.596]],["t/1592",[9,0.585,924,5.335,2926,9.283,2927,7.567,2928,7.567,2929,7.567,2930,7.567]],["t/1594",[9,0.513,21,2.761,2931,7.444,2932,7.444,2933,7.444,2934,7.444,2935,7.444,2936,7.444,2937,7.444,2938,7.444,2939,7.444]]],"invertedIndex":[["",{"_index":9,"t":{"803":{"position":[[57,1],[70,1],[80,2],[94,1],[102,1],[106,1],[113,1],[127,1],[132,1],[136,1],[140,1],[145,1],[149,1],[157,2],[164,1],[172,2],[178,1],[185,2],[190,2],[199,1],[201,1],[213,1]]},"805":{"position":[[0,20],[94,1],[99,1],[103,1],[107,1],[112,1],[116,1],[122,2],[125,5],[133,2],[140,1],[165,1],[181,1],[188,1],[205,1],[211,1],[217,1],[222,1],[230,1]]},"808":{"position":[[0,9],[21,6],[41,4]]},"810":{"position":[[0,39],[40,24]]},"814":{"position":[[57,1],[73,2],[88,1],[98,1],[132,1],[142,1],[158,1],[165,1],[182,1],[188,1],[194,1],[199,1],[213,1],[223,1],[228,2],[234,1],[241,1],[251,1],[269,1],[271,1],[328,2],[334,1],[341,2],[349,2],[368,1],[375,1],[382,2],[388,2],[397,1],[399,1]]},"816":{"position":[[19,19],[71,8],[83,1],[98,1]]},"818":{"position":[[4,12]]},"820":{"position":[[0,15],[21,2]]},"822":{"position":[[4,93],[98,5],[264,5]]},"824":{"position":[[57,1],[131,1],[139,2],[144,2],[149,2],[155,1],[164,1],[179,2],[182,6],[198,1],[204,1],[206,2],[209,6],[216,1],[218,2],[221,5],[238,1],[245,1],[267,1],[279,2],[297,1],[352,1],[364,2],[369,2],[376,2],[382,1],[391,1],[404,1],[415,2],[428,1],[457,1],[459,1],[471,1]]},"826":{"position":[[0,47]]},"830":{"position":[[28,14],[43,16],[60,7],[68,11],[80,8],[195,10]]},"834":{"position":[[19,9],[29,9],[39,7]]},"836":{"position":[[0,9],[10,20]]},"838":{"position":[[49,5],[105,17],[123,19],[143,19]]},"840":{"position":[[0,6],[167,16],[184,8],[258,58],[317,3],[354,5],[441,17],[459,2],[487,2],[514,2],[517,4],[538,2],[557,3],[578,5],[633,1],[702,2],[733,1],[799,1],[862,1],[929,1],[970,2],[982,1],[1060,2],[1097,3],[1143,5],[1198,1],[1234,2],[1259,2],[1287,2],[1290,4],[1308,2],[1421,1],[1449,1],[1570,1],[1572,14],[1643,1],[1645,14],[1774,1],[1803,1],[1878,1],[1905,1],[1963,1],[2023,1],[2080,5],[2086,12],[2099,16],[2116,15]]},"844":{"position":[[123,1],[159,1],[184,1],[200,1],[220,2],[232,1],[256,9],[266,1],[282,1],[292,1],[315,1],[347,2],[374,1],[415,1],[417,8],[441,1],[472,1],[479,1],[493,1],[495,1],[497,1],[499,1],[501,1],[572,1],[630,4],[635,2],[638,1],[666,1],[736,1],[805,1],[863,1],[881,1],[883,1],[885,1],[936,1],[1002,2],[1335,1],[1371,1],[1396,1],[1412,1],[1432,2],[1444,1],[1468,9],[1478,1],[1494,1],[1504,1],[1527,1],[1559,2],[1586,1],[1656,1],[1687,1],[1694,1],[1708,1],[1710,1],[1712,1],[1714,1],[1716,1],[1718,2],[1721,1],[1750,1],[1803,1],[1830,2],[1871,1],[2145,2],[2254,1],[2312,2],[2340,2],[2352,1]]},"846":{"position":[[67,2],[225,15],[365,1],[393,1],[514,1],[516,14],[587,1],[589,14],[718,1],[747,1],[822,1],[849,1],[907,1],[967,1],[1024,13],[1053,36]]},"850":{"position":[[0,23],[24,25],[50,23],[74,6],[81,4],[86,6],[93,17]]},"852":{"position":[[0,20],[21,36],[58,13],[72,22]]},"854":{"position":[[0,14]]},"861":{"position":[[72,31],[277,5],[283,5]]},"863":{"position":[[0,57]]},"867":{"position":[[234,3],[370,11],[382,7],[403,5],[413,28],[442,39]]},"871":{"position":[[33,44],[78,47]]},"874":{"position":[[55,19],[75,13]]},"879":{"position":[[4,19]]},"883":{"position":[[0,10],[19,31],[51,5],[65,20],[86,7],[94,7],[102,15]]},"886":{"position":[[0,37],[42,23],[66,30]]},"888":{"position":[[0,7],[8,20],[39,13]]},"891":{"position":[[0,15],[16,43],[60,15],[76,22]]},"893":{"position":[[12,9],[22,24],[90,2],[93,4],[107,1],[270,1],[444,1],[631,1]]},"896":{"position":[[0,61],[62,42],[105,56],[162,39],[202,32],[235,47]]},"898":{"position":[[0,40],[41,61],[174,7],[182,4],[208,13],[258,5],[264,42],[309,5],[315,32],[350,5],[356,20],[379,5],[385,46],[434,5],[456,4],[463,5],[469,54],[526,4],[531,27],[559,6],[566,17],[584,22]]},"900":{"position":[[0,42],[45,5],[51,32],[86,5],[92,20]]},"902":{"position":[[0,46],[49,5],[71,4],[78,5],[84,54]]},"904":{"position":[[0,27],[28,6],[35,17],[53,22]]},"906":{"position":[[4,14]]},"908":{"position":[[0,15],[21,2]]},"910":{"position":[[0,4]]},"915":{"position":[[0,137]]},"917":{"position":[[10,8],[19,19],[39,15],[55,21],[77,6],[84,21],[106,6],[113,52],[166,3],[170,32],[203,6],[210,6],[217,16],[234,6],[241,26]]},"919":{"position":[[244,51]]},"921":{"position":[[0,3],[87,3],[91,53]]},"923":{"position":[[0,3],[96,3],[100,53]]},"925":{"position":[[0,32],[33,82],[116,43],[504,27],[663,17],[681,118]]},"927":{"position":[[0,4],[14,5],[66,3],[113,40]]},"940":{"position":[[0,5],[389,4],[812,13]]},"944":{"position":[[0,1],[24,2],[64,39],[116,18],[154,25],[199,19],[219,5],[225,25],[259,14],[274,5],[280,38],[327,1],[338,10],[357,1],[410,5],[424,32],[465,1],[505,46],[560,1],[621,10]]},"948":{"position":[[790,31],[1546,15],[1590,1],[1616,2],[1635,1]]},"953":{"position":[[86,3],[151,24],[176,3],[180,84],[265,18]]},"955":{"position":[[77,18],[96,3],[100,3],[104,5],[110,7],[171,5],[266,4],[363,4]]},"961":{"position":[[45,8],[54,8],[63,4],[68,9],[88,10],[99,64],[164,56],[221,54],[276,8],[285,9],[404,10],[518,9],[699,10],[878,8],[887,9],[1009,10],[1148,9],[1344,10]]},"964":{"position":[[0,19],[20,5],[26,5],[32,5],[38,4],[43,3],[47,2]]},"966":{"position":[[0,16],[42,33]]},"968":{"position":[[0,7],[25,3]]},"971":{"position":[[0,5],[6,39],[46,7],[54,6],[61,6],[68,6],[75,6],[82,22],[105,22],[128,73],[202,3],[206,64],[275,74]]},"973":{"position":[[0,7],[8,3],[12,50],[63,3],[261,12],[274,3],[278,7],[286,3],[290,26],[317,3],[321,40],[362,3],[509,10],[520,3]]},"975":{"position":[[0,23],[24,5],[40,6],[47,3],[51,3],[55,12],[68,7],[76,6],[83,6],[90,4],[95,8],[104,4],[109,5],[115,5],[121,5],[127,53],[181,3],[185,12],[198,39],[238,19],[282,19]]},"977":{"position":[[0,76],[77,34]]},"979":{"position":[[0,17],[358,42],[884,60],[1113,2],[1168,2]]},"981":{"position":[[0,14],[169,27],[630,2],[685,2]]},"983":{"position":[[0,5],[6,38],[45,13],[359,5],[365,40],[406,13]]},"990":{"position":[[0,5],[6,15],[22,12],[35,23],[59,29],[89,14],[104,5],[110,84],[195,16],[212,22],[235,20],[256,77],[334,90]]},"992":{"position":[[99,10],[110,5],[116,39]]},"995":{"position":[[0,6],[160,6]]},"997":{"position":[[0,9],[10,45],[56,8],[65,30],[96,26],[123,27]]},"999":{"position":[[10,7],[18,34],[53,13],[76,7],[84,17],[102,13],[127,7],[135,37],[173,13],[198,7],[206,17],[272,10],[283,9],[326,16],[343,6]]},"1001":{"position":[[0,10],[11,64],[76,56],[133,54],[188,11],[200,19],[220,24],[245,25],[271,18],[290,21],[312,26],[362,18],[433,35],[469,38],[508,27],[536,49],[586,12],[599,46],[646,68],[715,8],[724,3],[728,3],[732,12],[745,25],[771,46],[818,56],[875,17],[896,4],[901,2]]},"1005":{"position":[[0,30],[232,7]]},"1007":{"position":[[0,12]]},"1009":{"position":[[187,15]]},"1022":{"position":[[201,18],[285,17],[562,26]]},"1026":{"position":[[256,10],[917,5],[923,10]]},"1038":{"position":[[0,8],[13,13],[74,11]]},"1043":{"position":[[0,3],[19,2],[30,9]]},"1045":{"position":[[0,13],[27,2],[45,8],[69,5]]},"1049":{"position":[[9,7],[71,1],[88,1],[90,1],[92,1],[94,1],[130,1],[147,1],[149,1],[151,1],[153,1],[174,8],[198,1],[209,15]]},"1051":{"position":[[0,19],[55,1],[122,1],[230,1],[330,1],[397,1],[502,1],[586,1],[593,4],[605,19],[637,2],[650,19],[674,5],[685,5],[719,2],[748,8],[782,1],[809,5],[824,17],[870,1],[948,1],[1022,1],[1100,1]]},"1053":{"position":[[0,1],[9,2],[20,3],[32,11],[53,8],[124,17],[146,32],[195,17],[220,50]]},"1055":{"position":[[9,1],[31,1],[91,1],[120,1],[159,1],[181,3],[187,3],[237,5],[245,8],[278,33],[312,1],[327,8],[340,8],[351,3],[359,30],[413,14],[430,10],[441,1],[456,9],[514,1],[526,2],[595,1],[724,1],[726,1],[739,1],[766,3],[772,6],[779,1],[790,9],[802,5],[810,7],[820,16],[839,13],[855,27],[885,1],[889,7],[917,22],[940,5],[959,6],[1008,5],[1022,12],[1043,10],[1062,5],[1081,32],[1130,6],[1145,28],[1182,13],[1204,14],[1276,6]]},"1057":{"position":[[0,15],[155,5],[170,7],[221,1],[269,1],[293,1],[323,1],[392,1],[505,1],[516,14],[531,1],[587,46],[649,10],[660,3],[666,5],[681,24],[744,33],[803,13],[826,11],[912,6],[921,13],[937,13],[953,44],[998,5],[1012,6],[1029,3],[1118,2],[1134,11],[1154,5],[1177,8],[1210,9],[1228,1],[1277,5],[1298,8]]},"1059":{"position":[[0,4],[10,2],[59,7],[87,11],[99,9],[113,2],[129,6],[141,4],[237,30],[268,4],[273,46],[345,7],[356,26],[396,10],[407,5],[416,11],[447,1],[618,8],[736,2],[817,7],[843,6],[850,4],[871,5],[883,10],[897,4],[907,10],[921,1],[973,17],[1018,4],[1028,7],[1050,2],[1057,3],[1072,1],[1086,1],[1092,7],[1104,1],[1113,2],[1127,1],[1142,1],[1148,10],[1162,3],[1183,2],[1236,8],[1245,2],[1251,20],[1295,13],[1322,4],[1351,12],[1373,20],[1400,1],[1473,28],[1512,43],[1560,15],[1590,2],[1597,3],[1612,1],[1626,1],[1632,7],[1644,1],[1653,2],[1667,1],[1682,1],[1688,10],[1703,3],[1710,2],[1765,29],[1816,4],[1855,3],[1944,13],[1964,24],[2028,11],[2064,11],[2076,7],[2105,3],[2109,6],[2461,3],[2465,6]]},"1062":{"position":[[0,19],[31,2],[46,12],[69,2],[74,4],[133,9],[176,3],[191,6],[210,7],[258,17],[276,5],[282,9],[302,5],[319,15],[344,1],[348,6],[365,7],[373,8],[414,2],[441,20],[474,35],[510,19],[535,3],[548,5],[578,3],[602,15],[620,3],[642,2],[649,1],[667,33]]},"1064":{"position":[[0,2],[7,4],[31,7],[46,11],[68,16],[94,3],[98,21],[125,14],[140,45],[217,23],[244,12],[282,15],[307,7],[315,8],[324,3],[328,18],[347,24],[372,18],[391,23]]},"1067":{"position":[[7,5],[15,31],[56,4],[68,3],[110,3],[282,3],[373,1],[391,1],[406,1],[408,2],[411,1],[461,1],[502,1],[585,1],[629,1],[723,1],[819,1],[909,1],[911,17],[947,1],[972,1],[1071,1]]},"1069":{"position":[[0,1],[6,14],[29,6],[100,2],[138,12],[171,1],[195,7],[207,35],[278,3],[305,14],[328,1],[338,2],[361,4],[388,8],[404,11],[425,7],[446,32],[479,4],[509,3],[536,12],[556,17],[583,7],[604,11],[642,4],[657,27],[698,3],[702,2],[710,8],[722,5],[728,1],[758,9],[795,5],[819,24],[867,11],[891,5],[905,17],[923,3],[931,3],[942,4],[1008,24],[1041,11],[1061,11],[1081,15],[1097,12],[1117,37],[1163,9],[1181,9],[1208,3],[1220,10]]},"1071":{"position":[[107,13],[154,8],[194,50],[252,39],[292,15],[316,13],[337,40],[385,7],[401,9],[411,4],[424,1],[446,6],[570,6],[890,1],[909,4],[918,15],[942,24],[975,14],[1013,1],[1037,14],[1070,7],[1086,8],[1103,8],[1145,14],[1160,1],[1184,4],[1193,14]]},"1073":{"position":[[4,33]]},"1075":{"position":[[0,89],[90,157]]},"1079":{"position":[[191,2],[199,8],[242,16],[286,31],[354,31]]},"1081":{"position":[[361,3]]},"1083":{"position":[[12,34],[225,28],[821,29],[856,41],[1154,5],[1388,5]]},"1085":{"position":[[302,11],[314,2],[322,49],[372,1],[379,32],[424,65],[498,87],[594,40],[635,11]]},"1091":{"position":[[102,50]]},"1093":{"position":[[0,171],[172,34],[207,31],[239,41]]},"1095":{"position":[[0,100],[101,123],[225,116]]},"1097":{"position":[[0,41]]},"1099":{"position":[[0,75],[76,12],[181,1],[357,9],[385,1],[392,1],[399,36],[441,1],[448,37]]},"1101":{"position":[[6,35],[44,28],[79,20],[102,27],[130,11],[212,3],[220,7],[228,8],[239,15],[264,2],[276,31],[310,21],[332,8],[595,1],[601,1],[648,3],[673,11]]},"1103":{"position":[[6,6],[21,24],[48,18],[69,10],[80,11],[162,3],[170,7],[178,28],[356,3],[417,5],[431,14],[470,2],[482,31],[516,20],[704,1],[710,1],[780,11]]},"1105":{"position":[[0,61],[68,1],[76,51]]},"1112":{"position":[[29,7]]},"1114":{"position":[[30,1],[41,1],[43,3],[47,1],[181,1],[183,22],[206,1],[278,1],[353,1]]},"1116":{"position":[[30,1],[152,1],[154,22]]},"1118":{"position":[[0,4]]},"1122":{"position":[[518,3],[535,3],[553,3],[571,3]]},"1126":{"position":[[10,5]]},"1129":{"position":[[95,35],[371,1],[375,18],[405,3],[409,8],[420,24],[449,19],[469,3],[473,48],[522,3],[526,51],[589,3],[593,8],[604,31],[638,7],[646,3],[650,14],[667,31],[699,3],[703,47],[753,14],[770,6],[779,19]]},"1131":{"position":[[104,5],[290,81]]},"1138":{"position":[[10,9],[20,40],[61,13],[79,1],[83,3],[93,14],[136,4],[149,2],[181,5],[286,1],[341,1],[393,1],[437,8],[904,1],[960,1],[1050,1],[1140,1],[1242,1],[1282,1],[1291,1],[1307,1],[1316,1],[1333,1],[1342,1],[1375,1],[1413,1],[1449,1],[1458,11],[1470,33],[1651,1],[1694,1],[1704,1],[1824,1],[1870,1],[1898,1],[2130,1],[2196,1],[2253,1],[2271,1],[2292,2],[2347,10],[2548,1],[2574,1],[2603,3],[3195,5],[3201,1],[3331,1],[3388,1],[3440,1],[3484,1],[3544,1],[3634,1],[3724,1],[3806,1],[3846,1],[3855,1],[3871,1],[3880,1],[3897,1],[3906,1],[3939,1],[3977,1],[4013,1],[4022,1],[4064,1],[4074,1],[4120,1],[4166,1],[4194,1],[4289,1],[4355,1],[4412,1],[4430,1],[4451,2],[4506,1],[4532,1]]},"1140":{"position":[[130,4],[190,4],[257,4]]},"1144":{"position":[[217,40],[258,13],[276,1],[280,3],[290,14],[333,4],[346,2]]},"1146":{"position":[[0,15],[33,20],[71,4]]},"1148":{"position":[[127,36],[170,16],[221,12],[240,5],[269,8],[278,6],[287,1],[331,6],[340,13],[371,51],[460,5],[466,13],[488,51],[540,5],[546,15],[564,27],[594,29]]},"1152":{"position":[[0,12],[88,5],[239,100],[352,13]]},"1154":{"position":[[69,37],[187,1],[189,16]]},"1158":{"position":[[106,12],[162,1]]},"1160":{"position":[[95,1]]},"1162":{"position":[[107,1],[137,6]]},"1164":{"position":[[0,8],[13,13],[74,11]]},"1169":{"position":[[0,3],[19,2],[30,9]]},"1171":{"position":[[0,13],[27,2],[45,8],[69,5]]},"1175":{"position":[[9,7],[71,1],[88,1],[90,1],[92,1],[94,1],[130,1],[147,1],[149,1],[151,1],[153,1],[174,8],[198,1],[209,15]]},"1177":{"position":[[0,19],[55,1],[122,1],[230,1],[330,1],[397,1],[502,1],[586,1],[593,4],[605,19],[637,2],[650,19],[674,5],[685,5],[719,2],[748,8],[782,1],[809,5],[824,17],[870,1],[948,1],[1022,1],[1100,1]]},"1179":{"position":[[0,1],[9,2],[20,3],[32,11],[53,8],[124,17],[146,32],[195,17],[220,50]]},"1181":{"position":[[9,1],[31,1],[91,1],[120,1],[159,1],[181,3],[187,3],[237,5],[245,8],[278,33],[312,1],[327,8],[340,8],[351,3],[359,30],[413,14],[430,10],[441,1],[456,9],[514,1],[526,2],[595,1],[724,1],[726,1],[739,1],[766,3],[772,6],[779,1],[790,9],[802,5],[810,7],[820,16],[839,13],[855,27],[885,1],[889,7],[917,22],[940,5],[959,6],[1008,5],[1022,12],[1043,10],[1062,5],[1081,32],[1130,6],[1145,28],[1182,13],[1204,14],[1276,6]]},"1183":{"position":[[0,15],[155,5],[170,7],[221,1],[269,1],[293,1],[323,1],[392,1],[505,1],[516,14],[531,1],[587,46],[649,10],[660,3],[666,5],[681,24],[744,33],[803,13],[826,11],[912,6],[921,13],[937,13],[953,44],[998,5],[1012,6],[1029,3],[1118,2],[1134,11],[1154,5],[1177,8],[1210,9],[1228,1],[1277,5],[1298,8]]},"1185":{"position":[[0,4],[10,2],[59,7],[87,11],[99,9],[113,2],[129,6],[141,4],[237,30],[268,4],[273,46],[345,7],[356,26],[396,10],[407,5],[416,11],[447,1],[618,8],[736,2],[817,7],[843,6],[850,4],[871,5],[883,10],[897,4],[907,10],[921,1],[973,17],[1018,4],[1028,7],[1050,2],[1057,3],[1072,1],[1086,1],[1092,7],[1104,1],[1113,2],[1127,1],[1142,1],[1148,10],[1162,3],[1183,2],[1236,8],[1245,2],[1251,20],[1295,13],[1322,4],[1351,12],[1373,20],[1400,1],[1473,28],[1512,43],[1560,15],[1590,2],[1597,3],[1612,1],[1626,1],[1632,7],[1644,1],[1653,2],[1667,1],[1682,1],[1688,10],[1703,3],[1710,2],[1765,29],[1816,4],[1855,3],[1944,13],[1964,24],[2028,11],[2064,11],[2076,7],[2105,3],[2109,6],[2461,3],[2465,6]]},"1188":{"position":[[0,19],[31,2],[46,12],[69,2],[74,4],[133,9],[176,3],[191,6],[210,7],[258,17],[276,5],[282,9],[302,5],[319,15],[344,1],[348,6],[365,7],[373,8],[414,2],[441,20],[474,35],[510,19],[535,3],[548,5],[578,3],[602,15],[620,3],[642,2],[649,1],[667,33]]},"1190":{"position":[[0,2],[7,4],[31,7],[46,11],[68,16],[94,3],[98,21],[125,14],[140,45],[217,23],[244,12],[282,15],[307,7],[315,8],[324,3],[328,18],[347,24],[372,18],[391,23]]},"1193":{"position":[[7,5],[15,31],[56,4],[68,3],[110,3],[282,3],[373,1],[391,1],[406,1],[408,2],[411,1],[461,1],[502,1],[585,1],[629,1],[723,1],[819,1],[909,1],[911,17],[947,1],[972,1],[1071,1]]},"1195":{"position":[[0,1],[6,14],[29,6],[100,2],[138,12],[171,1],[195,7],[207,35],[278,3],[305,14],[328,1],[338,2],[361,4],[388,8],[404,11],[425,7],[446,32],[479,4],[509,3],[536,12],[556,17],[583,7],[604,11],[642,4],[657,27],[698,3],[702,2],[710,8],[722,5],[728,1],[758,9],[795,5],[819,24],[867,11],[891,5],[905,17],[923,3],[931,3],[942,4],[1008,24],[1041,11],[1061,11],[1081,15],[1097,12],[1117,37],[1163,9],[1181,9],[1208,3],[1220,10]]},"1197":{"position":[[107,13],[154,8],[194,50],[252,39],[292,15],[316,13],[337,40],[385,7],[401,9],[411,4],[424,1],[446,6],[570,6],[890,1],[909,4],[918,15],[942,24],[975,14],[1013,1],[1037,14],[1070,7],[1086,8],[1103,8],[1145,14],[1160,1],[1184,4],[1193,14]]},"1201":{"position":[[24,1],[37,1],[115,1],[306,1],[351,1]]},"1203":{"position":[[23,1],[41,1],[45,1],[53,1],[159,1],[177,1],[181,1],[189,1],[297,1]]},"1206":{"position":[[37,19],[89,1],[189,1]]},"1208":{"position":[[0,4],[314,2],[350,2],[353,2],[705,2],[717,2],[741,2],[775,2],[801,2],[815,2],[829,2],[877,2],[890,1],[945,2],[998,1],[1030,1],[1061,1],[1118,4],[1173,2],[1221,2],[1229,1],[1246,2],[1259,2],[1262,2],[1338,2],[1436,1],[1448,1],[1490,1],[1512,1],[1535,1],[1582,1],[1632,1],[1679,1],[1704,1],[1738,1],[1770,1],[1815,1],[1865,1]]},"1210":{"position":[[4,8]]},"1216":{"position":[[72,1],[161,15],[253,1],[297,1],[335,1],[451,1],[563,1]]},"1219":{"position":[[139,16]]},"1223":{"position":[[0,24],[25,14],[40,32]]},"1235":{"position":[[271,1],[494,3],[498,14],[513,32],[546,15]]},"1238":{"position":[[0,24],[25,23],[49,5],[55,3]]},"1240":{"position":[[0,26],[179,20],[495,17],[801,16]]},"1242":{"position":[[0,31],[261,10],[341,1]]},"1247":{"position":[[0,35]]},"1251":{"position":[[89,38],[158,1],[306,1],[369,1],[397,1],[425,1],[540,1],[582,1],[624,1]]},"1254":{"position":[[0,88],[89,62]]},"1256":{"position":[[0,115],[116,80],[197,64],[262,47],[310,50]]},"1258":{"position":[[0,78],[79,65]]},"1261":{"position":[[0,150]]},"1263":{"position":[[335,30],[498,38],[537,15],[566,1],[576,1],[593,1],[598,2],[610,1],[614,1],[619,1],[630,1],[634,1]]},"1270":{"position":[[197,1],[249,1],[503,1],[542,1],[771,1],[775,1],[1022,1],[1067,1],[1137,1],[1173,1],[1190,1],[1252,1],[1285,1],[1358,2],[1459,2],[1483,2],[1580,1],[1628,1],[1683,1],[1803,1],[1841,1],[1991,1],[2026,1],[2037,1],[2097,1],[2163,1],[2175,1],[2197,1],[2209,1],[2231,1],[2236,1],[2251,2],[2257,2],[2271,2],[2310,1],[2319,1],[2324,1],[2376,1],[2435,1],[2564,1],[2577,1],[2650,1],[2678,1],[2735,1],[2751,1]]},"1275":{"position":[[83,1],[102,1],[121,1],[141,1],[160,1],[180,1],[209,1],[267,1],[293,1],[323,1],[388,1],[485,1],[518,1],[524,1],[551,1],[556,1],[589,1],[594,1],[606,3],[610,1],[625,1],[714,1],[762,1],[764,8],[775,1],[862,1],[893,2],[930,1],[1041,1],[1069,1],[1097,1],[1130,3],[1203,3],[1209,1],[1250,1],[1291,1],[1324,3],[1406,3]]},"1281":{"position":[[0,107],[132,2],[135,8],[159,14],[180,5],[267,34],[308,50]]},"1283":{"position":[[0,49],[50,16],[103,12],[122,5],[128,20]]},"1285":{"position":[[0,18],[110,1],[112,4],[132,2],[139,1],[145,6],[177,1],[360,3],[401,2],[443,7],[573,5],[598,1],[615,4],[620,6],[651,3],[664,8],[747,6],[763,8],[776,1],[794,12],[807,12],[827,4],[862,11],[898,4],[907,3]]},"1287":{"position":[[0,10],[35,20],[291,2],[303,6],[340,5],[360,4],[369,7],[423,12],[445,2]]},"1290":{"position":[[0,14],[19,1],[29,10],[42,1],[46,21],[74,1],[82,13],[96,12],[117,19],[143,6]]},"1297":{"position":[[0,2],[13,12],[32,1],[40,19],[63,2]]},"1299":{"position":[[0,4],[14,5],[66,3],[113,40]]},"1312":{"position":[[0,5],[389,4],[812,13]]},"1316":{"position":[[0,1],[24,2],[64,39],[116,18],[154,25],[199,19],[219,5],[225,25],[259,14],[274,5],[280,38],[327,1],[338,10],[357,1],[410,5],[424,32],[465,1],[505,46],[560,1],[621,10]]},"1320":{"position":[[790,31],[1546,15],[1590,1],[1616,2],[1635,1]]},"1322":{"position":[[0,4],[154,4]]},"1327":{"position":[[0,4],[24,5]]},"1330":{"position":[[0,8],[81,6],[90,7],[98,11],[176,3],[249,6],[561,6],[627,20],[753,3]]},"1332":{"position":[[0,10],[263,14],[376,2],[401,2],[404,2],[473,2],[476,2]]},"1336":{"position":[[122,1],[126,1],[139,3],[143,33],[177,17],[195,14]]},"1342":{"position":[[0,8],[13,7],[36,12],[88,1],[96,11],[111,16],[131,20],[160,4],[189,2],[196,14],[216,16],[233,7],[244,5],[279,2],[286,1],[293,35],[329,19],[349,3],[375,6],[458,8],[467,32],[505,34],[557,5],[567,1],[585,8],[599,2],[608,11],[620,28],[660,1],[673,11],[696,30],[727,2],[730,8],[766,15]]},"1345":{"position":[[0,4],[11,3],[41,3],[45,4],[59,6],[85,7],[102,25],[137,5],[143,1],[154,6],[196,6],[219,5],[241,2],[260,12]]},"1347":{"position":[[0,3],[25,8],[40,11],[57,54],[117,11],[129,2],[154,2],[177,6],[192,17],[220,15],[244,3],[258,2],[271,16],[298,4],[305,7],[448,8],[465,35],[501,2],[523,3],[557,23],[581,5],[591,5],[605,24],[636,12],[862,9],[895,8],[911,59],[985,10],[1074,60],[1330,7],[1342,4],[1361,3],[1369,23],[1398,36],[1435,8],[1449,11],[1553,33],[1709,6],[1731,1],[1751,5],[1761,8],[1779,1],[1789,13],[1803,1],[1811,4],[1831,4],[1855,60],[1925,6],[1974,5],[1980,5],[1986,68],[2055,5],[2061,4],[2092,11],[2125,1],[2134,1],[2173,3],[2408,15],[2441,5],[2493,23]]},"1349":{"position":[[0,4]]},"1354":{"position":[[213,6]]},"1356":{"position":[[157,19]]},"1366":{"position":[[0,107],[132,2],[135,8],[159,14],[180,5],[267,34],[308,50]]},"1368":{"position":[[0,49],[50,16],[103,12],[122,5],[128,20]]},"1370":{"position":[[0,18],[110,1],[112,4],[132,2],[139,1],[145,6],[177,1],[360,3],[401,2],[443,7],[573,5],[598,1],[615,4],[620,6],[651,3],[664,8],[747,6],[763,8],[776,1],[794,12],[807,12],[827,4],[862,11],[898,4],[907,3]]},"1372":{"position":[[0,10],[35,20],[291,2],[303,6],[340,5],[360,4],[369,7],[423,12],[445,2]]},"1375":{"position":[[0,14],[19,1],[29,10],[42,1],[46,21],[74,1],[82,13],[96,12],[117,19],[143,6]]},"1382":{"position":[[0,2],[13,12],[32,1],[40,19],[63,2]]},"1386":{"position":[[123,7],[199,18],[268,1],[270,10],[367,10],[387,1],[389,9],[403,23],[565,25],[657,8],[684,1]]},"1389":{"position":[[72,40]]},"1391":{"position":[[0,4]]},"1393":{"position":[[0,4],[37,7],[91,10],[109,4],[123,1],[134,4],[139,1],[150,1],[253,56],[429,5],[435,27],[463,29],[493,9],[510,1]]},"1395":{"position":[[8,1],[17,61],[93,62],[156,5],[162,29],[192,35],[228,4],[233,3],[253,26]]},"1397":{"position":[[6,3],[79,21],[172,8],[235,5],[241,20],[262,4],[274,1],[280,19],[300,4],[305,3],[324,25]]},"1399":{"position":[[75,36],[187,27],[215,5],[221,22],[244,2],[259,19],[279,4],[284,3],[295,1],[318,28]]},"1401":{"position":[[12,1],[21,7],[41,31],[73,45],[119,5],[125,19],[145,9],[167,18],[186,4],[211,38]]},"1403":{"position":[[78,24],[114,5],[132,61],[194,5],[200,24],[225,30],[256,4],[261,3],[287,22]]},"1408":{"position":[[0,9],[25,6],[43,9],[66,9],[80,3],[89,15],[108,8],[117,4],[125,6],[145,7],[153,6],[160,7],[178,3],[195,9],[209,20]]},"1410":{"position":[[0,20],[43,32],[76,2],[97,5],[172,7],[222,31],[267,2],[460,5],[475,6],[495,8],[843,7],[855,8],[888,3],[1008,9],[1022,10],[1039,3],[1079,10],[1105,11],[1123,2],[1153,4],[1218,2],[1274,14],[1302,18],[1321,9],[1337,37],[1375,12],[1392,33],[1440,2],[1460,11],[1771,15],[1791,20],[1825,3],[1860,21],[1882,22],[1920,4],[1993,1],[2001,5],[2036,8],[2074,7]]},"1413":{"position":[[178,10],[205,1],[246,12],[270,12],[294,3],[320,5],[348,31],[468,3],[476,7],[570,9],[589,10],[617,13],[693,1],[794,9],[858,3],[889,5],[899,11],[970,2],[982,8],[995,10],[1015,2],[1033,13],[1056,8],[1069,16],[1086,5],[1096,23],[1129,7],[1175,10],[1186,18],[1227,1],[1274,30],[1310,3],[1327,3]]},"1415":{"position":[[26,17],[70,25],[108,7],[124,3],[140,2],[155,7],[190,19],[283,8],[306,9],[332,5],[344,1],[352,18],[402,4],[420,23],[476,55],[548,18],[577,12],[629,8],[638,26],[678,7]]},"1417":{"position":[[0,7],[23,33],[68,47],[116,14],[131,2],[151,14],[181,13],[195,1],[210,13],[251,32],[292,1],[307,8],[331,4],[363,6],[397,15],[417,3],[459,4],[492,6],[499,6],[510,7],[523,3],[540,3]]},"1423":{"position":[[0,8],[13,7],[36,12],[88,1],[96,11],[111,16],[131,20],[160,4],[189,2],[196,14],[216,16],[233,7],[244,5],[279,2],[286,1],[293,35],[329,19],[349,3],[375,6],[458,8],[467,32],[505,34],[557,5],[567,1],[585,8],[599,2],[608,11],[620,28],[660,1],[673,11],[696,30],[727,10],[765,15]]},"1426":{"position":[[0,4],[11,3],[41,3],[45,4],[59,6],[85,7],[102,25],[137,5],[143,1],[154,6],[196,6],[219,5],[241,2],[260,12]]},"1428":{"position":[[0,3],[25,8],[40,11],[57,54],[117,11],[129,2],[154,2],[177,6],[192,17],[220,15],[244,3],[258,2],[271,16],[298,4],[305,7],[448,8],[465,35],[501,2],[523,3],[557,23],[581,5],[591,5],[605,24],[636,12],[862,9],[895,8],[911,59],[971,5],[986,10],[1075,60],[1331,7],[1343,4],[1362,3],[1370,23],[1399,36],[1436,8],[1450,11],[1554,33],[1710,6],[1732,1],[1752,5],[1762,8],[1780,1],[1790,13],[1804,1],[1812,4],[1832,4],[1856,60],[1926,6],[1975,5],[1981,5],[1987,68],[2056,5],[2062,4],[2093,11],[2126,1],[2135,1],[2174,3],[2409,15],[2442,5],[2494,23]]},"1431":{"position":[[0,4],[5,2]]},"1433":{"position":[[5,40],[66,18],[131,5],[148,84],[255,4],[267,13],[314,7],[326,1],[361,15],[384,16],[475,31],[514,6],[614,35],[715,8],[731,46],[815,5],[824,1],[830,5],[846,19],[880,7],[895,1],[903,1],[921,6],[932,5],[959,31],[1023,72],[1157,4],[1162,8],[1222,15],[1258,7],[1286,7],[1301,10],[1319,4],[1344,1],[1360,7],[1398,2],[1421,2],[1441,7],[1455,2],[1467,3],[1477,12],[1494,5],[1500,7],[1511,3],[1523,13],[1542,3],[1561,13],[1598,12],[1625,10],[1649,22],[1730,51],[1784,16],[1801,5],[1815,11],[1829,3],[1835,16],[1854,3],[1955,3],[1961,16],[1982,2],[2093,4],[2114,1],[2116,6]]},"1435":{"position":[[14,1],[33,4],[44,2],[108,7],[131,11]]},"1437":{"position":[[0,4],[5,2]]},"1439":{"position":[[10,32]]},"1441":{"position":[[244,14],[293,1],[345,1],[384,1],[391,2],[400,1],[422,3],[495,3],[502,1],[515,1],[537,3],[598,3],[609,1],[614,1],[633,3],[661,1],[676,1],[692,2],[708,1],[723,1],[794,3],[874,2],[877,2],[880,1],[888,2],[891,2],[952,2],[955,2],[958,1],[966,2],[969,2],[1030,2],[1033,2],[1036,1],[1044,2],[1047,2],[1272,69]]},"1445":{"position":[[69,11],[81,1],[98,1],[116,1],[140,1],[158,1],[189,1],[216,1],[218,8],[265,3],[354,10],[365,1],[367,10],[378,1],[380,8],[404,1],[425,1],[427,8],[476,3]]},"1447":{"position":[[0,4]]},"1449":{"position":[[0,5],[145,9],[172,12]]},"1452":{"position":[[0,2],[9,8],[48,1],[82,2],[97,9],[113,3],[124,3],[140,1]]},"1454":{"position":[[403,17],[976,36],[1030,4],[1035,8],[1060,45],[1122,15],[1233,49],[1283,28],[1318,20],[1341,12],[1354,34],[1391,11],[1405,27],[1505,23],[1533,5],[1539,22],[1601,24],[1645,30],[1749,9],[1765,4],[1774,1],[1782,14],[1806,20],[1827,1],[2066,2],[2152,5],[2186,36],[2238,8],[2283,51],[2335,13],[2402,4],[2407,8],[2444,9],[2471,13],[2485,19],[2552,7],[2583,7],[2607,8],[2621,4],[2626,32],[2674,3],[2695,8],[2724,1],[2736,3],[2763,1],[2794,5],[2825,4],[2830,12],[2860,12],[3015,15],[3054,3],[3099,1],[3130,5],[3188,1],[3225,5],[3247,10],[3322,5],[3950,7],[3963,22],[4028,7],[4053,9],[4063,4],[4082,10],[4099,4],[4116,11],[4159,17],[4189,10],[4215,13],[4229,2],[4249,2],[4266,2],[4283,2],[4301,7],[4309,11],[4333,8],[4342,10],[4371,9],[4409,9],[4448,14],[4504,12],[4526,21],[4548,12],[4582,12],[4604,7],[4621,15],[4650,7],[4658,22],[5044,11],[5079,38],[5272,5],[5295,20],[5325,1],[5358,2],[5386,2],[5389,15],[5438,24],[5466,8],[5475,3],[5509,5],[5526,1],[5550,8],[5571,1],[5579,3],[5599,4],[5609,3],[5613,8],[5622,7],[5659,18],[5707,24],[5754,6],[5763,2],[5900,13],[5952,25],[6009,13],[6144,7],[6161,5],[6171,7],[6196,1],[6214,9],[6245,1],[6340,2],[6363,1],[6397,10],[6500,2],[6541,1],[6646,2],[6662,6],[6698,5],[6709,8],[6904,48],[6953,28],[6987,8],[7185,30],[7216,5],[7241,6],[7259,6],[7299,7],[7495,4],[7519,2],[7529,14],[7555,4],[7566,5],[7574,9],[7601,4],[7613,3],[7617,1],[7626,1],[7634,5],[7653,41],[7782,14],[7837,11],[7879,2],[7895,2],[7921,1],[8047,5],[8124,12],[8277,2],[8316,1],[8369,1],[8386,2],[8424,1],[8602,10],[8613,2],[8633,13],[8651,8],[8665,15],[8681,6],[8695,3],[8705,3],[8739,3],[8759,3],[8770,5],[8776,5],[8792,1],[8827,16],[8847,6],[8857,9],[8869,7]]},"1461":{"position":[[0,2],[3,14],[18,11],[37,4],[42,9],[95,2],[111,10]]},"1463":{"position":[[0,15],[21,2]]},"1465":{"position":[[0,7],[19,1],[28,3]]},"1467":{"position":[[0,3],[7,6],[18,3],[40,18],[63,5],[73,5],[84,9],[115,10],[154,2]]},"1469":{"position":[[224,4],[235,2],[244,11],[268,2]]},"1471":{"position":[[111,10],[139,3],[155,16]]},"1473":{"position":[[0,11],[25,30],[69,18],[100,29]]},"1475":{"position":[[0,6],[65,1],[148,2],[151,9],[211,1],[412,2],[433,1]]},"1481":{"position":[[0,9],[25,6],[43,9],[66,9],[80,3],[89,15],[108,8],[117,4],[125,6],[145,7],[153,6],[160,7],[178,3],[195,9],[209,20]]},"1483":{"position":[[0,20],[43,32],[76,2],[97,5],[172,7],[222,31],[267,2],[460,5],[475,6],[495,8],[843,7],[855,8],[888,3],[1008,9],[1022,10],[1039,3],[1079,10],[1105,11],[1123,2],[1153,4],[1218,2],[1274,14],[1302,18],[1321,9],[1337,37],[1375,12],[1392,33],[1440,2],[1460,11],[1771,15],[1791,20],[1825,3],[1860,21],[1882,22],[1920,4],[1993,1],[2001,5],[2036,8],[2074,7]]},"1486":{"position":[[178,10],[205,1],[246,12],[270,12],[294,3],[320,5],[348,31],[468,3],[476,7],[570,9],[589,10],[617,13],[693,1],[794,9],[858,3],[889,5],[899,11],[970,2],[982,8],[995,10],[1015,2],[1033,13],[1056,8],[1069,16],[1086,5],[1096,23],[1129,7],[1175,10],[1186,18],[1227,1],[1274,30],[1310,3],[1327,3]]},"1488":{"position":[[26,17],[70,25],[108,7],[124,3],[140,2],[155,7],[190,19],[283,8],[306,9],[332,5],[344,1],[352,18],[402,4],[420,23],[476,55],[548,18],[577,12],[629,8],[638,26],[678,7]]},"1490":{"position":[[0,5],[22,4],[31,6],[38,6],[149,13],[204,6],[233,7],[246,5],[256,8],[270,10],[286,7]]},"1492":{"position":[[0,7],[23,33],[68,47],[116,14],[131,2],[151,14],[181,13],[195,1],[210,13],[251,32],[292,1],[307,8],[331,4],[363,6],[397,15],[417,3],[459,4],[492,6],[499,6],[510,7],[523,3],[540,3]]},"1494":{"position":[[4,12]]},"1496":{"position":[[0,15],[21,2]]},"1498":{"position":[[0,18]]},"1500":{"position":[[24,1],[26,4],[183,7],[191,1],[193,1],[195,4],[281,7],[289,1]]},"1502":{"position":[[75,13]]},"1504":{"position":[[0,16],[26,16]]},"1509":{"position":[[43,6],[56,2],[92,2]]},"1514":{"position":[[148,12],[209,2],[330,5]]},"1517":{"position":[[0,5],[6,2],[11,6],[22,2],[39,14],[54,7],[62,9],[74,5],[86,1],[91,1],[95,8],[104,7],[112,22],[135,41],[177,43],[221,45]]},"1519":{"position":[[0,5],[6,4],[11,30],[42,6],[49,40],[107,4],[112,1],[114,1]]},"1523":{"position":[[0,31]]},"1525":{"position":[[39,1],[41,2],[44,1],[46,23],[70,2],[90,2],[93,1],[103,1],[108,2],[111,17],[143,1],[148,2],[151,2],[163,1],[179,1],[191,1],[193,2],[196,13],[215,1],[227,1],[232,1],[250,2],[276,2],[288,1],[290,2],[347,1],[357,1],[375,1],[382,1],[393,1],[400,1],[402,2],[405,9],[438,1],[440,2],[502,1],[514,1],[522,2],[525,26],[552,1],[561,2],[564,6],[571,1],[595,1],[597,2],[648,1],[667,2],[670,6],[677,1]]},"1527":{"position":[[0,14],[33,7],[126,1],[128,2],[131,1],[133,23],[157,2],[177,2],[180,1],[190,1],[195,2],[198,17],[230,1],[235,2],[238,2],[250,1],[266,1],[278,1],[280,2],[283,13],[302,1],[314,1],[319,1],[337,2],[363,2],[375,1],[377,2],[434,1],[444,1],[462,1],[469,1],[480,1],[487,1],[489,2],[492,9],[525,1],[527,2],[589,1],[601,1],[609,2],[612,26],[639,1],[648,2],[651,6],[658,1],[682,1],[684,2],[735,1],[754,2],[757,6],[764,1],[777,1],[800,2],[806,1],[833,1],[854,2],[862,1],[878,1],[899,1],[932,1],[954,1],[966,1],[985,1],[992,2],[1000,1],[1007,1],[1022,1],[1041,2],[1062,1],[1069,2],[1077,1],[1079,1],[1086,2],[1095,1],[1107,1]]},"1530":{"position":[[0,7],[8,74],[83,9],[93,90],[184,9],[260,10],[271,46],[318,5],[324,36],[361,16],[378,16],[395,8],[404,33],[467,1],[482,1],[519,3],[552,9],[564,12],[603,9],[615,11],[660,3],[666,10],[677,41],[764,5]]},"1532":{"position":[[4,5],[10,8],[19,12],[32,7],[40,4]]},"1534":{"position":[[0,55],[56,20]]},"1536":{"position":[[0,12],[13,4],[18,10],[29,12],[42,9],[52,63],[120,14],[135,13],[149,15],[233,20],[258,72],[331,13],[345,43],[389,68],[458,87],[546,14],[561,9],[571,49],[621,12],[634,28],[663,14],[789,55]]},"1539":{"position":[[0,55],[56,20],[140,12],[153,4],[158,10],[169,12],[182,9],[192,63],[260,14],[275,13],[289,15],[373,20],[398,72],[471,13],[485,43],[529,68],[598,87],[686,14],[701,9],[711,49],[761,12],[774,28],[803,14],[929,55]]},"1541":{"position":[[0,7],[8,74],[83,9],[93,90],[184,9],[260,10],[271,46],[318,5],[324,36],[361,16],[378,16],[395,8],[404,33],[467,1],[482,1],[519,3],[552,9],[564,12],[603,9],[615,11],[660,3],[666,10],[677,41],[764,5]]},"1547":{"position":[[140,15],[156,30],[187,6]]},"1549":{"position":[[162,10],[173,6],[180,6],[187,12],[200,4]]},"1551":{"position":[[0,31],[81,48]]},"1556":{"position":[[600,20],[725,14],[789,10]]},"1558":{"position":[[11,7]]},"1560":{"position":[[3,5],[62,24],[87,33],[198,4],[203,28]]},"1570":{"position":[[134,38]]},"1574":{"position":[[146,6]]},"1576":{"position":[[0,48],[105,15],[121,91]]},"1581":{"position":[[3,7],[47,7]]},"1583":{"position":[[3,7],[47,7]]},"1585":{"position":[[4,14]]},"1587":{"position":[[0,15],[21,2]]},"1590":{"position":[[0,59],[60,30],[91,30]]},"1592":{"position":[[0,4],[5,4],[193,2],[196,13]]},"1594":{"position":[[0,16],[69,1]]}}}],["0",{"_index":15,"t":{"803":{"position":[[115,2],[129,2],[210,2]]},"805":{"position":[[96,2],[167,2],[183,2]]},"814":{"position":[[90,2],[100,2],[144,2],[160,2],[208,4],[231,2],[365,2]]},"822":{"position":[[161,24],[186,3],[257,2],[260,3]]},"824":{"position":[[86,3],[142,1],[152,2],[367,1],[379,2],[468,2]]},"826":{"position":[[48,22]]},"840":{"position":[[2156,5]]},"842":{"position":[[50,5]]},"861":{"position":[[0,17]]},"948":{"position":[[1506,39],[1619,2]]},"1055":{"position":[[1283,2]]},"1057":{"position":[[223,2]]},"1114":{"position":[[131,5]]},"1116":{"position":[[56,5],[62,5]]},"1138":{"position":[[348,2],[354,3],[358,3],[365,2],[980,2],[994,3],[998,3],[1005,3],[1016,4],[1070,2],[1081,2],[1084,3],[1088,3],[1092,2],[1099,3],[1163,3],[1167,3],[1174,3],[1182,2],[1196,4],[3395,2],[3401,3],[3405,3],[3412,2],[3564,2],[3578,3],[3582,3],[3589,3],[3600,4],[3654,2],[3665,2],[3668,3],[3672,3],[3676,2],[3683,3],[3747,3],[3751,3],[3758,3],[3766,2],[3780,4]]},"1181":{"position":[[1283,2]]},"1183":{"position":[[223,2]]},"1201":{"position":[[168,14]]},"1203":{"position":[[39,1],[47,3],[62,3],[175,1],[183,3],[198,3]]},"1251":{"position":[[308,1]]},"1263":{"position":[[568,1],[578,1],[601,1]]},"1270":{"position":[[2260,1]]},"1272":{"position":[[44,74]]},"1275":{"position":[[958,2],[982,3]]},"1320":{"position":[[1506,39],[1619,2]]},"1347":{"position":[[303,1]]},"1428":{"position":[[303,1]]},"1441":{"position":[[648,3],[1217,2],[1229,2],[1253,2]]},"1454":{"position":[[5463,2]]},"1525":{"position":[[105,2],[145,2],[165,2],[244,5],[359,5],[443,33],[600,25]]},"1527":{"position":[[192,2],[232,2],[252,2],[331,5],[446,5],[530,33],[687,25],[857,4],[894,4],[995,4],[1104,2]]}}}],["0&d(u,v)<d_1\\end{cases}\\tag{22}h(u,v)=⎩⎨⎧​1d0​−d1​d(u,v)−d1​​0​d(u,v)>d0​d1​≤d(u,v)≤d0​d(u,v)<d1​​(22",{"_index":630,"t":{"981":{"position":[[688,102]]}}}],["0&d(u,v)>d_0\\end{cases}\\tag{14}h(u,v)={10​d(u,v)≤d0​d(u,v)>d0​​(14)d(u,v)=u2+v2(15)d(u,v)=\\sqrt{u^2+v^2}\\tag{15}d(u,v)=u2+v2​(15",{"_index":596,"t":{"979":{"position":[[142,129]]}}}],["0&d(u,v)>d_1\\end{cases}\\tag{18}h(u,v)=⎩⎨⎧​1d0​−d1​d(u,v)−d1​​0​d(u,v)<d0​d0​≤d(u,v)≤d1​d(u,v)>d1​​(18",{"_index":615,"t":{"979":{"position":[[1171,102]]}}}],["0&d(u,v)\\leq",{"_index":618,"t":{"981":{"position":[[97,12]]}}}],["0.1",{"_index":1875,"t":{"1275":{"position":[[826,5],[850,4]]}}}],["0.1959",{"_index":967,"t":{"1059":{"position":[[1195,8]]},"1185":{"position":[[1195,8]]}}}],["0.2",{"_index":1878,"t":{"1275":{"position":[[855,6]]}}}],["0.2042",{"_index":965,"t":{"1059":{"position":[[1175,7]]},"1185":{"position":[[1175,7]]}}}],["0.3",{"_index":1872,"t":{"1275":{"position":[[802,5]]}}}],["0.7",{"_index":1876,"t":{"1275":{"position":[[832,4]]}}}],["0.8",{"_index":1873,"t":{"1275":{"position":[[814,4],[837,5]]}}}],["0.88",{"_index":1877,"t":{"1275":{"position":[[843,6]]}}}],["0.9",{"_index":1839,"t":{"1270":{"position":[[2737,4]]},"1275":{"position":[[797,4],[808,5]]}}}],["0.94",{"_index":2123,"t":{"1340":{"position":[[630,4]]},"1421":{"position":[[630,4]]}}}],["0.99",{"_index":1874,"t":{"1275":{"position":[[819,6]]}}}],["0.998",{"_index":2377,"t":{"1406":{"position":[[1036,6]]},"1479":{"position":[[1036,6]]}}}],["000",{"_index":2631,"t":{"1454":{"position":[[5563,5]]}}}],["0xff;//d",{"_index":218,"t":{"844":{"position":[[294,8],[1506,8]]}}}],["0维度的相加过程中出现了shape为(3",{"_index":2495,"t":{"1441":{"position":[[1080,24]]}}}],["0阶矩和1",{"_index":644,"t":{"995":{"position":[[167,33]]}}}],["1",{"_index":21,"t":{"805":{"position":[[63,21]]},"812":{"position":[[64,1]]},"814":{"position":[[243,2],[331,2],[386,1]]},"824":{"position":[[200,2],[240,2],[293,3]]},"861":{"position":[[18,12]]},"867":{"position":[[78,57]]},"948":{"position":[[1657,2]]},"975":{"position":[[259,1],[262,1],[265,1],[274,1],[277,1],[280,1],[303,1],[308,1],[311,1],[316,1],[319,1],[324,1]]},"1051":{"position":[[124,3],[399,3],[815,3],[950,3],[1102,3]]},"1055":{"position":[[957,1]]},"1069":{"position":[[336,1],[955,12]]},"1101":{"position":[[308,1]]},"1103":{"position":[[514,1]]},"1129":{"position":[[369,1],[394,10],[418,1]]},"1138":{"position":[[343,4],[351,2],[372,3],[376,2],[379,2],[382,3],[983,3],[987,3],[991,2],[1002,2],[1009,3],[1013,2],[1073,3],[1077,3],[1095,3],[1103,2],[1106,4],[1178,3],[1189,3],[1193,2],[1789,9],[1895,2],[3390,4],[3398,2],[3419,3],[3423,2],[3426,2],[3429,3],[3567,3],[3571,3],[3575,2],[3586,2],[3593,3],[3597,2],[3657,3],[3661,3],[3679,3],[3687,2],[3690,4],[3762,3],[3773,3],[3777,2],[4191,2]]},"1162":{"position":[[87,1]]},"1177":{"position":[[124,3],[399,3],[815,3],[950,3],[1102,3]]},"1181":{"position":[[957,1]]},"1195":{"position":[[336,1],[955,12]]},"1201":{"position":[[131,2],[183,13]]},"1203":{"position":[[187,1]]},"1208":{"position":[[1558,3],[1622,3],[1793,3]]},"1251":{"position":[[721,4]]},"1265":{"position":[[22,20]]},"1270":{"position":[[404,2],[407,2],[2233,2],[2288,2],[2321,2],[2437,2]]},"1275":{"position":[[961,3],[1243,4],[1382,4]]},"1320":{"position":[[1657,2]]},"1340":{"position":[[552,3]]},"1421":{"position":[[552,3]]},"1433":{"position":[[1827,1]]},"1441":{"position":[[0,30],[409,2],[441,5],[521,2],[555,3],[652,3],[663,3]]},"1445":{"position":[[100,3],[269,3],[480,3]]},"1459":{"position":[[84,3]]},"1517":{"position":[[20,1],[25,13],[93,1],[267,37]]},"1527":{"position":[[889,2]]},"1549":{"position":[[134,16]]},"1556":{"position":[[35,7]]},"1558":{"position":[[0,2]]},"1560":{"position":[[0,2]]},"1564":{"position":[[70,2],[190,2]]},"1566":{"position":[[84,2]]},"1568":{"position":[[91,2]]},"1574":{"position":[[100,2]]},"1581":{"position":[[0,2]]},"1583":{"position":[[0,2]]},"1594":{"position":[[46,1]]}}}],["1)[1,−1)的索引区间中的元素值都会加1，而对于某次刷漆终点e的下一个索引为e+1的元素值由于−1",{"_index":71,"t":{"826":{"position":[[193,51]]}}}],["1)[\\frac{d(u,v)}{d_0}]^{2n}}\\tag{16}h(u,v)=1+(2​−1)[d0​d(u,v)​]2n1​(16",{"_index":600,"t":{"979":{"position":[[460,71]]}}}],["1)[\\frac{d_0}{d(u,v)}]^{2n}}\\tag{20}h(u,v)=1+(2​−1)[d(u,v)d0​​]2n1​(20",{"_index":622,"t":{"981":{"position":[[273,71]]}}}],["1)\\tag{13}s(k)=ceil(sk​×l−1)(13",{"_index":583,"t":{"973":{"position":[[210,32]]}}}],["1)的每一列复制三次为(3",{"_index":2498,"t":{"1441":{"position":[[1144,14]]}}}],["1)的矩阵与shape为(1",{"_index":2496,"t":{"1441":{"position":[[1105,15]]}}}],["1.51",{"_index":250,"t":{"844":{"position":[[876,4]]}}}],["1.73",{"_index":2362,"t":{"1406":{"position":[[635,5]]},"1479":{"position":[[635,5]]}}}],["10",{"_index":16,"t":{"803":{"position":[[142,2],[151,3],[160,3]]},"805":{"position":[[109,2],[118,3],[136,3],[219,2]]},"814":{"position":[[196,2]]},"1059":{"position":[[353,2],[894,2],[918,2]]},"1185":{"position":[[353,2],[894,2],[918,2]]},"1201":{"position":[[334,3]]},"1203":{"position":[[325,3]]},"1270":{"position":[[850,4],[2742,2]]},"1342":{"position":[[685,2]]},"1423":{"position":[[685,2]]},"1441":{"position":[[733,6]]},"1454":{"position":[[5559,3]]},"1525":{"position":[[229,2]]},"1527":{"position":[[316,2]]}}}],["100",{"_index":123,"t":{"840":{"position":[[687,4]]},"1201":{"position":[[324,4],[329,4]]},"1203":{"position":[[315,4],[320,4]]},"1285":{"position":[[874,3]]},"1370":{"position":[[874,3]]},"1454":{"position":[[2398,3],[5732,5]]}}}],["1000",{"_index":10,"t":{"803":{"position":[[72,5]]},"1059":{"position":[[136,4],[326,18],[1023,4]]},"1185":{"position":[[136,4],[326,18],[1023,4]]},"1285":{"position":[[857,4]]},"1370":{"position":[[857,4]]},"1454":{"position":[[8660,4]]}}}],["10000",{"_index":950,"t":{"1059":{"position":[[320,5]]},"1185":{"position":[[320,5]]},"1454":{"position":[[7500,6],[7619,6]]}}}],["10241024×1024",{"_index":2591,"t":{"1454":{"position":[[3211,13]]}}}],["1024×10241024",{"_index":2590,"t":{"1454":{"position":[[3190,13]]}}}],["1024，stabl",{"_index":2630,"t":{"1454":{"position":[[5528,11]]}}}],["106",{"_index":138,"t":{"840":{"position":[[957,4]]}}}],["108",{"_index":1449,"t":{"1158":{"position":[[164,3]]}}}],["10px",{"_index":2745,"t":{"1475":{"position":[[367,5]]}}}],["111m",{"_index":2129,"t":{"1340":{"position":[[760,4]]},"1421":{"position":[[760,4]]}}}],["1125",{"_index":861,"t":{"1051":{"position":[[195,5],[211,5],[467,5]]},"1177":{"position":[[195,5],[211,5],[467,5]]}}}],["11×1",{"_index":2861,"t":{"1551":{"position":[[150,18]]}}}],["11×1卷积核，每个filter对上一步的featur",{"_index":1457,"t":{"1162":{"position":[[14,28]]}}}],["12",{"_index":1459,"t":{"1162":{"position":[[109,2]]},"1441":{"position":[[740,6]]}}}],["120",{"_index":1761,"t":{"1270":{"position":[[780,5]]}}}],["120，210都是30的倍数，由于要找最大的，所以答案是210",{"_index":31,"t":{"812":{"position":[[108,33]]}}}],["13",{"_index":2482,"t":{"1441":{"position":[[747,4],[758,5]]}}}],["1313×13个grid",{"_index":2893,"t":{"1558":{"position":[[75,12]]}}}],["14",{"_index":2483,"t":{"1441":{"position":[[752,5],[764,4],[775,5]]}}}],["148",{"_index":154,"t":{"840":{"position":[[1237,8]]}}}],["149",{"_index":111,"t":{"840":{"position":[[462,8]]}}}],["15",{"_index":2484,"t":{"1441":{"position":[[769,5],[781,4]]}}}],["1500",{"_index":2585,"t":{"1454":{"position":[[2616,4],[2820,4]]}}}],["16",{"_index":1758,"t":{"1270":{"position":[[669,3]]},"1340":{"position":[[600,3]]},"1421":{"position":[[600,3]]},"1441":{"position":[[786,7]]}}}],["17",{"_index":2599,"t":{"1454":{"position":[[3602,4]]}}}],["18.65",{"_index":2361,"t":{"1406":{"position":[[626,5]]},"1479":{"position":[[626,5]]}}}],["1\\eta",{"_index":1598,"t":{"1216":{"position":[[544,10]]}}}],["1\\mu_1μ1​和μ2\\mu_2μ2​分别是第一个和第二个高斯分布的均值向量；σ1\\sigma_1σ1​和σ2\\sigma_2σ2​则是它们的协方差矩阵；tr(⋅)\\mathrm{tr}(\\cdot)tr",{"_index":762,"t":{"1024":{"position":[[592,119]]}}}],["1][b,c,1,1]的tensor",{"_index":2904,"t":{"1564":{"position":[[73,43]]}}}],["1][b,c,1,1]的tensor，再送入共享的多层感知机网络进行降维再升维，最后将二者相加再经过sigmoid",{"_index":2908,"t":{"1566":{"position":[[87,72]]}}}],["1][b,c,1,1]的tensor，该tensor",{"_index":2922,"t":{"1574":{"position":[[103,37]]}}}],["1]])12b",{"_index":1699,"t":{"1251":{"position":[[574,7]]}}}],["1]，即reduce了dim=1",{"_index":1870,"t":{"1275":{"position":[[742,19]]}}}],["1_44=fd.img",{"_index":178,"t":{"840":{"position":[[1685,12]]},"846":{"position":[[629,12]]}}}],["1_44=fd_aug.img",{"_index":180,"t":{"840":{"position":[[1724,16]]},"846":{"position":[[668,16]]}}}],["1a",{"_index":1688,"t":{"1251":{"position":[[366,2]]}}}],["1e9",{"_index":498,"t":{"948":{"position":[[1623,4]]},"1320":{"position":[[1623,4]]}}}],["1k−1",{"_index":1625,"t":{"1228":{"position":[[117,4]]}}}],["1t<1",{"_index":1181,"t":{"1099":{"position":[[443,4]]}}}],["1t>1",{"_index":1179,"t":{"1099":{"position":[[394,4]]}}}],["1})(r1​,r2​,…,rk−1",{"_index":2433,"t":{"1413":{"position":[[949,20]]},"1486":{"position":[[949,20]]}}}],["1})(x1​,x2​,...,xt−1​)，具有单向相关性（unidirect",{"_index":2401,"t":{"1410":{"position":[[1163,48]]},"1483":{"position":[[1163,48]]}}}],["1})p(r1​,r2​,…,rk​)=k=1∏k​p(rk​∣r1​,r2​,…,rk−1",{"_index":2428,"t":{"1413":{"position":[[745,48]]},"1486":{"position":[[745,48]]}}}],["1}=\\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t",{"_index":1962,"t":{"1287":{"position":[[97,37]]},"1372":{"position":[[97,37]]}}}],["1}=\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(\\mathbf{x}_{t",{"_index":697,"t":{"1009":{"position":[[254,50]]}}}],["1}\\right)x<t​=(x1​,x2​,…,xt−1",{"_index":2319,"t":{"1386":{"position":[[335,31]]}}}],["1}\\sum_{v=0}^{n",{"_index":574,"t":{"961":{"position":[[1435,15]]}}}],["1}\\sum_{y=0}^{n",{"_index":570,"t":{"961":{"position":[[1224,15]]}}}],["1}][x1​,...,xt−1",{"_index":2452,"t":{"1433":{"position":[[112,18]]}}}],["1}a−1",{"_index":2839,"t":{"1534":{"position":[[134,5]]},"1539":{"position":[[134,5]]}}}],["1}f(u)e^{j\\frac{2\\pi",{"_index":567,"t":{"961":{"position":[[1079,20]]}}}],["1}f(u,v)e^{j2\\pi",{"_index":575,"t":{"961":{"position":[[1451,16]]}}}],["1}f(x)e",{"_index":563,"t":{"961":{"position":[[943,9]]}}}],["1}f(x,y)e",{"_index":571,"t":{"961":{"position":[[1240,11]]}}}],["1}key0,...,keyn−1,query0,...,queryn−1query^{0",{"_index":1407,"t":{"1140":{"position":[[142,47]]}}}],["1}query0,...,queryn−1以及value0,...,valuen−1value^{0",{"_index":1409,"t":{"1140":{"position":[[204,52]]}}}],["1}value0,...,valuen−1",{"_index":1411,"t":{"1140":{"position":[[271,22]]}}}],["1}xt−1​代表当前步骤即将输出的降噪后的图像，ϵθ\\epsilon_\\thetaϵθ​代表nois",{"_index":702,"t":{"1009":{"position":[[539,52]]}}}],["1×11",{"_index":1456,"t":{"1162":{"position":[[0,6]]},"1551":{"position":[[130,12]]}}}],["1×11\\times11×1",{"_index":2411,"t":{"1413":{"position":[[189,15]]},"1486":{"position":[[189,15]]}}}],["1×1×3×4=12(3)1",{"_index":1458,"t":{"1162":{"position":[[65,14]]}}}],["1ηλ<1",{"_index":1599,"t":{"1216":{"position":[[565,15]]}}}],["1−1。这样在所有输入结束后的计算前缀和阶段，在每一个值为[1,−1)[1",{"_index":70,"t":{"826":{"position":[[153,38]]}}}],["1−1加上之前元素所累积的1",{"_index":73,"t":{"826":{"position":[[261,42]]}}}],["1−1而抵消影响（自身值为−1",{"_index":72,"t":{"826":{"position":[[245,15]]}}}],["1−σ)(2",{"_index":1469,"t":{"1201":{"position":[[159,8]]}}}],["1−σ)(2)\\frac{{\\rm",{"_index":1464,"t":{"1201":{"position":[[76,18]]}}}],["1个filter，其中包含3个kernel。每个kernel分别对输入图像的3",{"_index":1451,"t":{"1160":{"position":[[0,55]]}}}],["1，即n0",{"_index":2803,"t":{"1517":{"position":[[80,5]]}}}],["1：a为（4，5）的二维数组，b为（4，1）的二维数组，其中一方维度为1",{"_index":2471,"t":{"1441":{"position":[[141,55]]}}}],["2",{"_index":587,"t":{"975":{"position":[[267,1],[269,1],[271,1],[305,1],[313,1],[321,1]]},"1057":{"position":[[1010,1]]},"1069":{"position":[[490,1],[976,23]]},"1138":{"position":[[362,2],[368,3],[1160,2],[3409,2],[3415,3],[3744,2]]},"1183":{"position":[[1010,1]]},"1195":{"position":[[490,1],[976,23]]},"1208":{"position":[[1651,3],[1834,3]]},"1240":{"position":[[474,20]]},"1251":{"position":[[388,2],[560,2],[571,2],[678,2],[718,2]]},"1275":{"position":[[976,2],[979,2],[1060,2],[1229,2],[1240,2],[1340,2],[1379,2]]},"1340":{"position":[[689,3]]},"1395":{"position":[[6,1],[91,1],[251,1]]},"1415":{"position":[[342,1]]},"1421":{"position":[[689,3]]},"1433":{"position":[[1852,1]]},"1441":{"position":[[447,6],[559,5],[656,4],[667,3],[678,3]]},"1445":{"position":[[104,2],[283,3],[484,2]]},"1488":{"position":[[342,1]]},"1517":{"position":[[72,1]]},"1527":{"position":[[901,2],[946,3]]},"1547":{"position":[[5,1]]},"1556":{"position":[[786,2]]},"1558":{"position":[[8,2]]},"1560":{"position":[[195,2]]},"1581":{"position":[[44,2]]},"1583":{"position":[[44,2]]}}}],["2)中，当i,ji,ji,j",{"_index":1663,"t":{"1240":{"position":[[426,47]]}}}],["2.173",{"_index":1009,"t":{"1059":{"position":[[2342,5]]},"1185":{"position":[[2342,5]]}}}],["2.18",{"_index":2131,"t":{"1340":{"position":[[795,4]]},"1421":{"position":[[795,4]]}}}],["2.296",{"_index":991,"t":{"1059":{"position":[[2148,5]]},"1185":{"position":[[2148,5]]}}}],["2.2960",{"_index":964,"t":{"1059":{"position":[[1166,8]]},"1185":{"position":[[1166,8]]}}}],["2.320",{"_index":996,"t":{"1059":{"position":[[2192,5]]},"1185":{"position":[[2192,5]]}}}],["2.464",{"_index":997,"t":{"1059":{"position":[[2198,5]]},"1185":{"position":[[2198,5]]}}}],["2.482",{"_index":1010,"t":{"1059":{"position":[[2348,5]]},"1185":{"position":[[2348,5]]}}}],["2.489",{"_index":1001,"t":{"1059":{"position":[[2254,5]]},"1185":{"position":[[2254,5]]}}}],["2.517",{"_index":1011,"t":{"1059":{"position":[[2354,5]]},"1185":{"position":[[2354,5]]}}}],["2.578",{"_index":998,"t":{"1059":{"position":[[2204,5]]},"1185":{"position":[[2204,5]]}}}],["2.642",{"_index":992,"t":{"1059":{"position":[[2154,5]]},"1185":{"position":[[2154,5]]}}}],["2.6420",{"_index":966,"t":{"1059":{"position":[[1186,8]]},"1185":{"position":[[1186,8]]}}}],["2.701",{"_index":993,"t":{"1059":{"position":[[2160,5]]},"1185":{"position":[[2160,5]]}}}],["2.715",{"_index":1002,"t":{"1059":{"position":[[2260,5]]},"1185":{"position":[[2260,5]]}}}],["2.851",{"_index":1003,"t":{"1059":{"position":[[2266,5]]},"1185":{"position":[[2266,5]]}}}],["2.95.2",{"_index":120,"t":{"840":{"position":[[658,6]]}}}],["20",{"_index":2365,"t":{"1406":{"position":[[687,3]]},"1479":{"position":[[687,3]]}}}],["200",{"_index":184,"t":{"840":{"position":[[1799,3]]},"846":{"position":[[743,3]]}}}],["2001,2003,2004",{"_index":235,"t":{"844":{"position":[[682,14]]}}}],["2003",{"_index":240,"t":{"844":{"position":[[752,5]]}}}],["2004",{"_index":245,"t":{"844":{"position":[[821,5]]}}}],["201",{"_index":29,"t":{"812":{"position":[[66,14],[142,8]]}}}],["2019",{"_index":978,"t":{"1059":{"position":[[1800,4]]},"1185":{"position":[[1800,4]]},"1354":{"position":[[356,5]]}}}],["201，210，012，021，102，120",{"_index":30,"t":{"812":{"position":[[81,26]]}}}],["201，让数字随意组合，是否能组合出30的倍数，如果能够组合成30",{"_index":28,"t":{"812":{"position":[[4,59]]}}}],["2020",{"_index":2258,"t":{"1354":{"position":[[491,5]]}}}],["2021",{"_index":2273,"t":{"1354":{"position":[[669,5],[814,5]]}}}],["2021】transform",{"_index":399,"t":{"927":{"position":[[44,21]]},"1299":{"position":[[44,21]]}}}],["2022",{"_index":2531,"t":{"1454":{"position":[[107,5]]},"1457":{"position":[[8,5],[38,5]]}}}],["2022发表的diffusionclip使用了diffusion模型代替nada中的stylegan",{"_index":2286,"t":{"1354":{"position":[[952,65]]}}}],["2022的文章few",{"_index":2226,"t":{"1349":{"position":[[68,10]]}}}],["2023",{"_index":2223,"t":{"1349":{"position":[[13,4]]},"1449":{"position":[[11,4]]},"1454":{"position":[[1158,5],[1438,5],[5797,5]]},"1457":{"position":[[119,5]]},"1459":{"position":[[6,5],[94,5]]}}}],["2024",{"_index":2526,"t":{"1449":{"position":[[90,4]]}}}],["203",{"_index":2697,"t":{"1457":{"position":[[194,4]]}}}],["20px",{"_index":2743,"t":{"1475":{"position":[[327,5]]}}}],["21",{"_index":2694,"t":{"1454":{"position":[[8854,2]]}}}],["210",{"_index":32,"t":{"812":{"position":[[151,8]]}}}],["25.5",{"_index":2635,"t":{"1454":{"position":[[5604,4]]}}}],["256",{"_index":1837,"t":{"1270":{"position":[[2652,3]]},"1433":{"position":[[1978,3]]}}}],["256256×256",{"_index":2587,"t":{"1454":{"position":[[2783,10],[3119,10],[3177,10],[5497,11],[5648,10]]}}}],["256×256",{"_index":2132,"t":{"1340":{"position":[[816,7]]},"1406":{"position":[[518,7]]},"1421":{"position":[[816,7]]},"1479":{"position":[[518,7]]}}}],["256×256256",{"_index":2586,"t":{"1454":{"position":[[2765,10],[3101,10],[3159,10],[5479,10],[5630,10]]}}}],["27",{"_index":1453,"t":{"1160":{"position":[[97,2]]}}}],["28",{"_index":1745,"t":{"1270":{"position":[[410,3],[414,3]]}}}],["2\\left(\\sigma_1\\sigma_2\\right)^{\\frac12}\\right)\\tag{1}fid=∥μ1​−μ2​∥22​+tr(σ1​+σ2​−2(σ1​σ2​)21​)(1",{"_index":761,"t":{"1024":{"position":[[493,98]]}}}],["2d",{"_index":2186,"t":{"1347":{"position":[[996,2]]},"1428":{"position":[[997,2]]}}}],["2型文法（上下文无关语法，cfg",{"_index":341,"t":{"871":{"position":[[0,32]]}}}],["2型文法，又称上下文无关文法（context",{"_index":322,"t":{"861":{"position":[[31,22]]}}}],["2的top",{"_index":2933,"t":{"1594":{"position":[[51,8]]}}}],["2的整数次幂表示，如大多图像为彩色rgb图像，256个灰度级，位深度为8（28=2562^8=25628=256），则对于分辨率为256×256的图像来说，需要256×256×3×8位表示，即每一个像素实际上使用24",{"_index":526,"t":{"953":{"position":[[284,118]]}}}],["2（stanford",{"_index":1150,"t":{"1083":{"position":[[726,10]]}}}],["2，iter",{"_index":961,"t":{"1059":{"position":[[1074,11],[1614,11]]},"1185":{"position":[[1074,11],[1614,11]]}}}],["2，vqgan",{"_index":2343,"t":{"1397":{"position":[[316,7]]}}}],["2，抹除所有置信度更小的其iou超过阈值的bbox",{"_index":2887,"t":{"1556":{"position":[[681,39]]}}}],["3",{"_index":366,"t":{"898":{"position":[[256,1]]},"1129":{"position":[[636,1],[665,1],[751,1],[777,1]]},"1138":{"position":[[327,3],[1171,2],[1185,3],[1377,3],[1381,2],[1415,3],[1419,2],[1451,3],[1455,2],[3374,3],[3755,2],[3769,3],[3941,3],[3945,2],[3979,3],[3983,2],[4015,3],[4019,2]]},"1158":{"position":[[142,1],[151,1]]},"1160":{"position":[[77,1],[93,1]]},"1162":{"position":[[96,1]]},"1208":{"position":[[1555,2],[1562,3],[1790,2],[1797,3]]},"1251":{"position":[[563,3],[567,3],[681,4]]},"1275":{"position":[[118,2],[157,2],[287,3],[505,2],[508,2],[511,4],[520,3],[553,2],[591,2],[596,3],[600,2],[603,2],[868,2],[1063,3],[1232,3],[1236,3],[1343,4]]},"1340":{"position":[[897,3]]},"1421":{"position":[[897,3]]},"1433":{"position":[[1833,1]]},"1441":{"position":[[364,3],[368,3],[402,3],[406,2],[454,6],[517,3],[524,2],[565,5],[671,4],[682,3],[1182,12]]},"1445":{"position":[[107,2],[297,3],[487,2]]},"1454":{"position":[[3480,1],[4097,1],[7391,1],[7572,1]]},"1475":{"position":[[88,3]]},"1558":{"position":[[19,2]]},"1560":{"position":[[232,2]]}}}],["3)(3,3)，因此每个权重矩阵的形状应该是(4,3)(4",{"_index":1324,"t":{"1138":{"position":[[533,30]]}}}],["3)(4,3)。为了统一性分析，计key，query以及value各向量维度为numnumnum",{"_index":1325,"t":{"1138":{"position":[[564,49]]}}}],["3)中的a,ba,ba,b可缩小范围，并不用来实现全连接，此时a,ba,ba,b代表着卷积核的感受野，即kernel",{"_index":1666,"t":{"1240":{"position":[[698,97]]}}}],["3)的每一行复制三次为(3",{"_index":2500,"t":{"1441":{"position":[[1167,14]]}}}],["3)的矩阵相加的情况，此时进行广播，将(3",{"_index":2497,"t":{"1441":{"position":[[1121,22]]}}}],["3)，将(1",{"_index":2499,"t":{"1441":{"position":[[1159,7]]}}}],["3.1b",{"_index":2130,"t":{"1340":{"position":[[768,4]]},"1421":{"position":[[768,4]]}}}],["30",{"_index":40,"t":{"814":{"position":[[225,2]]},"1454":{"position":[[8844,2]]}}}],["300",{"_index":962,"t":{"1059":{"position":[[1088,3],[1144,3],[1628,3],[1684,3]]},"1185":{"position":[[1088,3],[1144,3],[1628,3],[1684,3]]}}}],["300000",{"_index":186,"t":{"840":{"position":[[1826,6]]},"846":{"position":[[770,6]]}}}],["30]的tensor",{"_index":2882,"t":{"1556":{"position":[[454,17]]}}}],["30]的tensor（包含所有预测框的坐标、置信度和类别结果），通过解析输出的tensor",{"_index":2868,"t":{"1556":{"position":[[100,51]]}}}],["320",{"_index":860,"t":{"1051":{"position":[[190,4],[206,4],[462,4],[478,4]]},"1177":{"position":[[190,4],[206,4],[462,4],[478,4]]}}}],["326",{"_index":2146,"t":{"1340":{"position":[[1246,4]]},"1421":{"position":[[1246,4]]}}}],["32，iter",{"_index":963,"t":{"1059":{"position":[[1129,12],[1669,12]]},"1185":{"position":[[1129,12],[1669,12]]}}}],["33×3卷积核，padding=1,stride=1padding=1",{"_index":1444,"t":{"1156":{"position":[[101,36]]}}}],["33×3卷积的消融实验发现，7×77",{"_index":2914,"t":{"1570":{"position":[[96,18]]}}}],["350.2",{"_index":2364,"t":{"1406":{"position":[[675,6]]},"1479":{"position":[[675,6]]}}}],["35deg",{"_index":2738,"t":{"1475":{"position":[[253,6]]}}}],["3])2b",{"_index":1689,"t":{"1251":{"position":[[391,5]]}}}],["3][5,5,3",{"_index":1440,"t":{"1156":{"position":[[27,12]]}}}],["3]图像，输出[7",{"_index":2867,"t":{"1556":{"position":[[86,10]]}}}],["3d",{"_index":2469,"t":{"1437":{"position":[[22,2]]},"1490":{"position":[[163,2],[178,2]]}}}],["3~5",{"_index":2640,"t":{"1454":{"position":[[5889,10],[8647,3]]}}}],["3×33",{"_index":1443,"t":{"1156":{"position":[[87,6]]}}}],["3×3×3×4=108(1)3",{"_index":1448,"t":{"1158":{"position":[[119,15]]}}}],["3×3××3=27(2)3",{"_index":1452,"t":{"1160":{"position":[[56,13]]}}}],["3个权重向量做乘法得到3个新的向量，分别为key，query以及value。在本例中将新的向量维度设为3，由于输出的k、q、v矩阵大小均为(3,3)(3",{"_index":1323,"t":{"1138":{"position":[[446,86]]}}}],["3型文法，又称正规文法（regular",{"_index":329,"t":{"861":{"position":[[245,19]]}}}],["3科成绩（假设年级只有a班和b",{"_index":1851,"t":{"1275":{"position":[[211,40]]}}}],["4",{"_index":349,"t":{"893":{"position":[[0,11]]},"898":{"position":[[377,1]]},"1138":{"position":[[331,2],[3378,2]]},"1158":{"position":[[160,1]]},"1162":{"position":[[105,1]]},"1275":{"position":[[284,2],[864,3],[1284,4],[1401,4]]},"1340":{"position":[[1115,3]]},"1421":{"position":[[1115,3]]},"1433":{"position":[[1858,1]]},"1441":{"position":[[461,5],[571,3],[686,5]]}}}],["40",{"_index":2632,"t":{"1454":{"position":[[5573,2]]}}}],["40px",{"_index":2746,"t":{"1475":{"position":[[407,4]]}}}],["414",{"_index":2147,"t":{"1340":{"position":[[1253,4]]},"1421":{"position":[[1253,4]]}}}],["448",{"_index":2866,"t":{"1556":{"position":[[73,7],[81,4]]}}}],["450,000",{"_index":2601,"t":{"1454":{"position":[[3713,7]]}}}],["4696",{"_index":869,"t":{"1051":{"position":[[483,5]]},"1177":{"position":[[483,5]]}}}],["49406",{"_index":859,"t":{"1051":{"position":[[181,8],[453,8]]},"1177":{"position":[[181,8],[453,8]]}}}],["49407",{"_index":863,"t":{"1051":{"position":[[217,6],[489,6]]},"1177":{"position":[[217,6],[489,6]]}}}],["4]))19'''python",{"_index":1708,"t":{"1251":{"position":[[740,15]]}}}],["4][5,5,4]的featur",{"_index":1442,"t":{"1156":{"position":[[65,17]]}}}],["4]])13c",{"_index":1701,"t":{"1251":{"position":[[616,7]]}}}],["4个filter（输出通道为4），每个filter3个kernel（输入通道为3",{"_index":1446,"t":{"1158":{"position":[[0,45]]}}}],["4邻域n4(p)n_4(p)n4​(p)即该像素上下左右的四个点，8邻域n8(p)n_8(p)n8​(p)，对角邻域nd(p)n_d(p)nd​(p",{"_index":527,"t":{"955":{"position":[[0,76]]}}}],["5",{"_index":368,"t":{"898":{"position":[[524,1]]},"1156":{"position":[[24,2],[62,2]]},"1251":{"position":[[416,2],[602,2],[613,2],[697,2],[737,2]]},"1268":{"position":[[97,40]]},"1270":{"position":[[773,1],[777,2],[2254,2]]},"1275":{"position":[[1088,2],[1270,2],[1281,2],[1359,2],[1398,2]]},"1441":{"position":[[467,6],[575,5]]},"1454":{"position":[[8709,1],[8763,1]]}}}],["50",{"_index":2161,"t":{"1342":{"position":[[662,2]]},"1423":{"position":[[662,2]]}}}],["5000",{"_index":959,"t":{"1059":{"position":[[902,4],[968,4]]},"1185":{"position":[[902,4],[968,4]]},"1454":{"position":[[7488,6],[7606,6]]}}}],["50000",{"_index":958,"t":{"1059":{"position":[[877,5]]},"1185":{"position":[[877,5]]}}}],["512",{"_index":877,"t":{"1051":{"position":[[958,4],[1110,4]]},"1055":{"position":[[65,4],[106,4],[174,4],[336,3],[355,3],[466,3]]},"1067":{"position":[[838,4]]},"1177":{"position":[[958,4],[1110,4]]},"1181":{"position":[[65,4],[106,4],[174,4],[336,3],[355,3],[466,3]]},"1193":{"position":[[838,4]]}}}],["512512×512",{"_index":2637,"t":{"1454":{"position":[[5696,10]]}}}],["5125×512",{"_index":1433,"t":{"1152":{"position":[[108,27],[181,57]]}}}],["512×512512",{"_index":2636,"t":{"1454":{"position":[[5678,10]]}}}],["512是clip中的n_dim，token_embed",{"_index":878,"t":{"1051":{"position":[[963,41],[1115,41]]},"1177":{"position":[[963,41],[1115,41]]}}}],["52",{"_index":2597,"t":{"1454":{"position":[[3560,4]]}}}],["539",{"_index":862,"t":{"1051":{"position":[[201,4],[473,4]]},"1177":{"position":[[201,4],[473,4]]}}}],["54",{"_index":1017,"t":{"1059":{"position":[[2510,2]]},"1185":{"position":[[2510,2]]}}}],["54、probabilist",{"_index":2043,"t":{"1327":{"position":[[30,16]]}}}],["55×5",{"_index":1435,"t":{"1152":{"position":[[148,18]]}}}],["58",{"_index":974,"t":{"1059":{"position":[[1713,3],[2507,2]]},"1185":{"position":[[1713,3],[2507,2]]}}}],["586",{"_index":2935,"t":{"1594":{"position":[[71,5]]}}}],["594",{"_index":2934,"t":{"1594":{"position":[[60,8]]}}}],["5×5125",{"_index":1432,"t":{"1152":{"position":[[94,6],[167,6]]}}}],["5×55",{"_index":1434,"t":{"1152":{"position":[[136,4]]}}}],["6",{"_index":307,"t":{"846":{"position":[[1044,8]]},"1129":{"position":[[373,1],[578,10],[602,1],[768,1]]},"1251":{"position":[[605,3],[609,3]]},"1270":{"position":[[571,2]]},"1275":{"position":[[405,3],[1091,3],[1273,3],[1277,3],[1362,4]]},"1441":{"position":[[474,6],[581,5],[695,3]]},"1469":{"position":[[113,1]]}}}],["60",{"_index":1099,"t":{"1071":{"position":[[767,2]]},"1197":{"position":[[767,2]]}}}],["600",{"_index":1255,"t":{"1122":{"position":[[54,3]]}}}],["60个prompts放在同一个python列表中，即每一个prompt作为该列表的字符串元素，输出整个python",{"_index":1090,"t":{"1071":{"position":[[503,66]]},"1197":{"position":[[503,66]]}}}],["60个描述迪士尼人像特有特征的文字prompt",{"_index":1089,"t":{"1071":{"position":[[453,49]]},"1197":{"position":[[453,49]]}}}],["64",{"_index":1981,"t":{"1297":{"position":[[60,2]]},"1382":{"position":[[60,2]]}}}],["6464×64",{"_index":2583,"t":{"1454":{"position":[[2384,13],[2575,7],[2755,7],[3046,7],[3091,7]]}}}],["64×6464",{"_index":2582,"t":{"1454":{"position":[[2369,7],[2560,7],[2740,7],[3031,7],[3076,7]]}}}],["67",{"_index":2404,"t":{"1410":{"position":[[1619,4]]},"1483":{"position":[[1619,4]]}}}],["68、vqvae预训练模型的论文原理及pytorch",{"_index":2335,"t":{"1393":{"position":[[45,32]]}}}],["6]))18(tensor([3",{"_index":1706,"t":{"1251":{"position":[[700,17]]}}}],["6])3c",{"_index":1691,"t":{"1251":{"position":[[419,5]]}}}],["6层encod",{"_index":2852,"t":{"1549":{"position":[[16,11]]}}}],["7",{"_index":1554,"t":{"1208":{"position":[[1566,2],[1801,2]]},"1441":{"position":[[481,5],[587,3],[699,3],[710,3]]},"1469":{"position":[[132,1]]},"1549":{"position":[[0,15]]},"1556":{"position":[[97,2],[451,2]]}}}],["700",{"_index":2748,"t":{"1475":{"position":[[428,4]]}}}],["77",{"_index":855,"t":{"1051":{"position":[[128,3],[403,3],[706,5],[722,2],[745,2],[819,4],[954,3],[1106,3]]},"1177":{"position":[[128,3],[403,3],[706,5],[722,2],[745,2],[819,4],[954,3],[1106,3]]}}}],["775m",{"_index":2136,"t":{"1340":{"position":[[948,4]]},"1421":{"position":[[948,4]]}}}],["77×7",{"_index":2915,"t":{"1570":{"position":[[122,11]]}}}],["77×7卷积与3×33",{"_index":2913,"t":{"1570":{"position":[[77,11]]}}}],["77×7卷积学习特征并降维，最后送入sigmoid",{"_index":2911,"t":{"1568":{"position":[[141,40]]}}}],["77是clip在tokenize方法中缺省的context_length，超过context_length将被truncate，不足的将用0",{"_index":865,"t":{"1051":{"position":[[232,72],[504,72]]},"1177":{"position":[[232,72],[504,72]]}}}],["79",{"_index":1085,"t":{"1069":{"position":[[719,2]]},"1195":{"position":[[719,2]]}}}],["7×77",{"_index":2912,"t":{"1570":{"position":[[0,69]]}}}],["7个损失项是最终融合得到的featur",{"_index":2857,"t":{"1549":{"position":[[86,21]]}}}],["8",{"_index":175,"t":{"840":{"position":[[1666,1]]},"846":{"position":[[610,1]]},"1433":{"position":[[1782,1],[1959,1]]},"1441":{"position":[[487,7],[591,6],[703,4],[714,3],[725,3]]},"1454":{"position":[[5569,1]]}}}],["80.4",{"_index":2363,"t":{"1406":{"position":[[667,4]]},"1479":{"position":[[667,4]]}}}],["84",{"_index":973,"t":{"1059":{"position":[[1707,2],[2504,2]]},"1185":{"position":[[1707,2],[2504,2]]},"1270":{"position":[[817,4]]}}}],["9",{"_index":13,"t":{"803":{"position":[[108,2]]},"1208":{"position":[[1569,4],[1804,4]]},"1275":{"position":[[570,2],[573,2],[576,4]]},"1441":{"position":[[718,4],[729,3]]},"1454":{"position":[[5761,1],[8867,1]]}}}],["928/929",{"_index":2926,"t":{"1592":{"position":[[10,7],[210,94]]}}}],["97",{"_index":2127,"t":{"1340":{"position":[[662,3]]},"1342":{"position":[[563,3]]},"1421":{"position":[[662,3]]},"1423":{"position":[[563,3]]}}}],["9999",{"_index":11,"t":{"803":{"position":[[83,5]]}}}],["9月28日（2023年是9月29日），是研招网全国推免系统开放的日子，也是已经拿到offer",{"_index":2927,"t":{"1592":{"position":[[18,61]]}}}],["_2^2",{"_index":2659,"t":{"1454":{"position":[[6587,6],[8362,6],[8466,6]]}}}],["_2^2\\right]l=ee(x),e(ci​),ct​,ε∼n(0,1),t​[∥ε−ϵθ​(zt​,t,e(ci​),ct​))∥22",{"_index":2622,"t":{"1454":{"position":[[4956,74]]}}}],["__init__(self",{"_index":1047,"t":{"1067":{"position":[[290,14]]},"1193":{"position":[[290,14]]},"1270":{"position":[[311,15],[447,15]]}}}],["__stack_chk_fail",{"_index":152,"t":{"840":{"position":[[1124,18]]}}}],["a)=\\frac{p(a\\mid",{"_index":2060,"t":{"1330":{"position":[[683,16]]}}}],["a)=p(b\\mid",{"_index":2073,"t":{"1332":{"position":[[207,10]]},"1334":{"position":[[248,10]]}}}],["a)\\cdot",{"_index":2071,"t":{"1332":{"position":[[116,7],[218,7]]},"1334":{"position":[[161,7],[259,7]]}}}],["a)move(t,a",{"_index":338,"t":{"867":{"position":[[311,11]]}}}],["a)p(bi​∣a",{"_index":2064,"t":{"1330":{"position":[[812,16]]}}}],["a,b",{"_index":2080,"t":{"1332":{"position":[[448,4],[506,4]]}}}],["a,b)\\cdot",{"_index":2068,"t":{"1332":{"position":[[73,9],[98,9]]},"1334":{"position":[[120,9]]}}}],["a,b)p(b,c∣a)=p(b∣a)⋅p(c∣a,b",{"_index":2075,"t":{"1332":{"position":[[234,28]]}}}],["a,ba,ba,b",{"_index":1660,"t":{"1240":{"position":[[200,40]]}}}],["a.argmax(dim=0",{"_index":1880,"t":{"1275":{"position":[[896,16]]}}}],["a.argmax(dim=1",{"_index":1881,"t":{"1275":{"position":[[913,16]]}}}],["a100",{"_index":2634,"t":{"1454":{"position":[[5590,4],[5745,4],[8750,4]]}}}],["aaa发生后，计算其条件事件bib_ibi​在事件aaa",{"_index":2058,"t":{"1330":{"position":[[568,58]]}}}],["aaa而言，假设有一组互斥且穷尽的条件事件b1,b2,…bnb_{1},b_{2},\\ldot",{"_index":2052,"t":{"1330":{"position":[[256,52]]}}}],["abbr",{"_index":668,"t":{"1005":{"position":[[585,5]]},"1014":{"position":[[59,5]]},"1030":{"position":[[33,5]]}}}],["abil",{"_index":2382,"t":{"1406":{"position":[[1109,7]]},"1479":{"position":[[1109,7]]}}}],["abov",{"_index":1378,"t":{"1138":{"position":[[2627,5]]}}}],["acc",{"_index":1682,"t":{"1251":{"position":[[233,63]]},"1270":{"position":[[1765,5],[1777,6],[2498,3],[2524,3]]}}}],["acc=∑i(predi==yi)len(y)(1)acc",{"_index":1679,"t":{"1251":{"position":[[128,29]]}}}],["accept",{"_index":1144,"t":{"1083":{"position":[[608,44]]}}}],["access",{"_index":2224,"t":{"1349":{"position":[[23,6]]}}}],["accompani",{"_index":1900,"t":{"1279":{"position":[[88,11]]},"1364":{"position":[[88,11]]}}}],["accur",{"_index":1630,"t":{"1228":{"position":[[321,8]]}}}],["accuraci",{"_index":1684,"t":{"1251":{"position":[[313,8]]},"1270":{"position":[[1254,13]]}}}],["accuracy(y_hat",{"_index":1782,"t":{"1270":{"position":[[1301,15],[2111,15]]}}}],["accuracy作为数学上的训练方法，即在训练过程中不使用与acc",{"_index":1678,"t":{"1251":{"position":[[47,41]]}}}],["acc，但并不会将maxim",{"_index":1677,"t":{"1251":{"position":[[0,46]]}}}],["achiev",{"_index":1920,"t":{"1279":{"position":[[856,8]]},"1324":{"position":[[123,7],[1211,7]]},"1340":{"position":[[300,7],[785,9],[1238,7]]},"1364":{"position":[[856,8]]},"1421":{"position":[[300,7],[785,9],[1238,7]]}}}],["acmer，一个cv",{"_index":2936,"t":{"1594":{"position":[[77,33]]}}}],["action=report",{"_index":200,"t":{"840":{"position":[[2066,13]]},"846":{"position":[[1010,13]]}}}],["activ",{"_index":823,"t":{"1041":{"position":[[37,8]]},"1167":{"position":[[37,8]]}}}],["activation=\"fused_lrelu",{"_index":907,"t":{"1055":{"position":[[699,24]]},"1181":{"position":[[699,24]]}}}],["activation='fused_lrelu",{"_index":1070,"t":{"1067":{"position":[[856,25],[999,25]]},"1193":{"position":[[856,25],[999,25]]}}}],["adain",{"_index":932,"t":{"1057":{"position":[[856,5]]},"1183":{"position":[[856,5]]}}}],["adaln",{"_index":2197,"t":{"1347":{"position":[[1461,6],[1805,5]]},"1428":{"position":[[1462,6],[1806,5]]}}}],["adaln⁡(x)=x−μ(x)σ(x)⋅γ+β\\operatorname{adaln}(x)=\\frac{x",{"_index":2200,"t":{"1347":{"position":[[1587,55]]},"1428":{"position":[[1588,55]]}}}],["adaln（adapt",{"_index":2198,"t":{"1347":{"position":[[1468,14]]},"1428":{"position":[[1469,14]]}}}],["adapt",{"_index":818,"t":{"1038":{"position":[[58,10]]},"1057":{"position":[[640,8],[862,9]]},"1164":{"position":[[58,10]]},"1183":{"position":[[640,8],[862,9]]},"1349":{"position":[[101,8]]},"1351":{"position":[[158,10]]},"1415":{"position":[[371,8]]},"1488":{"position":[[371,8]]}}}],["adaption的任务是使在大规模源域图片上训练的生成模型适应到数据有限的目标域中，根据目标域训练资料的大小可以分为few",{"_index":2238,"t":{"1354":{"position":[[17,61]]}}}],["add",{"_index":2539,"t":{"1454":{"position":[[284,4],[5152,3]]}}}],["addit",{"_index":2625,"t":{"1454":{"position":[[5156,10]]}}}],["addition",{"_index":1994,"t":{"1324":{"position":[[192,13]]}}}],["adio",{"_index":223,"t":{"844":{"position":[[394,6],[1606,6]]}}}],["adjust",{"_index":1249,"t":{"1120":{"position":[[80,6]]}}}],["admonit",{"_index":2723,"t":{"1471":{"position":[[26,11],[143,11]]}}}],["advanc",{"_index":2449,"t":{"1431":{"position":[[19,8]]}}}],["advantag",{"_index":1924,"t":{"1279":{"position":[[905,9]]},"1364":{"position":[[905,9]]}}}],["adversari",{"_index":370,"t":{"910":{"position":[[22,12]]},"919":{"position":[[15,11]]},"1354":{"position":[[325,11]]},"1397":{"position":[[140,11],[196,11]]},"1433":{"position":[[794,11]]}}}],["advis",{"_index":1368,"t":{"1138":{"position":[[2208,8],[4367,8]]}}}],["aesthet",{"_index":2140,"t":{"1340":{"position":[[1012,10]]},"1421":{"position":[[1012,10]]}}}],["affirm",{"_index":2105,"t":{"1340":{"position":[[179,11]]},"1421":{"position":[[179,11]]}}}],["afhq",{"_index":1012,"t":{"1059":{"position":[[2360,4],[2379,4],[2402,4],[2653,4],[2672,4],[2695,4]]},"1185":{"position":[[2360,4],[2379,4],[2402,4],[2653,4],[2672,4],[2695,4]]}}}],["aggreg",{"_index":1311,"t":{"1134":{"position":[[283,10]]}}}],["ai",{"_index":1727,"t":{"1265":{"position":[[0,21]]},"1349":{"position":[[41,2]]}}}],["aia^iai在计算注意力分数时，只将aia^iai的query向量与a1a^1a1至aia^{i}ai的iii个key向量做dot",{"_index":461,"t":{"940":{"position":[[1192,112]]},"1312":{"position":[[1192,112]]}}}],["ai论文】yolo",{"_index":2863,"t":{"1556":{"position":[[4,17]]}}}],["alexei",{"_index":2264,"t":{"1354":{"position":[[538,6]]}}}],["alexnet是指2012年由alex",{"_index":1600,"t":{"1219":{"position":[[0,19]]}}}],["align",{"_index":1028,"t":{"1062":{"position":[[417,15],[554,14]]},"1188":{"position":[[417,15],[554,14]]},"1340":{"position":[[1104,10]]},"1349":{"position":[[141,9]]},"1421":{"position":[[1104,10]]},"1457":{"position":[[199,5]]}}}],["all_proxi",{"_index":2761,"t":{"1500":{"position":[[228,9]]}}}],["all_proxy=socks5://127.0.0.1:7890",{"_index":2756,"t":{"1500":{"position":[[59,33]]}}}],["allow",{"_index":1306,"t":{"1134":{"position":[[146,6]]},"1279":{"position":[[389,6]]},"1324":{"position":[[224,6],[765,6]]},"1364":{"position":[[389,6]]},"1406":{"position":[[303,6]]},"1479":{"position":[[303,6]]}}}],["alpha",{"_index":308,"t":{"857":{"position":[[0,66],[117,6]]},"861":{"position":[[160,6]]}}}],["alpha,\\spac",{"_index":311,"t":{"857":{"position":[[103,13]]}}}],["alpha_t",{"_index":2650,"t":{"1454":{"position":[[6343,8],[6520,9],[8295,9]]}}}],["alpha_t}{\\sqrt{1",{"_index":1963,"t":{"1287":{"position":[[143,17]]},"1372":{"position":[[143,17]]}}}],["alpha_{t",{"_index":2687,"t":{"1454":{"position":[[8404,12]]}}}],["alpha_{t}}{\\sqrt{1",{"_index":699,"t":{"1009":{"position":[[313,19]]}}}],["analysi",{"_index":1156,"t":{"1083":{"position":[[926,9]]}}}],["anchor",{"_index":2891,"t":{"1558":{"position":[[22,7]]}}}],["anchor宽高比的聚类，聚类数越大，覆盖的i",{"_index":2897,"t":{"1558":{"position":[[150,43]]}}}],["anchor是通过k",{"_index":2896,"t":{"1558":{"position":[[115,10]]}}}],["anim",{"_index":1800,"t":{"1270":{"position":[[1674,8]]}}}],["animator.add(epoch",{"_index":1826,"t":{"1270":{"position":[[2291,18],[2416,18]]}}}],["announcementbar",{"_index":2729,"t":{"1475":{"position":[[48,16],[71,16]]}}}],["answer",{"_index":1133,"t":{"1083":{"position":[[374,9],[1186,10]]},"1340":{"position":[[191,6]]},"1421":{"position":[[191,6]]}}}],["appli",{"_index":1909,"t":{"1279":{"position":[[402,5]]},"1324":{"position":[[616,5]]},"1340":{"position":[[68,5]]},"1364":{"position":[[402,5]]},"1421":{"position":[[68,5]]}}}],["applic",{"_index":1927,"t":{"1279":{"position":[[1045,13]]},"1324":{"position":[[61,11]]},"1364":{"position":[[1045,13]]},"1514":{"position":[[41,11],[88,11]]}}}],["approach",{"_index":1910,"t":{"1279":{"position":[[587,8]]},"1364":{"position":[[587,8]]}}}],["appropri",{"_index":1333,"t":{"1138":{"position":[[714,11]]}}}],["ar",{"_index":710,"t":{"1014":{"position":[[205,120]]},"1297":{"position":[[7,5]]},"1342":{"position":[[108,2],[128,2],[241,2],[372,2]]},"1382":{"position":[[7,5]]},"1406":{"position":[[325,4],[444,2],[563,2]]},"1408":{"position":[[105,2],[122,2],[175,2]]},"1423":{"position":[[108,2],[128,2],[241,2],[372,2]]},"1433":{"position":[[1152,4],[1508,2]]},"1479":{"position":[[325,4],[444,2],[563,2]]},"1481":{"position":[[105,2],[122,2],[175,2]]}}}],["ar/var",{"_index":2386,"t":{"1406":{"position":[[1391,6]]},"1479":{"position":[[1391,6]]}}}],["architectur",{"_index":1273,"t":{"1122":{"position":[[354,12]]},"1324":{"position":[[962,13]]}}}],["arch系用户通过以下命令即可完成bochs和nasm",{"_index":99,"t":{"838":{"position":[[55,31]]}}}],["argmax",{"_index":884,"t":{"1053":{"position":[[213,6]]},"1179":{"position":[[213,6]]},"1275":{"position":[[612,12],[932,6]]}}}],["argmin",{"_index":1866,"t":{"1275":{"position":[[627,6]]}}}],["args.mix",{"_index":889,"t":{"1055":{"position":[[70,12]]},"1181":{"position":[[70,12]]}}}],["args.source_class",{"_index":847,"t":{"1049":{"position":[[96,18]]},"1175":{"position":[[96,18]]}}}],["args.target_class",{"_index":849,"t":{"1049":{"position":[[155,18]]},"1175":{"position":[[155,18]]}}}],["argument",{"_index":1262,"t":{"1122":{"position":[[201,10]]}}}],["arm",{"_index":708,"t":{"1014":{"position":[[81,6]]}}}],["arm（autoregress",{"_index":709,"t":{"1014":{"position":[[88,18]]}}}],["art",{"_index":1991,"t":{"1324":{"position":[[144,3],[1236,3]]},"1340":{"position":[[321,3]]},"1421":{"position":[[321,3]]}}}],["arxiv",{"_index":397,"t":{"927":{"position":[[5,8]]},"1299":{"position":[[5,8]]},"1327":{"position":[[5,8]]},"1459":{"position":[[0,5],[88,5]]}}}],["ar模型与非自回归nar",{"_index":705,"t":{"1012":{"position":[[0,73]]}}}],["ar）的方法逐token",{"_index":707,"t":{"1014":{"position":[[65,15]]}}}],["asciicod",{"_index":217,"t":{"844":{"position":[[272,9],[481,11],[1484,9],[1696,11]]}}}],["asciicode=='d')//ctrl+d",{"_index":221,"t":{"844":{"position":[[350,23],[1562,23]]}}}],["assign",{"_index":1285,"t":{"1124":{"position":[[17,10]]}}}],["attent",{"_index":454,"t":{"940":{"position":[[800,11],[978,9],[1429,11]]},"948":{"position":[[32,15]]},"1126":{"position":[[34,9],[55,17]]},"1134":{"position":[[7,9],[126,9],[239,9],[252,14],[320,9],[401,34]]},"1138":{"position":[[1670,9],[1840,9],[3224,9],[4041,9],[4136,9]]},"1140":{"position":[[294,12]]},"1142":{"position":[[53,19]]},"1144":{"position":[[12,9],[132,54]]},"1150":{"position":[[29,52]]},"1154":{"position":[[145,33]]},"1312":{"position":[[800,11],[978,9],[1429,11]]},"1320":{"position":[[32,15]]},"1324":{"position":[[930,9]]},"1410":{"position":[[1450,9],[1564,9],[1593,9],[1640,9],[1658,9],[1747,10]]},"1415":{"position":[[567,9]]},"1417":{"position":[[141,9]]},"1483":{"position":[[1450,9],[1564,9],[1593,9],[1640,9],[1658,9],[1747,10]]},"1488":{"position":[[567,9]]},"1492":{"position":[[141,9]]}}}],["attention(q,k,v)=softmax(qktdk)v(1)attention(q,k,v)=\\textit{softmax}(\\frac{qk^t}{\\sqrt{d_k}})v",{"_index":1395,"t":{"1138":{"position":[[3054,94]]}}}],["attention》的讨论环节：teach",{"_index":458,"t":{"940":{"position":[[923,23]]},"1312":{"position":[[923,23]]}}}],["attention之后，通过residu",{"_index":429,"t":{"936":{"position":[[67,22]]},"1308":{"position":[[67,22]]}}}],["attention可以看作是复杂化的cnn，cnn只能在感受野范围内考虑上下文信息，而self",{"_index":1413,"t":{"1142":{"position":[[5,47]]}}}],["attention和mask",{"_index":483,"t":{"948":{"position":[[15,16]]},"1320":{"position":[[15,16]]}}}],["attention应运而生，允许每个位置关注到序列中地所有其他位置。这种全局关联性质使得transform",{"_index":1300,"t":{"1131":{"position":[[479,71]]}}}],["attention接受任意向量数量的向量序列的输入，输出每一个向量所有向量（包括自身）的注意力分数。这使得self",{"_index":1312,"t":{"1134":{"position":[[343,57]]}}}],["attention的计算中应用一个掩码（mask",{"_index":460,"t":{"940":{"position":[[1111,80]]},"1312":{"position":[[1111,80]]}}}],["attention的输入是向量序列，其向量数量是任意的，计算每个输入向量之间的注意力分数。在本例中输入向量个数为3，同时为了统一性分析，计输入向量个数为batchbatchbatch",{"_index":1318,"t":{"1138":{"position":[[192,93]]}}}],["attention的输入是时间步sss之前decod",{"_index":465,"t":{"940":{"position":[[1373,43]]},"1312":{"position":[[1373,43]]}}}],["attention的输出向量维度与valu",{"_index":1375,"t":{"1138":{"position":[[2499,48]]}}}],["attention，token",{"_index":2442,"t":{"1415":{"position":[[532,15]]},"1488":{"position":[[532,15]]}}}],["attention，则对每个输入向量生成对应的key，query和value后，再次使用nnn个可学习的权重矩阵生成nnn个不同的key0,...,keyn−1key^{0",{"_index":1405,"t":{"1140":{"position":[[41,88]]}}}],["at）以及non",{"_index":436,"t":{"938":{"position":[[56,8]]},"1310":{"position":[[56,8]]}}}],["audio",{"_index":1271,"t":{"1122":{"position":[[322,5]]}}}],["augment",{"_index":1608,"t":{"1223":{"position":[[157,12]]},"1354":{"position":[[778,12]]}}}],["augustu",{"_index":2245,"t":{"1354":{"position":[[250,8]]}}}],["auto",{"_index":667,"t":{"1005":{"position":[[571,4]]},"1030":{"position":[[19,4]]},"1527":{"position":[[1013,5]]}}}],["autoencod",{"_index":1988,"t":{"1324":{"position":[[86,13],[670,13]]},"1393":{"position":[[351,77],[529,44]]}}}],["autoencoder(va",{"_index":2333,"t":{"1393":{"position":[[13,17]]}}}],["autoencoder（va",{"_index":2454,"t":{"1433":{"position":[[289,17]]}}}],["autoencoder）提出了另一种方法，通过离散的潜在空间来建模数据。vq",{"_index":2339,"t":{"1393":{"position":[[210,38]]}}}],["autoregress",{"_index":1893,"t":{"1277":{"position":[[38,23]]},"1279":{"position":[[31,14],[256,14],[408,14],[727,14],[772,14],[980,14]]},"1281":{"position":[[144,14]]},"1340":{"position":[[217,14]]},"1342":{"position":[[21,14],[49,14]]},"1347":{"position":[[4,14]]},"1362":{"position":[[38,23]]},"1364":{"position":[[31,14],[256,14],[408,14],[727,14],[772,14],[980,14]]},"1366":{"position":[[144,14]]},"1384":{"position":[[0,20]]},"1406":{"position":[[18,14],[94,14],[310,14]]},"1408":{"position":[[10,14]]},"1410":{"position":[[1043,14],[1058,20],[1829,14],[1905,14]]},"1413":{"position":[[1160,14]]},"1415":{"position":[[614,14]]},"1417":{"position":[[8,14],[166,14],[316,14]]},"1421":{"position":[[217,14]]},"1423":{"position":[[21,14],[49,14]]},"1428":{"position":[[4,14]]},"1433":{"position":[[1546,14]]},"1479":{"position":[[18,14],[94,14],[310,14]]},"1481":{"position":[[10,14]]},"1483":{"position":[[1043,14],[1058,20],[1829,14],[1905,14]]},"1486":{"position":[[1160,14]]},"1488":{"position":[[614,14]]},"1492":{"position":[[8,14],[166,14],[316,14]]}}}],["autoregressive（ar",{"_index":2450,"t":{"1433":{"position":[[46,19]]}}}],["autoregressive（非自回归，abbr",{"_index":437,"t":{"938":{"position":[[65,25]]},"1310":{"position":[[65,25]]}}}],["autoregressive，简称ar",{"_index":2311,"t":{"1386":{"position":[[0,122]]}}}],["averag",{"_index":1629,"t":{"1228":{"position":[[294,8]]}}}],["avoid",{"_index":1393,"t":{"1138":{"position":[[2952,5]]}}}],["a→b→ca\\to",{"_index":2091,"t":{"1334":{"position":[[0,42]]}}}],["a∈vn",{"_index":324,"t":{"861":{"position":[[109,5],[222,6]]}}}],["a为（3，4，5）的三维数据，b为（4，5）的二维数组。由于a和b的后缘维度都为（4，5），所以可以进行广播。同理，当a为（3，4）的二维数组，b为（4，）的数组，他们的后缘维度都是4",{"_index":2470,"t":{"1441":{"position":[[31,109]]}}}],["a是a班4位同学3科成绩，b是这4名同学其他3门课的成绩，拼接后代表这4名同学的6",{"_index":1856,"t":{"1275":{"position":[[325,47]]}}}],["a班4位同学，每位同学3",{"_index":1847,"t":{"1275":{"position":[[123,15]]}}}],["a而言，假设有一组互斥且穷尽的条件事件b，则事件a的概率等于事件a",{"_index":2827,"t":{"1530":{"position":[[194,65]]},"1541":{"position":[[194,65]]}}}],["a，若存在方阵b使得ab=ba=单位方阵i，则方阵b为方阵a的逆矩阵，记为a−1a",{"_index":2838,"t":{"1534":{"position":[[77,56]]},"1539":{"position":[[77,56]]}}}],["b",{"_index":57,"t":{"824":{"position":[[94,2],[122,3],[136,2]]},"986":{"position":[[0,26]]},"1001":{"position":[[893,2]]},"1214":{"position":[[48,2]]},"1216":{"position":[[69,2],[250,2]]},"1263":{"position":[[574,1],[628,1],[632,1]]},"1275":{"position":[[139,1],[198,3],[312,3],[516,1],[1067,1],[1106,2],[1248,1],[1300,2]]},"1290":{"position":[[40,1]]},"1330":{"position":[[79,1]]},"1375":{"position":[[40,1]]},"1445":{"position":[[124,4],[287,4],[496,4]]},"1454":{"position":[[3528,3]]},"1530":{"position":[[550,1],[613,1],[664,1]]},"1541":{"position":[[550,1],[613,1],[664,1]]}}}],["b)14for",{"_index":1702,"t":{"1251":{"position":[[633,7]]}}}],["b)4for",{"_index":1693,"t":{"1251":{"position":[[434,6]]}}}],["b)=\\frac{p(a,b)}{p(b)}p(a∣b)=p(b)p(a,b",{"_index":2049,"t":{"1330":{"position":[[135,40]]}}}],["b)\\cdot",{"_index":2095,"t":{"1334":{"position":[[145,7]]}}}],["b)p(a∣b",{"_index":2047,"t":{"1330":{"position":[[63,15]]}}}],["b)p(b,c∣a)=p(b∣a)⋅p(c∣b",{"_index":2097,"t":{"1334":{"position":[[275,24]]}}}],["b)}{\\partial",{"_index":1590,"t":{"1216":{"position":[[319,12]]}}}],["b,c,h,w][b",{"_index":2902,"t":{"1564":{"position":[[0,23],[117,29]]},"1566":{"position":[[0,17]]},"1568":{"position":[[0,17]]}}}],["b1,b2b_1",{"_index":2183,"t":{"1347":{"position":[[820,15]]},"1428":{"position":[[820,15]]}}}],["b[i],e[i](0<=b[i]<=e[i]<=200000",{"_index":51,"t":{"822":{"position":[[104,56]]}}}],["b\\to",{"_index":2092,"t":{"1334":{"position":[[43,4]]}}}],["b_2b1​,b2",{"_index":2184,"t":{"1347":{"position":[[836,10]]},"1428":{"position":[[836,10]]}}}],["b_i)\\cdot",{"_index":2055,"t":{"1330":{"position":[[437,9],[700,9]]}}}],["b_i)p(a∣bi",{"_index":2066,"t":{"1330":{"position":[[844,16]]}}}],["b_t)}{\\partial",{"_index":1595,"t":{"1216":{"position":[[480,14]]}}}],["b_{n}b1​,b2​,…bn​构成一个完备事件组，则事件aaa的概率等于事件aaa在每个条件事件bib_ibi",{"_index":2053,"t":{"1330":{"position":[[309,79]]}}}],["background",{"_index":2737,"t":{"1475":{"position":[[213,11]]}}}],["bang",{"_index":2525,"t":{"1449":{"position":[[80,4]]}}}],["bao",{"_index":2279,"t":{"1354":{"position":[[718,3]]}}}],["bar{\\alpha}_t",{"_index":1950,"t":{"1285":{"position":[[501,15]]},"1287":{"position":[[161,16]]},"1370":{"position":[[501,15]]},"1372":{"position":[[161,16]]}}}],["bar{\\alpha}_{t}}\\boldsymbol{\\epsilon},t)\\right\\|^{2}\\tag{1}∇θ​​ϵ−ϵθ​(αˉt​​x0​+1−αˉt​​ϵ,t)​2(1",{"_index":685,"t":{"1007":{"position":[[345,95]]}}}],["bar{\\alpha}_{t}}}\\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_{t},t)\\right)+\\sigma_{t}\\mathbf{z}\\tag{2}xt−1​=αt​​1​(xt​−1−αˉt​​1−αt​​ϵθ​(xt​,t))+σt​z(2",{"_index":700,"t":{"1009":{"position":[[333,150]]}}}],["base",{"_index":1160,"t":{"1083":{"position":[[1171,5]]},"1324":{"position":[[1522,5]]},"1449":{"position":[[115,5]]},"1454":{"position":[[1184,5],[1457,5]]},"1490":{"position":[[227,5]]}}}],["based的文字生成模型有很多，如gpt模型，大多使用自回归（autoregress",{"_index":706,"t":{"1014":{"position":[[12,46]]}}}],["baselin",{"_index":2256,"t":{"1354":{"position":[[441,8]]},"1406":{"position":[[566,8]]},"1479":{"position":[[566,8]]}}}],["bash的配置文件：~/.bashrc",{"_index":2766,"t":{"1502":{"position":[[55,19]]}}}],["basic",{"_index":1614,"t":{"1226":{"position":[[108,5]]}}}],["batch",{"_index":1811,"t":{"1270":{"position":[[1910,6],[2224,6],[2265,5],[2312,6]]},"1454":{"position":[[5515,5]]}}}],["batch)(batch,batch",{"_index":1353,"t":{"1138":{"position":[[1630,20]]}}}],["batch_siz",{"_index":891,"t":{"1055":{"position":[[93,12],[161,12]]},"1059":{"position":[[1061,10],[1116,10],[1601,10],[1656,10]]},"1181":{"position":[[93,12],[161,12]]},"1185":{"position":[[1061,10],[1116,10],[1601,10],[1656,10]]},"1270":{"position":[[2639,10]]},"1447":{"position":[[192,11]]}}}],["batch_size，比如torch.nn.linear的输入(batch_size,in_features)，torch.nn.conv2d的输入(batch_s",{"_index":2514,"t":{"1447":{"position":[[44,119]]}}}],["batchbatchbatch个向量计算得到的注意力分数，都要与其对应的value向量相乘，计算加权的注意力分数。最终的注意力分数矩阵的形状应为(batch,num)(batch",{"_index":1364,"t":{"1138":{"position":[[2012,100]]}}}],["batch中有大量样本均存在这种情况，此时acc有显著提升而网络的权重的更新极小，此时，与acc有关的loss",{"_index":1712,"t":{"1251":{"position":[[828,129]]}}}],["batteri",{"_index":2794,"t":{"1514":{"position":[[201,7]]}}}],["bbb",{"_index":1580,"t":{"1214":{"position":[[181,10]]},"1263":{"position":[[178,38]]}}}],["bbox与其他所有置信度更小的bbox做iou判断，若iou大于设置的阈值，则抹除置信度小的bbox",{"_index":2886,"t":{"1556":{"position":[[621,59]]}}}],["bbox包含(x",{"_index":2876,"t":{"1556":{"position":[[290,11]]}}}],["bbox的置信度与其父grid",{"_index":2885,"t":{"1556":{"position":[[549,17]]}}}],["bbox都会在loss",{"_index":2888,"t":{"1556":{"position":[[740,19]]}}}],["be",{"_index":107,"t":{"840":{"position":[[330,5]]},"1228":{"position":[[209,5]]}}}],["becom",{"_index":1392,"t":{"1138":{"position":[[2887,7]]},"1324":{"position":[[1132,7]]}}}],["befor",{"_index":1338,"t":{"1138":{"position":[[828,6]]}}}],["begin",{"_index":66,"t":{"824":{"position":[[303,6],[337,7],[357,6],[406,6]]},"940":{"position":[[221,8]]},"1312":{"position":[[221,8]]}}}],["begin[i]+1",{"_index":54,"t":{"822":{"position":[[289,37]]}}}],["begin[i],end[i]（0<=begin[i]<=end[i]<=200000",{"_index":52,"t":{"822":{"position":[[190,66]]}}}],["begin{cas",{"_index":1478,"t":{"1203":{"position":[[25,13],[161,13]]}}}],["begin符号又叫start符号或sos符号（start",{"_index":448,"t":{"940":{"position":[[520,28]]},"1312":{"position":[[520,28]]}}}],["begin符号是在lexicon中添加的特殊符号，用来表示decoder生成的开始。begin符号通常被嵌入到一个低维的连续向量空间中，这个向量空间是通过嵌入层（embed",{"_index":446,"t":{"940":{"position":[[394,90]]},"1312":{"position":[[394,90]]}}}],["behind",{"_index":1616,"t":{"1226":{"position":[[119,6]]}}}],["below",{"_index":1275,"t":{"1122":{"position":[[397,6]]}}}],["benchmark",{"_index":1124,"t":{"1083":{"position":[[86,10],[215,9],[541,40]]},"1340":{"position":[[678,10],[824,11]]},"1406":{"position":[[526,10]]},"1421":{"position":[[678,10],[824,11]]},"1479":{"position":[[526,10]]}}}],["benchmark：fid与clip",{"_index":751,"t":{"1024":{"position":[[0,37]]}}}],["bert",{"_index":1105,"t":{"1077":{"position":[[0,18]]},"1079":{"position":[[194,4],[237,4],[281,4],[349,4]]},"1083":{"position":[[0,11],[936,86]]},"1085":{"position":[[317,4],[374,4],[490,7],[586,7]]}}}],["bert也可以用来完成截取式问答任务，提供一篇文章以及问题，要求输出两个integ",{"_index":1161,"t":{"1083":{"position":[[1197,66]]}}}],["bert在大规模语料库（corpus）上预训练后，输出的向量表示了对应输入token的意思，特别地，是考虑了上下文信息的意思。比如，对于水果的苹果已经苹果公司的苹果，bert对一样的苹果有不同的输出。即对一个处在不同上下文信息中的相同词汇，bert有不同的embedding。类似于word",{"_index":1165,"t":{"1085":{"position":[[0,145]]}}}],["bert模型的核心就是通过预训练来学习上下文信息，进而对每个输入token生成相应的向量表示。这个向量表示考虑了上下文信息，所以对于处在不同上下文中的相同词汇，bert会有不同的embed",{"_index":1167,"t":{"1085":{"position":[[202,99]]}}}],["bert模型的自监督性质主要体现在其训练数据并不需要人为标注label，而是通过对输入句子中的部分词汇做mask，将输入数据的部分内容使用speci",{"_index":1112,"t":{"1081":{"position":[[15,76]]}}}],["bert的损失函数主要是mask",{"_index":1115,"t":{"1081":{"position":[[174,18]]}}}],["bert的结构其实是transformer的encoder部分，仅使用encod",{"_index":1106,"t":{"1079":{"position":[[0,49]]}}}],["bert（bidirect",{"_index":1107,"t":{"1079":{"position":[[50,18]]}}}],["best",{"_index":979,"t":{"1059":{"position":[[1805,4]]},"1185":{"position":[[1805,4]]}}}],["beta",{"_index":319,"t":{"859":{"position":[[43,5],[82,16]]},"861":{"position":[[183,5]]},"1347":{"position":[[1781,7]]},"1428":{"position":[[1782,7]]}}}],["beta,\\spac",{"_index":318,"t":{"859":{"position":[[30,12]]},"861":{"position":[[147,12]]}}}],["better",{"_index":2799,"t":{"1514":{"position":[[310,6]]}}}],["between",{"_index":1389,"t":{"1138":{"position":[[2817,7]]},"1324":{"position":[[821,7]]}}}],["beyond",{"_index":1993,"t":{"1324":{"position":[[184,7]]}}}],["bhpf",{"_index":620,"t":{"981":{"position":[[197,16]]}}}],["bias",{"_index":2111,"t":{"1340":{"position":[[271,6]]},"1421":{"position":[[271,6]]}}}],["bib_ibi​）推“果”（结果事件aaa",{"_index":2057,"t":{"1330":{"position":[[480,80]]}}}],["bidirect",{"_index":2402,"t":{"1410":{"position":[[1235,26]]},"1483":{"position":[[1235,26]]}}}],["big",{"_index":1588,"t":{"1216":{"position":[[291,5]]},"1449":{"position":[[76,3]]},"1512":{"position":[[101,3]]}}}],["big(l(w",{"_index":1587,"t":{"1216":{"position":[[240,9]]}}}],["bilinear",{"_index":578,"t":{"968":{"position":[[8,16]]}}}],["bits/stdc++.h",{"_index":3,"t":{"803":{"position":[[9,15]]},"814":{"position":[[9,15]]},"824":{"position":[[9,15]]},"1527":{"position":[[50,15]]}}}],["block",{"_index":2847,"t":{"1547":{"position":[[127,8]]}}}],["block。每一个block",{"_index":434,"t":{"936":{"position":[[215,35]]},"1308":{"position":[[215,35]]}}}],["blpf",{"_index":598,"t":{"979":{"position":[[341,16]]}}}],["blue",{"_index":2548,"t":{"1454":{"position":[[584,4],[654,4]]}}}],["bn",{"_index":2890,"t":{"1558":{"position":[[3,4]]}}}],["boch",{"_index":97,"t":{"838":{"position":[[18,8],[94,5]]},"840":{"position":[[217,20],[1357,13],[1630,5],[1935,5]]},"846":{"position":[[574,5],[879,5],[1038,5],[1092,20]]}}}],["bochs.out",{"_index":182,"t":{"840":{"position":[[1762,11],[1965,10]]},"846":{"position":[[706,11],[909,10]]}}}],["bochs.out、depend.mak以及fd.img文件，bochs.out文件是日志输出文件，depend.mak是编译中间生成的文件，最终生成的fd.img是最重要的geekos映像文件，有了它才能使用bochs运行geeko",{"_index":305,"t":{"846":{"position":[[87,137]]}}}],["bochsrc",{"_index":163,"t":{"840":{"position":[[1434,8]]},"846":{"position":[[378,8]]}}}],["bochs是一个x86",{"_index":88,"t":{"832":{"position":[[0,20]]}}}],["bochs的镜像文件fd.img",{"_index":94,"t":{"834":{"position":[[167,22]]}}}],["bochs的镜像文件fd.img以构建geeko",{"_index":104,"t":{"840":{"position":[[113,33]]}}}],["bochs运行geekos系统了，可以说bochs的运行依赖两个文件，一个是配置文件.bochsrc，一个是映像文件fd.img，映像文件的加载路径需要在.bochsrc",{"_index":306,"t":{"846":{"position":[[241,123]]}}}],["bochs运行所必须的文件,也是geeko",{"_index":95,"t":{"834":{"position":[[190,34]]}}}],["boldsymbol{\\epsilon}_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1",{"_index":684,"t":{"1007":{"position":[[268,76]]}}}],["boost",{"_index":2023,"t":{"1324":{"position":[[883,8]]}}}],["boot",{"_index":176,"t":{"840":{"position":[[1668,5]]},"846":{"position":[[612,5]]}}}],["boot_info",{"_index":278,"t":{"844":{"position":[[1850,10]]}}}],["bootinfo",{"_index":279,"t":{"844":{"position":[[1861,9]]}}}],["bound",{"_index":2028,"t":{"1324":{"position":[[1087,8]]}}}],["box",{"_index":2029,"t":{"1324":{"position":[[1096,5]]}}}],["box预测框（bbox），每个bbox的中心点都落在该grid",{"_index":2874,"t":{"1556":{"position":[[234,31]]}}}],["break",{"_index":61,"t":{"824":{"position":[[157,6],[384,6]]}}}],["bs",{"_index":2792,"t":{"1514":{"position":[[191,2]]}}}],["b与e，b作为开始刷的节点索引令前缀和数组中对应元素的值+1+1+1，e+1作为刷墙结束的下一个节点的索引令对应的值−1",{"_index":69,"t":{"826":{"position":[[71,81]]}}}],["b班4位同学，每位同学3",{"_index":1848,"t":{"1275":{"position":[[162,15]]}}}],["b（semant",{"_index":1140,"t":{"1083":{"position":[[511,10]]}}}],["c",{"_index":43,"t":{"816":{"position":[[0,5]]},"840":{"position":[[642,1],[740,1],[945,1],[1219,1]]},"844":{"position":[[647,1],[678,3],[748,3],[817,3],[1730,1]]},"846":{"position":[[1090,1]]},"1043":{"position":[[104,1],[115,1]]},"1169":{"position":[[104,1],[115,1]]},"1275":{"position":[[178,1],[1095,1],[1118,2],[1289,1],[1312,2]]},"1290":{"position":[[44,1]]},"1375":{"position":[[44,1]]},"1410":{"position":[[957,27]]},"1445":{"position":[[129,4],[301,4],[501,4]]},"1447":{"position":[[164,2]]},"1454":{"position":[[3701,3]]},"1483":{"position":[[957,27]]},"1564":{"position":[[24,2],[67,2],[147,2]]},"1566":{"position":[[18,2],[81,2]]},"1568":{"position":[[18,2]]},"1574":{"position":[[50,2],[97,2]]}}}],["c)五种信息，其中x",{"_index":2877,"t":{"1556":{"position":[[311,11]]}}}],["c:15",{"_index":1703,"t":{"1251":{"position":[[646,4]]}}}],["c:5",{"_index":1694,"t":{"1251":{"position":[[446,3]]}}}],["c=γ(p)\\mathbf{c",{"_index":2644,"t":{"1454":{"position":[[6179,16]]}}}],["c\\ell_cℓc",{"_index":2214,"t":{"1347":{"position":[[2177,11]]},"1428":{"position":[[2178,11]]}}}],["c\\mathbf{c}c",{"_index":2663,"t":{"1454":{"position":[[6682,15]]}}}],["c_i",{"_index":1644,"t":{"1235":{"position":[[62,3],[431,3]]}}}],["c_ico​×ci",{"_index":1640,"t":{"1233":{"position":[[105,15]]},"1235":{"position":[[122,10]]}}}],["c_ihw×ci​，权重形状为co×cic_o",{"_index":1639,"t":{"1233":{"position":[[74,23]]}}}],["c_k",{"_index":1196,"t":{"1101":{"position":[[579,4]]}}}],["c_kpi′​=∑wj​∈ck​​pj​pi",{"_index":1199,"t":{"1101":{"position":[[611,24]]}}}],["c_p",{"_index":1217,"t":{"1103":{"position":[[688,4]]}}}],["c_ppi′​=∑wj​∈cp​​pj​pi",{"_index":1218,"t":{"1103":{"position":[[720,24]]}}}],["c_t",{"_index":2616,"t":{"1454":{"position":[[4827,4]]}}}],["c_t\\right)\\right",{"_index":2621,"t":{"1454":{"position":[[4938,17]]}}}],["c_{pr",{"_index":2690,"t":{"1454":{"position":[[8449,7]]}}}],["c_{pr})xpr​=x^θ​(zt1​​,cpr",{"_index":2680,"t":{"1454":{"position":[[7947,37]]}}}],["can't",{"_index":2718,"t":{"1469":{"position":[[171,5]]},"1514":{"position":[[106,5]]}}}],["canni",{"_index":650,"t":{"999":{"position":[[261,10]]},"1549":{"position":[[151,10]]},"1560":{"position":[[158,36]]}}}],["caption",{"_index":2463,"t":{"1435":{"position":[[6,7]]},"1454":{"position":[[3518,9],[3649,9],[4020,7],[4074,7],[4135,9],[4151,7],[4207,7],[4275,7],[4293,7]]}}}],["casc",{"_index":2937,"t":{"1594":{"position":[[111,44]]}}}],["case",{"_index":1915,"t":{"1279":{"position":[[701,6]]},"1364":{"position":[[701,6]]}}}],["categor",{"_index":1904,"t":{"1279":{"position":[[204,11],[478,11]]},"1364":{"position":[[204,11],[478,11]]}}}],["causal",{"_index":1438,"t":{"1154":{"position":[[130,14]]},"1410":{"position":[[1443,6],[1633,6],[1735,6]]},"1415":{"position":[[590,6]]},"1417":{"position":[[134,6]]},"1483":{"position":[[1443,6],[1633,6],[1735,6]]},"1488":{"position":[[590,6]]},"1492":{"position":[[134,6]]}}}],["caution",{"_index":347,"t":{"883":{"position":[[11,7],[57,7]]}}}],["ca→b→c",{"_index":2093,"t":{"1334":{"position":[[48,10]]}}}],["cbam是通道+空间注意力机制（senet",{"_index":2901,"t":{"1562":{"position":[[82,30]]}}}],["cc_general_opt",{"_index":112,"t":{"840":{"position":[[471,15],[522,15]]}}}],["ccc",{"_index":2166,"t":{"1345":{"position":[[203,6]]},"1426":{"position":[[203,6]]}}}],["cd",{"_index":85,"t":{"830":{"position":[[321,16]]}}}],["cdot",{"_index":2081,"t":{"1332":{"position":[[453,5],[491,5]]},"1347":{"position":[[1210,5],[1251,5],[1662,5]]},"1428":{"position":[[1211,5],[1252,5],[1663,5]]}}}],["cd命令进入geeko",{"_index":93,"t":{"834":{"position":[[93,20]]}}}],["cell",{"_index":2884,"t":{"1556":{"position":[[506,12],[567,22]]}}}],["cell。在yolov1中s=7s=7s=7",{"_index":2871,"t":{"1556":{"position":[[184,22]]}}}],["cell中。在yolov1中b=2b=2b=2",{"_index":2875,"t":{"1556":{"position":[[266,23]]}}}],["cell生成5个anchor",{"_index":2895,"t":{"1558":{"position":[[100,14]]}}}],["cell选出条件类别概率最大的类别，因此每个grid",{"_index":2883,"t":{"1556":{"position":[[479,26]]}}}],["cell预测出bbb个bound",{"_index":2873,"t":{"1556":{"position":[[214,19]]}}}],["cell预测出一组与数据集有关的条件类别概率。在yolov1论文使用的数据集pasc",{"_index":2880,"t":{"1556":{"position":[[378,44]]}}}],["cell，每个grid",{"_index":2894,"t":{"1558":{"position":[[88,11]]}}}],["center",{"_index":2170,"t":{"1347":{"position":[[264,6]]},"1428":{"position":[[264,6]]},"1514":{"position":[[115,8]]}}}],["center(replac",{"_index":2795,"t":{"1514":{"position":[[220,14],[270,14]]}}}],["cfg",{"_index":1960,"t":{"1285":{"position":[[903,3]]},"1347":{"position":[[2424,3],[2428,12],[2489,3]]},"1370":{"position":[[903,3]]},"1428":{"position":[[2425,3],[2429,12],[2490,3]]}}}],["cfg（classifi",{"_index":2206,"t":{"1347":{"position":[[1932,14]]},"1428":{"position":[[1933,14]]}}}],["chang",{"_index":1686,"t":{"1251":{"position":[[344,8]]}}}],["channel",{"_index":2626,"t":{"1454":{"position":[[5173,8],[5378,7]]}}}],["charact",{"_index":1005,"t":{"1059":{"position":[[2289,9],[2600,9]]},"1071":{"position":[[728,9]]},"1185":{"position":[[2289,9],[2600,9]]},"1197":{"position":[[728,9]]},"1454":{"position":[[7410,10]]}}}],["checkpoint",{"_index":1022,"t":{"1062":{"position":[[198,11],[308,10]]},"1188":{"position":[[198,11],[308,10]]},"1454":{"position":[[5427,10]]}}}],["cheung",{"_index":2284,"t":{"1354":{"position":[[762,7]]}}}],["cho",{"_index":2253,"t":{"1354":{"position":[[384,4]]}}}],["choos",{"_index":1634,"t":{"1230":{"position":[[73,6]]}}}],["church",{"_index":504,"t":{"950":{"position":[[65,7]]}}}],["cic_ici",{"_index":2610,"t":{"1454":{"position":[[4595,8]]}}}],["cin",{"_index":34,"t":{"814":{"position":[[69,3]]},"1527":{"position":[[795,4]]}}}],["ci×h×wc_i",{"_index":1641,"t":{"1235":{"position":[[0,12]]}}}],["ck={wi∣pi",{"_index":1189,"t":{"1101":{"position":[[341,9]]}}}],["ckc_kck",{"_index":1188,"t":{"1101":{"position":[[255,8],[267,8]]}}}],["class",{"_index":1041,"t":{"1067":{"position":[[72,5]]},"1071":{"position":[[614,6]]},"1193":{"position":[[72,5]]},"1197":{"position":[[614,6]]},"1270":{"position":[[276,5],[418,5]]},"1324":{"position":[[1272,5]]},"1340":{"position":[[705,5]]},"1421":{"position":[[705,5]]},"1454":{"position":[[7151,6],[7640,6],[7695,5],[7797,5],[8110,6]]},"1490":{"position":[[45,5]]}}}],["classifi",{"_index":1246,"t":{"1120":{"position":[[0,8]]},"1263":{"position":[[656,10]]},"1347":{"position":[[2269,11]]},"1428":{"position":[[2270,11]]}}}],["clear",{"_index":2372,"t":{"1406":{"position":[[930,5]]},"1479":{"position":[[930,5]]}}}],["clip",{"_index":764,"t":{"1024":{"position":[[912,4],[924,4],[1000,9]]},"1051":{"position":[[588,4],[680,4]]},"1057":{"position":[[1019,4]]},"1177":{"position":[[588,4],[680,4]]},"1183":{"position":[[1019,4]]},"1359":{"position":[[639,4],[885,4]]}}}],["clip_directional_loss",{"_index":935,"t":{"1057":{"position":[[1033,34]]},"1071":{"position":[[1162,21]]},"1183":{"position":[[1033,34]]},"1197":{"position":[[1162,21]]}}}],["clip_model.token_embedding(source_tokenized_prompts).type(clip_model.dtyp",{"_index":876,"t":{"1051":{"position":[[872,75]]},"1177":{"position":[[872,75]]}}}],["clip_model.token_embedding(target_tokenized_prompts).type(clip_model.dtyp",{"_index":880,"t":{"1051":{"position":[[1024,75]]},"1177":{"position":[[1024,75]]}}}],["clock",{"_index":2800,"t":{"1514":{"position":[[324,5]]}}}],["closure(t)\\epsilon",{"_index":339,"t":{"867":{"position":[[323,22]]}}}],["closure(t)ϵ−closure(t",{"_index":340,"t":{"867":{"position":[[347,22]]}}}],["cloud",{"_index":840,"t":{"1045":{"position":[[39,5]]},"1171":{"position":[[39,5]]}}}],["cnn",{"_index":1297,"t":{"1131":{"position":[[0,21],[22,81]]},"1391":{"position":[[29,42]]},"1433":{"position":[[928,3],[955,3]]}}}],["cnn中没有全连接层时，本质上可以接受任意尺寸的输入，但这是狭隘的。若考虑其下游任务以及输出，如fcn（fulli",{"_index":1291,"t":{"1129":{"position":[[131,68]]}}}],["cnn本质上可以接受任意通道数的图像输入，但是其模型效果将会受到极大的影响。以一个使用通道数为3的数据集进行训练的cnn",{"_index":1294,"t":{"1129":{"position":[[285,83]]}}}],["cnn模型的输入向量的形状是固定的，其输出向量的形状也是固定的或可以根据不同的下游任务而唯一确定，即输入形状与下游任务共同确定了一个cnn",{"_index":1290,"t":{"1129":{"position":[[0,84]]}}}],["cnn的权值共享使得模型能够学习到图像中的局部特征，这也是一种对于上下文的假设。相邻位置上的权重共享使得模型能够对局部结构进行建模，这种权重共享使得cnn",{"_index":1298,"t":{"1131":{"position":[[110,175]]}}}],["cnn的设计理念认为：在图像任务中，局部结构通常更为重要，局部连接和权值共享使得cnn更适用于图像处理等任务。但也正是这种设计理念，使得cnn在面临长输入序列时不能很好地综合上下文信息、提取位置信息，因此self",{"_index":1299,"t":{"1131":{"position":[[372,106]]}}}],["co",{"_index":2192,"t":{"1347":{"position":[[1216,4]]},"1428":{"position":[[1217,4]]}}}],["coars",{"_index":2352,"t":{"1406":{"position":[[131,6]]},"1413":{"position":[[0,75]]},"1479":{"position":[[131,6]]},"1486":{"position":[[0,75]]}}}],["coco",{"_index":2139,"t":{"1340":{"position":[[998,4]]},"1342":{"position":[[655,4]]},"1421":{"position":[[998,4]]},"1423":{"position":[[655,4]]}}}],["code",{"_index":231,"t":{"844":{"position":[[649,4],[1732,4]]},"1062":{"position":[[86,39],[469,4],[631,10]]},"1138":{"position":[[3210,4]]},"1188":{"position":[[86,39],[469,4],[631,10]]},"1340":{"position":[[1293,5]]},"1406":{"position":[[1355,5]]},"1410":{"position":[[270,5]]},"1421":{"position":[[1293,5]]},"1479":{"position":[[1355,5]]},"1483":{"position":[[270,5]]}}}],["codebook",{"_index":2125,"t":{"1340":{"position":[[644,8]]},"1342":{"position":[[540,16]]},"1345":{"position":[[50,8],[93,8],[128,8],[145,8],[187,8],[210,8],[232,8]]},"1410":{"position":[[254,8],[466,8]]},"1421":{"position":[[644,8]]},"1423":{"position":[[540,16]]},"1426":{"position":[[50,8],[93,8],[128,8],[145,8],[187,8],[210,8],[232,8]]},"1433":{"position":[[1324,15],[1458,8]]},"1483":{"position":[[254,8],[466,8]]}}}],["coeffici",{"_index":2376,"t":{"1406":{"position":[[1018,12]]},"1479":{"position":[[1018,12]]}}}],["cola（corpu",{"_index":1142,"t":{"1083":{"position":[[582,11]]}}}],["collapse（模式坍塌），因此提出了从每个源域图像中学习出多样且准确的prompt",{"_index":2288,"t":{"1354":{"position":[[1088,65]]}}}],["color",{"_index":2740,"t":{"1475":{"position":[[270,5],[304,5],[343,5],[383,5]]}}}],["colors(200001",{"_index":56,"t":{"824":{"position":[[71,14]]}}}],["colors.s",{"_index":64,"t":{"824":{"position":[[247,14]]}}}],["colors[",{"_index":63,"t":{"824":{"position":[[189,8]]}}}],["colors[b",{"_index":62,"t":{"824":{"position":[[166,12]]}}}],["colors[i",{"_index":65,"t":{"824":{"position":[[269,9],[282,8],[445,11]]}}}],["combin",{"_index":2598,"t":{"1454":{"position":[[3568,11]]}}}],["commun",{"_index":2151,"t":{"1340":{"position":[[1325,9]]},"1421":{"position":[[1325,9]]}}}],["compar",{"_index":2042,"t":{"1324":{"position":[[1504,8]]}}}],["competit",{"_index":2038,"t":{"1324":{"position":[[1317,11]]},"1340":{"position":[[1053,11]]},"1421":{"position":[[1053,11]]}}}],["compil",{"_index":118,"t":{"840":{"position":[[644,9],[742,9],[768,7],[947,9]]}}}],["complex",{"_index":2018,"t":{"1324":{"position":[[829,10]]}}}],["compressor",{"_index":2158,"t":{"1342":{"position":[[359,12]]},"1423":{"position":[[359,12]]}}}],["comput",{"_index":1347,"t":{"1138":{"position":[[1244,7],[1653,7],[2132,7],[2550,7],[3808,7],[4024,7],[4291,7],[4508,7]]},"1324":{"position":[[542,13],[1477,13]]}}}],["compute_text_featur",{"_index":1079,"t":{"1069":{"position":[[173,21]]},"1195":{"position":[[173,21]]}}}],["concat",{"_index":1081,"t":{"1069":{"position":[[320,7],[397,6],[549,6]]},"1195":{"position":[[320,7],[397,6],[549,6]]},"1275":{"position":[[50,32]]},"1454":{"position":[[5361,16]]},"1560":{"position":[[121,32]]}}}],["concaten",{"_index":2627,"t":{"1454":{"position":[[5216,13]]}}}],["concat与stack",{"_index":1844,"t":{"1275":{"position":[[0,14]]}}}],["conda",{"_index":820,"t":{"1041":{"position":[[0,5],[31,5]]},"1043":{"position":[[40,5]]},"1167":{"position":[[0,5],[31,5]]},"1169":{"position":[[40,5]]}}}],["condit",{"_index":381,"t":{"921":{"position":[[405,11]]},"961":{"position":[[33,11]]},"1283":{"position":[[73,18]]},"1324":{"position":[[1051,12],[1278,11]]},"1340":{"position":[[711,11],[908,11]]},"1368":{"position":[[73,18]]},"1421":{"position":[[711,11],[908,11]]},"1454":{"position":[[5135,13]]},"1490":{"position":[[51,11]]}}}],["connection加上自身的输入向量，再经过lay",{"_index":430,"t":{"936":{"position":[[90,28]]},"1308":{"position":[[90,28]]}}}],["connection加上送入fcn的输入自身，最终再进行lay",{"_index":432,"t":{"936":{"position":[[152,33]]},"1308":{"position":[[152,33]]}}}],["connection，norm指的是lay",{"_index":405,"t":{"932":{"position":[[49,23]]},"1304":{"position":[[49,23]]}}}],["consist",{"_index":984,"t":{"1059":{"position":[[2005,11]]},"1185":{"position":[[2005,11]]},"1340":{"position":[[539,8]]},"1354":{"position":[[283,11]]},"1421":{"position":[[539,8]]},"1454":{"position":[[3339,8]]}}}],["consum",{"_index":2002,"t":{"1324":{"position":[[421,8]]}}}],["contain",{"_index":1280,"t":{"1122":{"position":[[463,10]]}}}],["content",{"_index":2698,"t":{"1459":{"position":[[26,7]]},"1475":{"position":[[92,8]]},"1579":{"position":[[5,7]]},"1581":{"position":[[16,7],[60,7]]},"1583":{"position":[[16,7],[60,7]]}}}],["context",{"_index":843,"t":{"1049":{"position":[[33,7]]},"1175":{"position":[[33,7]]}}}],["context_length",{"_index":872,"t":{"1051":{"position":[[691,14]]},"1177":{"position":[[691,14]]}}}],["continu",{"_index":1709,"t":{"1251":{"position":[[769,10]]},"1279":{"position":[[435,10],[1015,10]]},"1364":{"position":[[435,10],[1015,10]]}}}],["continun",{"_index":1711,"t":{"1251":{"position":[[815,12]]}}}],["contour",{"_index":2551,"t":{"1454":{"position":[[659,7]]}}}],["contrast",{"_index":2013,"t":{"1324":{"position":[[687,8]]}}}],["control",{"_index":1996,"t":{"1324":{"position":[[258,7]]},"1514":{"position":[[212,7],[262,7]]}}}],["conv2d",{"_index":1455,"t":{"1160":{"position":[[120,33]]}}}],["convent",{"_index":1896,"t":{"1279":{"position":[[0,12]]},"1364":{"position":[[0,12]]}}}],["convert",{"_index":1092,"t":{"1071":{"position":[[593,10]]},"1197":{"position":[[593,10]]}}}],["convolut",{"_index":1292,"t":{"1129":{"position":[[200,11]]},"1324":{"position":[[1154,13]]},"1454":{"position":[[5195,13]]}}}],["copi",{"_index":257,"t":{"844":{"position":[[991,10]]}}}],["copyright",{"_index":234,"t":{"844":{"position":[[668,9],[738,9],[807,9]]}}}],["corpu",{"_index":1148,"t":{"1083":{"position":[[688,33]]}}}],["correct",{"_index":1710,"t":{"1251":{"position":[[800,7]]}}}],["correctli",{"_index":1726,"t":{"1263":{"position":[[667,9]]}}}],["correl",{"_index":2375,"t":{"1406":{"position":[[1006,11]]},"1479":{"position":[[1006,11]]}}}],["correspond",{"_index":2272,"t":{"1354":{"position":[[644,15]]},"1454":{"position":[[7377,10]]}}}],["cosin",{"_index":1959,"t":{"1285":{"position":[[820,6]]},"1370":{"position":[[820,6]]}}}],["count",{"_index":300,"t":{"844":{"position":[[2207,9]]}}}],["cout",{"_index":17,"t":{"803":{"position":[[180,4]]},"814":{"position":[[336,4],[377,4]]},"1527":{"position":[[987,4],[1064,4],[1081,4]]}}}],["co×cic_o",{"_index":1647,"t":{"1235":{"position":[[103,11]]}}}],["co×ci×kh×kwc_o",{"_index":1643,"t":{"1235":{"position":[[38,16]]}}}],["co×h′×w′c_o",{"_index":1648,"t":{"1235":{"position":[[133,14]]}}}],["cp={wi∣∑j=1ipj≥p}c_p=\\left\\{w_i",{"_index":1213,"t":{"1103":{"position":[[537,31]]}}}],["cpc_pcp",{"_index":1212,"t":{"1103":{"position":[[461,8],[473,8]]}}}],["cpr:=γ(f(\\mathbf{c}_{\\mathrm{pr}}:=\\gamma(f(cpr​:=γ(f(\"a",{"_index":2683,"t":{"1454":{"position":[[8053,56]]}}}],["creat",{"_index":821,"t":{"1041":{"position":[[6,6]]},"1167":{"position":[[6,6]]},"1454":{"position":[[3684,6]]}}}],["criteria.clip_loss.cliploss.clip_directional_loss",{"_index":1104,"t":{"1071":{"position":[[1208,50]]},"1197":{"position":[[1208,50]]}}}],["cross",{"_index":466,"t":{"940":{"position":[[1417,11]]},"1208":{"position":[[5,5]]},"1226":{"position":[[7,5],[133,5]]},"1228":{"position":[[28,5]]},"1279":{"position":[[490,5]]},"1312":{"position":[[1417,11]]},"1324":{"position":[[924,5]]},"1351":{"position":[[189,5]]},"1354":{"position":[[631,5]]},"1364":{"position":[[490,5]]}}}],["cross_loss",{"_index":1561,"t":{"1208":{"position":[[1727,10]]}}}],["cross_loss(predict",{"_index":1563,"t":{"1208":{"position":[[1838,19]]}}}],["crossentropyloss",{"_index":1560,"t":{"1208":{"position":[[1706,20]]}}}],["cs",{"_index":2931,"t":{"1594":{"position":[[17,13]]}}}],["csp特征融合可以参考：https://blog.csdn.net/weixin_55073640/article/details/122614176",{"_index":2900,"t":{"1562":{"position":[[4,77]]}}}],["ctc_tct",{"_index":2611,"t":{"1454":{"position":[[4612,8]]}}}],["ctrl",{"_index":209,"t":{"844":{"position":[[118,4],[1330,4]]}}}],["ctx_init",{"_index":842,"t":{"1049":{"position":[[0,8],[200,8]]},"1069":{"position":[[36,19],[129,8],[151,10],[162,8],[416,8],[574,8],[730,8]]},"1175":{"position":[[0,8],[200,8]]},"1195":{"position":[[36,19],[129,8],[151,10],[162,8],[416,8],[574,8],[730,8]]}}}],["cuda",{"_index":825,"t":{"1043":{"position":[[11,7]]},"1169":{"position":[[11,7]]}}}],["cuda=11.8",{"_index":830,"t":{"1043":{"position":[[93,9]]},"1169":{"position":[[93,9]]}}}],["cudnn中rnn的api就是batch_size在第二维度。进一步讲，batch",{"_index":2519,"t":{"1447":{"position":[[331,43]]}}}],["cup",{"_index":313,"t":{"857":{"position":[[133,4]]},"861":{"position":[[198,4]]}}}],["curve_name_2",{"_index":1238,"t":{"1116":{"position":[[134,17]]}}}],["custom.css",{"_index":2735,"t":{"1475":{"position":[[161,18]]}}}],["cvpr",{"_index":2222,"t":{"1349":{"position":[[5,7],[53,14]]},"1354":{"position":[[475,4],[663,5],[944,7]]},"1449":{"position":[[6,4],[85,4]]},"1454":{"position":[[1433,4],[5792,4]]},"1457":{"position":[[189,4]]}}}],["c}f∈rh×w×c",{"_index":2414,"t":{"1413":{"position":[[457,10]]},"1486":{"position":[[457,10]]}}}],["c语言代码编写以及编译、使用boch",{"_index":201,"t":{"840":{"position":[[2162,36]]}}}],["d",{"_index":60,"t":{"824":{"position":[[117,4],[332,4]]},"1275":{"position":[[291,1]]},"1454":{"position":[[3740,3]]}}}],["d(g(z)))]\\tag{2}lossfake​=−ez∼pz​(z)​[log(1−d(g(z)))](2",{"_index":390,"t":{"925":{"position":[[405,56]]}}}],["d(g(z))]\\tag{4}lg​=−ez∼pz​(z)​[logd(g(z))](4",{"_index":395,"t":{"925":{"position":[[855,45]]}}}],["d(u,v)\\leq",{"_index":613,"t":{"979":{"position":[[1151,10]]},"981":{"position":[[668,10]]},"983":{"position":[[184,10],[514,10]]}}}],["d(u,v)d(u,v)d(u,v)达到截止频率时，d(u,v)d0=1\\frac{d(u,v)}{d_0}=1d0​d(u,v)​=1，此时h(u,v)=0.707h(u,v)=0.707h(u,v)=0.707",{"_index":602,"t":{"979":{"position":[[602,110]]}}}],["d(x)]\\tag{1}lossreal​=−ex∼pdata​(x)​[logd(x)](1",{"_index":385,"t":{"925":{"position":[[241,48]]}}}],["d(x)d(x)d(x)是判别器的输出，xxx是真实样本，eee",{"_index":386,"t":{"925":{"position":[[290,41]]}}}],["d.\\n",{"_index":210,"t":{"844":{"position":[[125,7],[1337,7]]}}}],["d0d_0d0​处垂直截止，通过频率和截止频率在d0d_0d0",{"_index":597,"t":{"979":{"position":[[272,68]]}}}],["d0d_0d0​尾部包含高频分量d1d_1d1",{"_index":608,"t":{"979":{"position":[[960,59]]}}}],["d0d_0d0​是通带中心频率，www",{"_index":636,"t":{"983":{"position":[[330,28]]}}}],["d2l",{"_index":1732,"t":{"1270":{"position":[[101,3],[121,3]]}}}],["d2l.accumulator(2",{"_index":1774,"t":{"1270":{"position":[[1069,18]]}}}],["d2l.accumulator(3",{"_index":1808,"t":{"1270":{"position":[[1843,18]]}}}],["d2l.animator(xlabel='epoch",{"_index":1801,"t":{"1270":{"position":[[1685,28]]}}}],["d2l.load_data_fashion_mnist(batch_s",{"_index":1838,"t":{"1270":{"position":[[2680,39]]}}}],["d2l.timer",{"_index":1806,"t":{"1270":{"position":[[1805,12]]}}}],["d2l.try_gpu",{"_index":1842,"t":{"1270":{"position":[[2814,14]]}}}],["d4(p,q)=∣x−s∣+∣y−t∣(2)d_4(p,q)=\\vert",{"_index":533,"t":{"955":{"position":[[271,36]]}}}],["d8(p,q)=max(∣x−s∣,∣y−t∣)(3)d_8(p,q)=max(\\vert",{"_index":537,"t":{"955":{"position":[[368,45]]}}}],["d=512d=512d=512，dk=512d_k=512dk​=512（查询和键的维度）。那么，q、k、v",{"_index":1431,"t":{"1152":{"position":[[33,54]]}}}],["d_0",{"_index":595,"t":{"979":{"position":[[136,5]]},"981":{"position":[[679,5]]}}}],["d_0+\\frac{w}{2}\\\\0&d(u,v)\\geq",{"_index":634,"t":{"983":{"position":[[195,29]]}}}],["d_0+\\frac{w}{2}\\end{cases}\\tag{23}h(u,v)=⎩⎨⎧​010​d(u,v)<d0​−2w​d0​−2w​≤d(u,v)≤d0​+2w​d(u,v)≥d0​+2w​​(23",{"_index":635,"t":{"983":{"position":[[225,104]]}}}],["d_0\\end{cases}\\tag{19}h(u,v)={10​d(u,v)>d0​d(u,v)≤d0​​(19",{"_index":619,"t":{"981":{"position":[[110,58]]}}}],["d_1",{"_index":614,"t":{"979":{"position":[[1162,5]]}}}],["d_1}&d_0\\leq",{"_index":612,"t":{"979":{"position":[[1138,12]]}}}],["d_1}&d_1\\leq",{"_index":629,"t":{"981":{"position":[[655,12]]}}}],["d_1}{d_0",{"_index":611,"t":{"979":{"position":[[1129,8]]},"981":{"position":[[646,8]]}}}],["d_kd×dk",{"_index":1422,"t":{"1148":{"position":[[200,11]]}}}],["d_kl×dk",{"_index":1425,"t":{"1148":{"position":[[259,9]]}}}],["d_model",{"_index":1316,"t":{"1138":{"position":[[108,8],[141,7]]},"1144":{"position":[[305,8],[338,7]]}}}],["d_{kl}(p\\space",{"_index":1540,"t":{"1208":{"position":[[1231,14]]}}}],["d_{kl}(q",{"_index":1522,"t":{"1208":{"position":[[759,8]]}}}],["dall",{"_index":2348,"t":{"1401":{"position":[[0,9],[191,17]]},"1403":{"position":[[265,4]]}}}],["data",{"_index":1265,"t":{"1122":{"position":[[238,4],[453,4],[496,5],[502,4]]},"1223":{"position":[[145,11]]},"1324":{"position":[[175,4]]},"1340":{"position":[[493,4]]},"1354":{"position":[[773,4]]},"1406":{"position":[[865,4]]},"1421":{"position":[[493,4]]},"1454":{"position":[[3817,4]]},"1457":{"position":[[183,5]]},"1479":{"position":[[865,4]]}}}],["data_dir",{"_index":1263,"t":{"1122":{"position":[[212,9]]}}}],["data_it",{"_index":1767,"t":{"1270":{"position":[[927,10],[1100,10]]}}}],["dataset",{"_index":1252,"t":{"1122":{"position":[[13,7],[174,7],[370,7]]},"1226":{"position":[[166,7]]},"1454":{"position":[[3390,8],[3438,8],[3693,7]]}}}],["daveho@cs.umd.edu",{"_index":239,"t":{"844":{"position":[[716,19]]}}}],["david",{"_index":236,"t":{"844":{"position":[[697,5]]}}}],["day",{"_index":2005,"t":{"1324":{"position":[[446,4]]}}}],["ddd",{"_index":1414,"t":{"1144":{"position":[[201,5]]},"1146":{"position":[[29,3]]},"1454":{"position":[[1770,3]]}}}],["ddpm",{"_index":2045,"t":{"1327":{"position":[[109,8]]}}}],["ddpm、ddim、plms算法分析）、lat",{"_index":1983,"t":{"1322":{"position":[[98,38]]}}}],["ddpm论文中的原图来分析ddpm",{"_index":678,"t":{"1005":{"position":[[854,31]]}}}],["ddpm（denois",{"_index":808,"t":{"1036":{"position":[[137,15]]}}}],["de(p,q)=(x−s)2+(y−t)2(1)d_e(p,q)=\\sqrt{(x",{"_index":530,"t":{"955":{"position":[[177,41]]}}}],["death",{"_index":516,"t":{"950":{"position":[[204,6]]}}}],["deathless",{"_index":515,"t":{"950":{"position":[[194,9]]}}}],["debug",{"_index":193,"t":{"840":{"position":[[1941,9],[2036,9],[2058,7]]},"846":{"position":[[885,9],[980,9],[1002,7]]}}}],["decod",{"_index":471,"t":{"944":{"position":[[27,7],[180,18],[251,7],[329,8],[416,7],[467,18],[562,7]]},"1022":{"position":[[254,30]]},"1026":{"position":[[762,8]]},"1154":{"position":[[179,7]]},"1290":{"position":[[109,7]]},"1316":{"position":[[27,7],[180,18],[251,7],[329,8],[416,7],[467,18],[562,7]]},"1345":{"position":[[33,7]]},"1375":{"position":[[109,7]]},"1410":{"position":[[452,7]]},"1426":{"position":[[33,7]]},"1483":{"position":[[452,7]]}}}],["decoder在推理时是一个一个词汇产生的，在产生第iii个词汇时其后续的词汇是未知的，更不用说进行注意力分数的就算了，而在训练过程中使用teach",{"_index":480,"t":{"946":{"position":[[215,82]]},"1318":{"position":[[215,82]]}}}],["decoder来说，它的输入是中间产物（即gener",{"_index":788,"t":{"1026":{"position":[[806,39]]}}}],["decoder每一步的输出是一个经过softmax的prob",{"_index":443,"t":{"940":{"position":[[119,37]]},"1312":{"position":[[119,37]]}}}],["decoder的任务是生成输出，可以根据是否一次性生成输出分为autoregressive（自回归，abbr",{"_index":435,"t":{"938":{"position":[[0,55]]},"1310":{"position":[[0,55]]}}}],["decoder的整体结构，掩码多头自注意力的输入是添加位置编码之后的decod",{"_index":459,"t":{"940":{"position":[[988,68]]},"1312":{"position":[[988,68]]}}}],["decoder的生成中，每一个时间步的输出是词汇表中每一个单词经过softmax之后的概率分布。为了保证生成任务可以通过模型自己停止而不是一直重复，我们向decoder的输出中加入end符号的生成，即每一次输出除了词汇表的所有词汇外还有end符号的概率，当end",{"_index":451,"t":{"940":{"position":[[610,155]]},"1312":{"position":[[610,155]]}}}],["decoder的结构训练生成模型的decod",{"_index":795,"t":{"1026":{"position":[[1134,25]]}}}],["decoder结构，将每一层对应的featur",{"_index":2853,"t":{"1549":{"position":[[28,24]]}}}],["decoder试图在一次操作中生成整个输出序列。这通常通过使用诸如注意力机制等策略来实现，这些策略允许解码器关注输入序列的不同部分，同时生成输出序列的不同部分。nat的优点在于其高效性，因为它不需要保存和更新大量的可能选项。然而，由于它不能利用上下文信息来生成输出，因此其生成的输出质量普遍会低于at",{"_index":440,"t":{"938":{"position":[[252,158]]},"1310":{"position":[[252,158]]}}}],["decoder输出，使用3x3卷积以及双线性插值上采样到原始分辨率得到该层的特征图，且卷积核的个数为1，输出的featur",{"_index":2848,"t":{"1547":{"position":[[214,62]]}}}],["decoder阶段，每个block",{"_index":2846,"t":{"1547":{"position":[[80,32]]}}}],["decoder需要逐步生成输出，并将之前自身输出的所有词汇经过嵌入层后生成token",{"_index":439,"t":{"938":{"position":[[101,150]]},"1310":{"position":[[101,150]]}}}],["decoder）在每个时间步（或每个解码步骤）的输入都来自于前一个时间步自身的输出以及编码器（encoder）的输出。特别地，首个时间步的输入是begin符号以及编码器（encoder）的输出，在每个后续的时间步，解码器的输入会是前一个时间步自身的输出以及编码器（encod",{"_index":445,"t":{"940":{"position":[[230,158]]},"1312":{"position":[[230,158]]}}}],["decoder）都是通过最小化整体损失来进行联合训练的。这是因为整体模型需要协同工作，encod",{"_index":472,"t":{"944":{"position":[[359,50]]},"1316":{"position":[[359,50]]}}}],["decompos",{"_index":1984,"t":{"1324":{"position":[[3,11]]}}}],["deep",{"_index":748,"t":{"1022":{"position":[[873,4]]}}}],["def",{"_index":1046,"t":{"1067":{"position":[[286,3]]},"1193":{"position":[[286,3]]},"1270":{"position":[[307,3],[364,3],[443,3],[855,3],[896,3],[1297,3],[1364,3],[1427,3]]}}}],["defin",{"_index":1319,"t":{"1138":{"position":[[288,6],[906,6],[3333,6]]},"1279":{"position":[[513,6]]},"1364":{"position":[[513,6]]}}}],["definit",{"_index":2696,"t":{"1457":{"position":[[63,10]]}}}],["delta",{"_index":1967,"t":{"1287":{"position":[[294,8]]},"1372":{"position":[[294,8]]}}}],["deltaxt−1​=αt​​1​(xt​−1−αˉt​​1−αt​​ϵθ​(xt​∣t,z))+σt",{"_index":1966,"t":{"1287":{"position":[[236,54]]},"1372":{"position":[[236,54]]}}}],["delta}^{\\delta}\\sum_{b",{"_index":1668,"t":{"1242":{"position":[[131,24]]}}}],["delta}^{\\delta}{v_{a,b}*x_{i_a,j+b",{"_index":1669,"t":{"1242":{"position":[[156,37]]}}}],["demonstr",{"_index":2141,"t":{"1340":{"position":[[1039,13]]},"1421":{"position":[[1039,13]]}}}],["denois",{"_index":1987,"t":{"1324":{"position":[[76,9]]},"1449":{"position":[[26,9]]},"1454":{"position":[[327,8]]}}}],["denoise模块的目标是预测出输入的噪声图片中的噪声，其资料可以通过对数据集中的图片不断加入从gaussian",{"_index":811,"t":{"1036":{"position":[[362,56]]}}}],["depend",{"_index":304,"t":{"846":{"position":[[75,6]]},"1415":{"position":[[597,16]]},"1488":{"position":[[597,16]]}}}],["depend以及mak",{"_index":103,"t":{"840":{"position":[[79,15]]}}}],["depend和mak",{"_index":87,"t":{"830":{"position":[[346,14]]},"834":{"position":[[152,14]]}}}],["deploy",{"_index":1035,"t":{"1064":{"position":[[20,10]]},"1190":{"position":[[20,10]]}}}],["depth",{"_index":2859,"t":{"1551":{"position":[[32,24]]}}}],["describ",{"_index":1095,"t":{"1071":{"position":[[682,10]]},"1197":{"position":[[682,10]]}}}],["design",{"_index":2115,"t":{"1340":{"position":[[388,6]]},"1421":{"position":[[388,6]]}}}],["desktop文件，因此在更换图标是最好直接更换在主题文件中替换icon，而不是更改desktop的icon",{"_index":2779,"t":{"1512":{"position":[[203,69]]}}}],["detail",{"_index":2020,"t":{"1324":{"position":[[854,6]]}}}],["detect",{"_index":2710,"t":{"1467":{"position":[[145,8]]},"1545":{"position":[[22,32]]}}}],["dev/your_partit",{"_index":2773,"t":{"1509":{"position":[[72,19]]}}}],["devic",{"_index":890,"t":{"1055":{"position":[[83,7]]},"1181":{"position":[[83,7]]},"1270":{"position":[[1007,7],[1015,6],[1418,8]]}}}],["device=non",{"_index":1768,"t":{"1270":{"position":[[938,13]]}}}],["df",{"_index":2771,"t":{"1509":{"position":[[50,2]]}}}],["dfa",{"_index":331,"t":{"867":{"position":[[0,13],[14,9],[238,8],[390,12]]},"893":{"position":[[109,11],[272,11],[446,11],[633,11]]}}}],["dfa的可接受以及接受集的定义：从开始状态开始，经过该符号串表示的路径，若能到达终态则称该符号串可被改dfa",{"_index":333,"t":{"867":{"position":[[136,57]]}}}],["diffedit",{"_index":2559,"t":{"1454":{"position":[[1164,9]]}}}],["differ",{"_index":1621,"t":{"1226":{"position":[[258,9]]}}}],["differenti",{"_index":2534,"t":{"1454":{"position":[[172,12]]}}}],["diffus",{"_index":675,"t":{"1005":{"position":[[813,9]]},"1020":{"position":[[53,19]]},"1022":{"position":[[680,9],[851,9]]},"1028":{"position":[[0,34]]},"1036":{"position":[[153,9],[480,9]]},"1069":{"position":[[1198,9]]},"1195":{"position":[[1198,9]]},"1279":{"position":[[362,9],[522,9]]},"1285":{"position":[[117,9],[878,14]]},"1322":{"position":[[55,9],[88,9],[137,9]]},"1324":{"position":[[100,9],[723,9],[984,9],[1187,9]]},"1327":{"position":[[47,9],[87,15]]},"1340":{"position":[[862,9]]},"1342":{"position":[[250,9]]},"1364":{"position":[[362,9],[522,9]]},"1370":{"position":[[117,9],[878,14]]},"1406":{"position":[[462,9],[772,9]]},"1421":{"position":[[862,9]]},"1423":{"position":[[250,9]]},"1433":{"position":[[2076,9]]},"1435":{"position":[[73,9]]},"1449":{"position":[[36,9],[105,9],[156,9]]},"1452":{"position":[[32,9],[64,9],[151,9],[240,9],[266,10],[288,9]]},"1454":{"position":[[1174,9],[1487,9],[2349,9],[2591,9],[2800,9],[3058,9],[3136,9],[3231,9],[3414,9],[3773,9],[3993,9],[4036,9],[4239,9],[4360,10],[4688,9],[5285,9],[5412,9],[5540,9],[5841,9],[5992,9],[6037,9],[7863,9],[8616,9],[8729,9]]},"1457":{"position":[[20,9],[96,9],[263,9]]},"1459":{"position":[[62,9],[137,10]]},"1479":{"position":[[462,9],[772,9]]},"1490":{"position":[[211,9]]}}}],["diffusion、dal",{"_index":725,"t":{"1022":{"position":[[313,14]]}}}],["diffusion以及dal",{"_index":729,"t":{"1022":{"position":[[400,15]]}}}],["diffusion学习笔记（十二）——rectifi",{"_index":2467,"t":{"1435":{"position":[[143,28]]}}}],["diffusion是目前图像生成的sota模型之一，在本章中我们快速的了解一下st",{"_index":717,"t":{"1020":{"position":[[7,45]]}}}],["diffusion等sota",{"_index":718,"t":{"1022":{"position":[[11,66]]}}}],["dim",{"_index":1360,"t":{"1138":{"position":[[1890,4],[4186,4]]},"1208":{"position":[[1617,4]]}}}],["dim=0",{"_index":1850,"t":{"1275":{"position":[[202,6]]}}}],["dim=1",{"_index":1855,"t":{"1275":{"position":[[316,6],[687,26]]}}}],["dim_feedforward=1024",{"_index":1059,"t":{"1067":{"position":[[550,21]]},"1193":{"position":[[550,21]]}}}],["dimens",{"_index":1362,"t":{"1138":{"position":[[1919,9],[2755,9],[4215,9]]},"1275":{"position":[[667,9]]},"1406":{"position":[[812,10]]},"1479":{"position":[[812,10]]}}}],["dim指定操作的维度，dim",{"_index":1867,"t":{"1275":{"position":[[634,23]]}}}],["direct",{"_index":2233,"t":{"1351":{"position":[[169,9]]},"1359":{"position":[[621,17]]},"1471":{"position":[[52,10],[88,11],[129,9],[198,9]]}}}],["directli",{"_index":1999,"t":{"1324":{"position":[[361,8]]}}}],["directori",{"_index":1266,"t":{"1122":{"position":[[243,10],[378,9],[507,10]]}}}],["dirichlet",{"_index":546,"t":{"961":{"position":[[0,32]]}}}],["discret",{"_index":1902,"t":{"1279":{"position":[[152,8],[620,8]]},"1364":{"position":[[152,8],[620,8]]}}}],["discrimin",{"_index":377,"t":{"919":{"position":[[224,19]]},"923":{"position":[[4,37]]},"1354":{"position":[[417,14]]}}}],["disney",{"_index":868,"t":{"1051":{"position":[[425,7]]},"1053":{"position":[[105,14]]},"1071":{"position":[[721,6]]},"1177":{"position":[[425,7]]},"1179":{"position":[[105,14]]},"1197":{"position":[[721,6]]}}}],["distanc",{"_index":754,"t":{"1024":{"position":[[67,10]]},"1059":{"position":[[2450,10]]},"1185":{"position":[[2450,10]]},"1406":{"position":[[606,8]]},"1479":{"position":[[606,8]]}}}],["distance（fid",{"_index":969,"t":{"1059":{"position":[[1222,13]]},"1185":{"position":[[1222,13]]}}}],["distinct",{"_index":1096,"t":{"1071":{"position":[[697,11]]},"1197":{"position":[[697,11]]}}}],["distribut",{"_index":444,"t":{"940":{"position":[[157,63]]},"1059":{"position":[[1412,56]]},"1138":{"position":[[733,12],[780,14]]},"1185":{"position":[[1412,56]]},"1279":{"position":[[216,13],[341,12]]},"1312":{"position":[[157,63]]},"1364":{"position":[[216,13],[341,12]]},"1406":{"position":[[359,13]]},"1452":{"position":[[211,13]]},"1454":{"position":[[687,13]]},"1479":{"position":[[359,13]]}}}],["distribution中sample出z\\mathbf{z}z，否则z=0\\mathbf{z}=\\mathbf{0}z=0",{"_index":695,"t":{"1009":{"position":[[123,63]]}}}],["distribution中sample出图片大小的噪声xt\\mathbf{x}_txt",{"_index":692,"t":{"1009":{"position":[[8,45]]}}}],["distribution中sample出的与x0\\mathbf{x}_0x0",{"_index":682,"t":{"1007":{"position":[[116,47]]}}}],["distribution中sample出的噪声的方法来获得，这个加噪声的过程我们称为forward",{"_index":812,"t":{"1036":{"position":[[419,49]]}}}],["distribution）中sampl",{"_index":714,"t":{"1018":{"position":[[76,35]]}}}],["distribution），并不是encoder直接输出一个distrubut",{"_index":802,"t":{"1030":{"position":[[185,43]]}}}],["dit",{"_index":2135,"t":{"1340":{"position":[[892,4]]},"1406":{"position":[[794,5]]},"1421":{"position":[[892,4]]},"1479":{"position":[[794,5]]}}}],["div[class^='announcementbar_",{"_index":2736,"t":{"1475":{"position":[[180,30]]}}}],["diverg",{"_index":2353,"t":{"1406":{"position":[[203,9]]},"1479":{"position":[[203,9]]}}}],["divergence)，又称相对熵（rel",{"_index":1515,"t":{"1208":{"position":[[510,26]]}}}],["division(",{"_index":2823,"t":{"1527":{"position":[[934,11]]}}}],["division(str",{"_index":2808,"t":{"1525":{"position":[[7,15]]},"1527":{"position":[[94,15]]}}}],["divisor",{"_index":2809,"t":{"1525":{"position":[[30,8],[279,8],[349,7],[384,8]]},"1527":{"position":[[117,8],[366,8],[436,7],[471,8]]}}}],["dk",{"_index":1386,"t":{"1138":{"position":[[2765,3],[2864,3]]}}}],["dk\\sqrt{d_k}dk",{"_index":1429,"t":{"1148":{"position":[[423,28]]}}}],["dkd_kdk",{"_index":1423,"t":{"1148":{"position":[[212,8]]}}}],["dkl(p",{"_index":1518,"t":{"1208":{"position":[[699,5]]}}}],["dkl(p,q)=log⁡σ2σ1+σ12+(μ1−μ2)22σ22−12d_{kl}(p,q)=\\log\\frac{\\sigma_2}{\\sigma_1}+\\frac{\\sigma_1^2+(\\mu_1",{"_index":2088,"t":{"1332":{"position":[[756,102]]}}}],["dkl(plabel",{"_index":1532,"t":{"1208":{"position":[[983,14]]}}}],["dlib",{"_index":1027,"t":{"1062":{"position":[[409,4],[530,4]]},"1188":{"position":[[409,4],[530,4]]}}}],["dl×d",{"_index":1417,"t":{"1146":{"position":[[66,4]]}}}],["dm",{"_index":1989,"t":{"1324":{"position":[[117,5],[411,3],[519,2],[1528,4]]},"1342":{"position":[[275,3]]},"1423":{"position":[[275,3]]}}}],["dock显示的图标是全局图标，程序启动器的desktop文件位于/usr/share/applications中，全局主题中图标主题的程序logo位于~/.local/share/icons/mko",{"_index":2775,"t":{"1512":{"position":[[0,100]]}}}],["docusauru",{"_index":26,"t":{"808":{"position":[[10,10]]},"1465":{"position":[[8,10]]},"1469":{"position":[[11,10],[138,10],[213,10]]},"1471":{"position":[[0,10],[100,10]]}}}],["docusaurus.config.j",{"_index":2708,"t":{"1467":{"position":[[94,20]]}}}],["docusaurus.config.js的themeconfig",{"_index":2728,"t":{"1475":{"position":[[7,40]]}}}],["dog",{"_index":507,"t":{"950":{"position":[[93,3]]}}}],["domain",{"_index":661,"t":{"1005":{"position":[[413,32]]},"1018":{"position":[[392,32]]},"1069":{"position":[[768,6],[879,6]]},"1071":{"position":[[0,23],[33,6],[990,6],[1015,6]]},"1195":{"position":[[768,6],[879,6]]},"1197":{"position":[[0,23],[33,6],[990,6],[1015,6]]},"1279":{"position":[[1033,7]]},"1340":{"position":[[162,7]]},"1351":{"position":[[195,6],[234,6]]},"1354":{"position":[[637,6]]},"1364":{"position":[[1033,7]]},"1421":{"position":[[162,7]]},"1454":{"position":[[6811,6]]}}}],["domain）符合某种分布。因此目前的sota模型除了将文字prompt作为输入，还从某随机分布中sample出图片shape的随机向量（矩阵）作为输入，期待模型根据prompt将源域（sourc",{"_index":660,"t":{"1005":{"position":[[313,99]]},"1018":{"position":[[292,99]]}}}],["done",{"_index":301,"t":{"844":{"position":[[2334,5]]},"1138":{"position":[[818,4]]}}}],["don’t",{"_index":1380,"t":{"1138":{"position":[[2647,5],[2839,5]]}}}],["dot",{"_index":1369,"t":{"1138":{"position":[[2225,3],[2662,3],[2804,3],[2875,3],[2977,3],[2990,6],[4384,3]]},"1148":{"position":[[291,17]]},"1454":{"position":[[589,4]]}}}],["dote",{"_index":1366,"t":{"1138":{"position":[[2163,6],[4322,6]]}}}],["download",{"_index":1261,"t":{"1122":{"position":[[148,8]]}}}],["downsampl",{"_index":2120,"t":{"1340":{"position":[[580,10]]},"1421":{"position":[[580,10]]}}}],["downstream",{"_index":1110,"t":{"1079":{"position":[[318,18]]},"1083":{"position":[[898,10]]},"1406":{"position":[[1120,10]]},"1479":{"position":[[1120,10]]}}}],["do循环中，list",{"_index":49,"t":{"816":{"position":[[152,33]]}}}],["dreambooth",{"_index":2638,"t":{"1454":{"position":[[5803,11]]}}}],["drift",{"_index":2670,"t":{"1454":{"position":[[7047,40],[7768,13]]}}}],["drive",{"_index":838,"t":{"1045":{"position":[[21,5]]},"1059":{"position":[[20,5]]},"1122":{"position":[[194,6]]},"1171":{"position":[[21,5]]},"1185":{"position":[[20,5]]}}}],["driven",{"_index":2639,"t":{"1454":{"position":[[5870,6]]}}}],["dropout",{"_index":1605,"t":{"1221":{"position":[[9,14]]},"1223":{"position":[[100,44]]}}}],["dropout=0.1",{"_index":1060,"t":{"1067":{"position":[[572,12]]},"1193":{"position":[[572,12]]}}}],["dtype=torch.float32",{"_index":1322,"t":{"1138":{"position":[[416,20],[1021,20],[1111,20],[1201,20],[3463,20],[3605,20],[3695,20],[3785,20]]},"1441":{"position":[[322,20]]}}}],["due",{"_index":2007,"t":{"1324":{"position":[[478,3]]}}}],["dvərˈseriəl",{"_index":371,"t":{"910":{"position":[[35,15]]}}}],["d}\\sigma}{{\\rm",{"_index":1465,"t":{"1201":{"position":[[95,14]]}}}],["d}etgt​∈rt×d",{"_index":2568,"t":{"1454":{"position":[[1729,15]]}}}],["d}f(x)}{{\\text",{"_index":1482,"t":{"1203":{"position":[[139,14]]}}}],["d}x",{"_index":1466,"t":{"1201":{"position":[[110,4]]},"1203":{"position":[[154,4]]}}}],["d×dkd",{"_index":1421,"t":{"1148":{"position":[[187,5]]}}}],["e",{"_index":58,"t":{"824":{"position":[[97,2],[126,4],[147,1]]},"1059":{"position":[[2252,1],[2581,1]]},"1185":{"position":[[2252,1],[2581,1]]},"1201":{"position":[[39,3]]},"1342":{"position":[[86,1]]},"1401":{"position":[[10,1],[209,1]]},"1403":{"position":[[270,1]]},"1423":{"position":[[86,1]]},"1500":{"position":[[181,1],[279,1]]}}}],["e(ci)\\mathcal{e}(c_i)e(ci",{"_index":2628,"t":{"1454":{"position":[[5243,28],[5327,30]]}}}],["e.g",{"_index":2108,"t":{"1340":{"position":[[240,5]]},"1421":{"position":[[240,5]]}}}],["e4",{"_index":1019,"t":{"1062":{"position":[[34,3],[164,3],[246,3],[651,3]]},"1188":{"position":[[34,3],[164,3],[246,3],[651,3]]}}}],["e\\mathbb{e}",{"_index":2623,"t":{"1454":{"position":[[5031,12]]}}}],["e\\mathcal{e}",{"_index":2605,"t":{"1454":{"position":[[4395,13]]}}}],["e_{x\\sim",{"_index":383,"t":{"925":{"position":[[207,8]]}}}],["e_{z\\sim",{"_index":388,"t":{"925":{"position":[[382,8],[833,8]]}}}],["each",{"_index":1101,"t":{"1071":{"position":[[810,4]]},"1134":{"position":[[181,4]]},"1197":{"position":[[810,4]]},"1226":{"position":[[285,4]]},"1228":{"position":[[3,4],[199,4],[267,4]]}}}],["echo",{"_index":296,"t":{"844":{"position":[[2179,4]]},"1500":{"position":[[175,4],[273,4]]}}}],["ediff",{"_index":2528,"t":{"1452":{"position":[[74,5]]}}}],["edit",{"_index":165,"t":{"840":{"position":[[1468,4]]},"846":{"position":[[412,4]]},"1406":{"position":[[1184,8]]},"1454":{"position":[[148,7],[609,7],[1205,7],[1474,7],[3300,7],[3382,7],[3511,6],[3825,4],[3900,4],[4128,6],[4200,6],[4286,6]]},"1459":{"position":[[117,7]]},"1479":{"position":[[1184,8]]}}}],["edit_direciton",{"_index":936,"t":{"1057":{"position":[[1068,35]]},"1183":{"position":[[1068,35]]}}}],["edit_direct",{"_index":943,"t":{"1057":{"position":[[1283,14]]},"1183":{"position":[[1283,14]]}}}],["effect",{"_index":1912,"t":{"1279":{"position":[[664,13]]},"1340":{"position":[[1133,13]]},"1364":{"position":[[664,13]]},"1421":{"position":[[1133,13]]}}}],["effici",{"_index":2369,"t":{"1406":{"position":[[870,11]]},"1479":{"position":[[870,11]]}}}],["efro",{"_index":2265,"t":{"1354":{"position":[[547,6]]}}}],["ehpf",{"_index":624,"t":{"981":{"position":[[414,14]]}}}],["ei",{"_index":2938,"t":{"1594":{"position":[[156,20]]}}}],["element",{"_index":420,"t":{"934":{"position":[[1053,32]]},"1055":{"position":[[1068,7]]},"1057":{"position":[[1121,7],[1244,8]]},"1069":{"position":[[433,7],[591,7]]},"1071":{"position":[[834,7]]},"1181":{"position":[[1068,7]]},"1183":{"position":[[1121,7],[1244,8]]},"1195":{"position":[[433,7],[591,7]]},"1197":{"position":[[834,7]]},"1306":{"position":[[1053,32]]}}}],["elf",{"_index":1007,"t":{"1059":{"position":[[2318,3],[2629,3]]},"1185":{"position":[[2318,3],[2629,3]]}}}],["elf_i386",{"_index":149,"t":{"840":{"position":[[1088,8]]}}}],["eli",{"_index":2268,"t":{"1354":{"position":[[568,3]]}}}],["elimin",{"_index":1911,"t":{"1279":{"position":[[596,10]]},"1364":{"position":[[596,10]]}}}],["ell_u",{"_index":2211,"t":{"1347":{"position":[[2127,6]]},"1428":{"position":[[2128,6]]}}}],["ell_u)ℓg​=ℓu​+s(ℓc​−ℓu",{"_index":2213,"t":{"1347":{"position":[[2147,25]]},"1428":{"position":[[2148,25]]}}}],["elpf",{"_index":603,"t":{"979":{"position":[[713,14]]}}}],["embed",{"_index":874,"t":{"1051":{"position":[[842,10]]},"1085":{"position":[[191,10]]},"1177":{"position":[[842,10]]},"1454":{"position":[[2228,9],[2421,9],[2664,9]]}}}],["embedding中的cbow，bert可以看作是contextu",{"_index":1166,"t":{"1085":{"position":[[146,39]]}}}],["embedding和posit",{"_index":425,"t":{"936":{"position":[[10,20]]},"1308":{"position":[[10,20]]}}}],["embedding相加得到。输入序列经过mutil",{"_index":426,"t":{"936":{"position":[[31,25]]},"1308":{"position":[[31,25]]}}}],["embedding）是一种位置编码方法，旨在解决绝对位置编码在处理较长序列时的局限性。rop",{"_index":2190,"t":{"1347":{"position":[[1026,47]]},"1428":{"position":[[1027,47]]}}}],["empir",{"_index":2367,"t":{"1406":{"position":[[726,11]]},"1479":{"position":[[726,11]]}}}],["emul",{"_index":2384,"t":{"1406":{"position":[[1233,8]]},"1479":{"position":[[1233,8]]}}}],["enabl",{"_index":2008,"t":{"1324":{"position":[[512,6]]}}}],["enabled=0",{"_index":188,"t":{"840":{"position":[[1840,9],[1868,9],[1895,9]]},"846":{"position":[[784,9],[812,9],[839,9]]}}}],["encdoer",{"_index":937,"t":{"1057":{"position":[[1110,7]]},"1183":{"position":[[1110,7]]}}}],["encod",{"_index":470,"t":{"944":{"position":[[14,9],[35,28],[135,18],[319,7],[349,7],[457,7],[552,7]]},"1005":{"position":[[576,8]]},"1022":{"position":[[602,7],[753,7]]},"1030":{"position":[[24,8]]},"1032":{"position":[[129,58]]},"1062":{"position":[[38,7],[168,7],[250,7]]},"1064":{"position":[[194,22]]},"1067":{"position":[[158,26]]},"1079":{"position":[[69,7]]},"1188":{"position":[[38,7],[168,7],[250,7]]},"1190":{"position":[[194,22]]},"1193":{"position":[[158,26]]},"1316":{"position":[[14,9],[35,28],[135,18],[319,7],[349,7],[457,7],[552,7]]},"1345":{"position":[[15,7]]},"1410":{"position":[[1227,7]]},"1415":{"position":[[116,7]]},"1426":{"position":[[15,7]]},"1433":{"position":[[2014,7]]},"1483":{"position":[[1227,7]]},"1488":{"position":[[116,7]]},"1490":{"position":[[137,11]]},"1547":{"position":[[194,19]]}}}],["encoder、decod",{"_index":2458,"t":{"1433":{"position":[[905,15]]}}}],["encoder中得到一个representation，再将对应prompt生成的图像输入至clip的imag",{"_index":769,"t":{"1024":{"position":[[1050,55]]}}}],["encoder中得到对应的representation，计算二者之间的距离，即得到clip",{"_index":770,"t":{"1024":{"position":[[1106,45]]}}}],["encoder中输入数据集中的高清预期图片，encoder将其转换为某种lat",{"_index":796,"t":{"1026":{"position":[[1160,48]]}}}],["encoder之后加入pixel",{"_index":1045,"t":{"1067":{"position":[[254,16]]},"1193":{"position":[[254,16]]}}}],["encoder使用数据集中的图片（即期待模型最终输出的图片）作为输入，输出该图片的某种lat",{"_index":782,"t":{"1026":{"position":[[604,52]]}}}],["encoder可以帮助模型理解prompt",{"_index":734,"t":{"1022":{"position":[[531,30]]}}}],["encoder后得到其在clip空间的编码表示。将源域的prompt文字和图像编码表示作为contrast",{"_index":2306,"t":{"1359":{"position":[[489,56]]}}}],["encoder后的编码共同作为domain",{"_index":2302,"t":{"1359":{"position":[[324,21]]}}}],["encoder和decoder的桥梁，也是decod",{"_index":467,"t":{"940":{"position":[[1441,46]]},"1312":{"position":[[1441,46]]}}}],["encoder和生成模型的decod",{"_index":798,"t":{"1026":{"position":[[1307,27]]}}}],["encoder得到源域图片prompt描述在clip空间的编码表示，同时源域图像再经过来自clip的imag",{"_index":2305,"t":{"1359":{"position":[[433,55]]}}}],["encoder得到目标域图片prompt在clip空间的编码表示，并与目标域标签经过text",{"_index":2301,"t":{"1359":{"position":[[277,46]]}}}],["encoder的多头自注意力不同，在decoder中，为注意力机制应用了掩码，使模型只能关注到当前位置及其之前的位置，而不能访问未来的信息。这解决了引入teach",{"_index":484,"t":{"948":{"position":[[48,84]]},"1320":{"position":[[48,84]]}}}],["encoder的模型大小对图像生成模型的影响是非常大的。text",{"_index":733,"t":{"1022":{"position":[[498,32]]}}}],["encoder结构如下图所示。其中，add指的是残差连接residu",{"_index":404,"t":{"932":{"position":[[12,36]]},"1304":{"position":[[12,36]]}}}],["encoder设计为可逆的（invertible），在训练阶段喂入多张图片，期待模型的向量符合某个随机分布。而在预测阶段，由于encoder是可逆的，输入从该随机分布中sampl",{"_index":803,"t":{"1032":{"position":[[0,128]]}}}],["encoder输出的prompt表示以及从随机分布sampl",{"_index":723,"t":{"1022":{"position":[[138,62]]}}}],["encoder阶段，每个block之后使用maxpool",{"_index":2845,"t":{"1547":{"position":[[42,37]]}}}],["encoder（t5",{"_index":2464,"t":{"1435":{"position":[[21,11]]}}}],["encoder）的输出序列（通常是输入序列的表示），另一个来自解码器（decod",{"_index":468,"t":{"940":{"position":[[1488,102]]},"1312":{"position":[[1488,102]]}}}],["encoder，psp",{"_index":1033,"t":{"1062":{"position":[[655,11]]},"1188":{"position":[[655,11]]}}}],["encoder，使用encod",{"_index":794,"t":{"1026":{"position":[[1116,17]]}}}],["encoder：根据输入的text",{"_index":720,"t":{"1022":{"position":[[83,17]]}}}],["encrypt",{"_index":2925,"t":{"1579":{"position":[[22,10]]},"1581":{"position":[[33,10],[77,10]]},"1583":{"position":[[33,10],[77,10]]}}}],["end",{"_index":67,"t":{"824":{"position":[[310,4],[345,6],[372,3],[418,4]]},"940":{"position":[[603,6]]},"1263":{"position":[[639,3]]},"1312":{"position":[[603,6]]},"1452":{"position":[[277,3],[284,3]]}}}],["end[i",{"_index":53,"t":{"822":{"position":[[270,18]]}}}],["endl",{"_index":18,"t":{"803":{"position":[[193,5]]},"814":{"position":[[352,5],[391,5]]},"1527":{"position":[[1089,5]]}}}],["endoftext",{"_index":871,"t":{"1051":{"position":[[640,9]]},"1177":{"position":[[640,9]]}}}],["endow",{"_index":2234,"t":{"1351":{"position":[[214,8]]}}}],["end{align*}p(b,c∣a)​=p(a)p(a,b,c)​=p(c∣a,b)⋅p(b∣a)p(a,b,c)​p(a,b,c)​=p(b∣a)⋅p(c∣a,b",{"_index":2084,"t":{"1332":{"position":[[511,86]]}}}],["end{align}h(p",{"_index":1543,"t":{"1208":{"position":[[1306,15]]}}}],["end{align}h(p)​=−i∑n​pi",{"_index":1511,"t":{"1208":{"position":[[408,25]]}}}],["end{cas",{"_index":1479,"t":{"1203":{"position":[[66,11],[202,11]]}}}],["enhanc",{"_index":2235,"t":{"1351":{"position":[[264,8]]}}}],["enjoy",{"_index":1922,"t":{"1279":{"position":[[886,8]]},"1364":{"position":[[886,8]]}}}],["entail",{"_index":1138,"t":{"1083":{"position":[[463,43]]}}}],["entir",{"_index":1102,"t":{"1071":{"position":[[870,6]]},"1197":{"position":[[870,6]]}}}],["entri",{"_index":232,"t":{"844":{"position":[[654,5],[1737,5]]}}}],["entropi",{"_index":1171,"t":{"1087":{"position":[[131,7]]},"1279":{"position":[[496,7]]},"1364":{"position":[[496,7]]}}}],["entropy)，是描述两个概率分布p和q",{"_index":1516,"t":{"1208":{"position":[[537,29]]}}}],["entropy中的entropi",{"_index":1494,"t":{"1208":{"position":[[11,50]]}}}],["entrpoy",{"_index":1499,"t":{"1208":{"position":[[97,7]]}}}],["enumerate(train_it",{"_index":1812,"t":{"1270":{"position":[[1927,22]]}}}],["eopt\\mathbf{e}_{\\text",{"_index":2580,"t":{"1454":{"position":[[2247,21]]}}}],["eopte_{opt}eopt",{"_index":2564,"t":{"1454":{"position":[[1626,18],[2454,16],[2505,26],[2873,26]]}}}],["eot",{"_index":857,"t":{"1051":{"position":[[156,4]]},"1053":{"position":[[120,3],[142,3],[188,6]]},"1069":{"position":[[203,3]]},"1177":{"position":[[156,4]]},"1179":{"position":[[120,3],[142,3],[188,6]]},"1195":{"position":[[203,3]]}}}],["epoch",{"_index":1233,"t":{"1114":{"position":[[313,8]]},"1270":{"position":[[1878,5]]}}}],["epsilon",{"_index":2656,"t":{"1454":{"position":[[6476,9],[6552,9],[8242,9],[8252,10],[8327,9],[8438,10]]}}}],["epsilon_\\theta",{"_index":691,"t":{"1007":{"position":[[812,34]]},"1454":{"position":[[4561,20],[5056,22]]}}}],["epsilon_\\theta\\left(x_t",{"_index":1964,"t":{"1287":{"position":[[178,24]]},"1372":{"position":[[178,24]]}}}],["epsilon_\\theta\\left(z_t",{"_index":2620,"t":{"1454":{"position":[[4880,25]]}}}],["epsilon_{\\theta}ϵθ​是nois",{"_index":688,"t":{"1007":{"position":[[684,28]]}}}],["epsilonzt​:=αt​x+σt",{"_index":2653,"t":{"1454":{"position":[[6374,22]]}}}],["epsilon}\\left[\\left\\|\\boldsymbol{\\epsilon",{"_index":2573,"t":{"1454":{"position":[[1919,43]]}}}],["epsilonϵ是从norm",{"_index":681,"t":{"1007":{"position":[[97,18]]}}}],["equallinear",{"_index":901,"t":{"1055":{"position":[[529,11],[649,12]]},"1181":{"position":[[529,11],[649,12]]}}}],["equallinear(512",{"_index":1068,"t":{"1067":{"position":[[821,16],[949,16]]},"1193":{"position":[[821,16],[949,16]]}}}],["equat",{"_index":2535,"t":{"1454":{"position":[[185,10]]},"1469":{"position":[[37,10]]}}}],["error",{"_index":109,"t":{"840":{"position":[[347,6]]}}}],["estim",{"_index":1631,"t":{"1228":{"position":[[330,8]]}}}],["eta",{"_index":1592,"t":{"1216":{"position":[[434,4],[453,4]]}}}],["eta)\\cdot\\mathbf{e}_{opt}eˉ=η⋅etgt​+(1−η)⋅eopt",{"_index":2589,"t":{"1454":{"position":[[2966,48]]}}}],["etc",{"_index":864,"t":{"1051":{"position":[[224,5],[496,5]]},"1177":{"position":[[224,5],[496,5]]}}}],["etgte_{tgt}etgt",{"_index":2563,"t":{"1454":{"position":[[1562,21],[1584,16],[2678,16],[2843,16]]}}}],["etgt∈rt×d\\mathbf{e}_{t",{"_index":2565,"t":{"1454":{"position":[[1676,22]]}}}],["evalu",{"_index":1612,"t":{"1226":{"position":[[67,8]]},"1279":{"position":[[651,8]]},"1324":{"position":[[496,12]]},"1364":{"position":[[651,8]]}}}],["evaluate_accuracy_gpu(net",{"_index":1766,"t":{"1270":{"position":[[900,26],[2378,26]]}}}],["evaluation）是一个评估自然语言处理模型在多个任务上综合性能的基准（benchmark）。它旨在测试模型对各种语言任务的通用理解能力。glu",{"_index":1127,"t":{"1083":{"position":[[139,75]]}}}],["evid",{"_index":2379,"t":{"1406":{"position":[[1052,9]]},"1479":{"position":[[1052,9]]}}}],["ex,c,ϵ,t[wt∥x~θ(αtx+σtϵ,c)−x∥22]\\mathbb{e}_{\\mathbf{x",{"_index":2654,"t":{"1454":{"position":[[6408,55]]}}}],["ex,c,ϵ,ϵ′,t[wt∥x^θ(αtx+σtϵ,c)−x∥22+λwt′∥x^θ(αt′xpr+σt′ϵ′,cpr)−xpr∥22]\\mathbb{e}_{\\mathbf{x",{"_index":2684,"t":{"1454":{"position":[[8137,92]]}}}],["ex\\mathbb{e}_xex",{"_index":954,"t":{"1059":{"position":[[739,17]]},"1185":{"position":[[739,17]]}}}],["exactli",{"_index":1627,"t":{"1228":{"position":[[220,7]]}}}],["exampl",{"_index":162,"t":{"840":{"position":[[1426,7]]},"846":{"position":[[370,7]]},"1454":{"position":[[3730,9]]}}}],["examples/sec",{"_index":1834,"t":{"1270":{"position":[[2596,12]]}}}],["excit",{"_index":2919,"t":{"1574":{"position":[[12,10]]}}}],["excitation激励操作就是通过sigmoid",{"_index":2923,"t":{"1574":{"position":[[153,42]]}}}],["execut",{"_index":129,"t":{"840":{"position":[[788,7]]}}}],["exhibit",{"_index":2371,"t":{"1406":{"position":[[921,8]]},"1479":{"position":[[921,8]]}}}],["exit",{"_index":207,"t":{"844":{"position":[[109,4],[1321,4]]}}}],["exit(0",{"_index":302,"t":{"844":{"position":[[2343,8]]}}}],["exit(1",{"_index":225,"t":{"844":{"position":[[426,8],[1641,8]]}}}],["exp\\left(\\frac{\\log(p(x_j))}{t}\\right)}p′(xi​)=∑j​exp(tlog(p(xj​))​)exp(tlog(p(xi",{"_index":1175,"t":{"1099":{"position":[[237,88]]}}}],["expens",{"_index":2006,"t":{"1324":{"position":[[468,9]]}}}],["explor",{"_index":2119,"t":{"1340":{"position":[[527,11]]},"1406":{"position":[[1376,11]]},"1421":{"position":[[527,11]]},"1479":{"position":[[1376,11]]}}}],["export",{"_index":2755,"t":{"1500":{"position":[[52,6],[93,6],[133,6]]}}}],["exp⁡\\expexp",{"_index":957,"t":{"1059":{"position":[[828,14]]},"1185":{"position":[[828,14]]}}}],["extra_c_opt",{"_index":157,"t":{"840":{"position":[[1271,15],[1341,15]]}}}],["extract",{"_index":1159,"t":{"1083":{"position":[[1160,10]]}}}],["eˉ=η⋅etgt+(1−η)⋅eopt\\bar{\\mathbf{e}}=\\eta\\cdot\\mathbf{e}_{tgt}+(1",{"_index":2588,"t":{"1454":{"position":[[2900,65]]}}}],["e将lat",{"_index":730,"t":{"1022":{"position":[[416,8]]}}}],["e系列以及google的imagen",{"_index":726,"t":{"1022":{"position":[[328,24]]}}}],["f",{"_index":1400,"t":{"1138":{"position":[[3277,1]]},"1201":{"position":[[289,1]]},"1203":{"position":[[280,1]]},"1270":{"position":[[94,1]]}}}],["f(u)=∑x=0n−1f(x)e−j2πuxn(9)f(u)=\\sum_{x=0}^{n",{"_index":562,"t":{"961":{"position":[[897,45]]}}}],["f(u)=∫−∞+∞f(x)e−j2πuxdx(5)f(u)=\\int_",{"_index":548,"t":{"961":{"position":[[295,37]]}}}],["f(u,v)=∑x=0m−1∑y=0n−1f(x,y)e−j2π(uxm+vyn)(11)f(u,v)=\\sum_{x=0}^{m",{"_index":569,"t":{"961":{"position":[[1158,65]]}}}],["f(u,v)=∫−∞∞∫−∞∞f(x,y)e−j2π(ux+vy)dxdy(7)f(u,v)=\\int_",{"_index":555,"t":{"961":{"position":[[528,53]]}}}],["f(x",{"_index":1477,"t":{"1203":{"position":[[18,4]]},"1206":{"position":[[176,4]]}}}],["f(x)=1n∑u=0n−1f(u)ej2πuxn(10)f(x)=\\frac{1}{n}\\sum_{u=0}^{n",{"_index":566,"t":{"961":{"position":[[1020,58]]}}}],["f(x)={0x<0xx≥0(3",{"_index":1476,"t":{"1203":{"position":[[0,17]]}}}],["f(x)=∫−∞∞f(u)ej2πuxdu(6)f(x)=\\int_",{"_index":552,"t":{"961":{"position":[[415,35]]}}}],["f(x)]^2",{"_index":1488,"t":{"1206":{"position":[[102,9],[210,9]]}}}],["f(x,y)=1mn∑u=0m−1∑v=0n−1f(u,v)ej2π(uxm+vyn)(12)f(x,y)=\\frac{1}{mn}\\sum_{u=0}^{m",{"_index":573,"t":{"961":{"position":[[1355,79]]}}}],["f(x,y)=∫−∞∞∫−∞∞f(u,v)ej2π(ux+vy)dudv(8)f(x,y)=\\int_",{"_index":559,"t":{"961":{"position":[[710,52]]}}}],["f(x,y)f(x,y)f(x,y)经过退化系统h(x,y)h(x,y)h(x,y)后再与噪声n(x,y)n(x,y)n(x,y)叠加，得到最后退化的图像g(x,y)g(x,y)g(x,i",{"_index":640,"t":{"992":{"position":[[0,98]]}}}],["f.relu(x",{"_index":1484,"t":{"1203":{"position":[[329,9]]}}}],["f.sigmoid(x",{"_index":1474,"t":{"1201":{"position":[[338,12]]}}}],["f.softmax(scor",{"_index":1359,"t":{"1138":{"position":[[1872,17],[4168,17]]}}}],["f0f_0f0",{"_index":2569,"t":{"1454":{"position":[[1797,8]]}}}],["f=e(im)f=\\mathcal{e}(im)f=e(im",{"_index":2388,"t":{"1410":{"position":[[116,31]]},"1483":{"position":[[116,31]]}}}],["f^=lookup⁡(z,q)\\hat{f}=\\operatorname{lookup}(z,q)f^​=lookup(z,q",{"_index":2394,"t":{"1410":{"position":[[504,64]]},"1483":{"position":[[504,64]]}}}],["f^\\hat{f}f",{"_index":2393,"t":{"1410":{"position":[[482,12]]},"1483":{"position":[[482,12]]}}}],["f^{(i,j)}\\|_2\\right)\\in[v]q(i,j)=(argv∈[v]min​∥lookup(z,v)−f(i,j)∥2​)∈[v",{"_index":2392,"t":{"1410":{"position":[[378,73]]},"1483":{"position":[[378,73]]}}}],["f_\\theta\\left(\\mathbf{x}_t",{"_index":2574,"t":{"1454":{"position":[[1963,27]]}}}],["facilit",{"_index":1903,"t":{"1279":{"position":[[178,10]]},"1340":{"position":[[1302,10]]},"1364":{"position":[[178,10]]},"1421":{"position":[[1302,10]]}}}],["factor",{"_index":1383,"t":{"1138":{"position":[[2722,6]]}}}],["fals",{"_index":2734,"t":{"1475":{"position":[[141,6]]}}}],["famili",{"_index":2103,"t":{"1340":{"position":[[29,6]]},"1421":{"position":[[29,6]]}}}],["fast",{"_index":2358,"t":{"1406":{"position":[[373,4]]},"1479":{"position":[[373,4]]}}}],["faster",{"_index":2366,"t":{"1406":{"position":[[691,6]]},"1479":{"position":[[691,6]]}}}],["fa根据当前的状态及扫描的输入字符，便能唯一地知道fa",{"_index":332,"t":{"867":{"position":[[24,49]]}}}],["fcn",{"_index":2899,"t":{"1560":{"position":[[235,9]]}}}],["featur",{"_index":929,"t":{"1057":{"position":[[778,8]]},"1059":{"position":[[1877,62]]},"1071":{"position":[[709,8]]},"1120":{"position":[[30,9]]},"1183":{"position":[[778,8]]},"1185":{"position":[[1877,62]]},"1197":{"position":[[709,8]]},"1410":{"position":[[103,7],[148,9],[1262,7]]},"1415":{"position":[[96,7],[128,7],[163,7],[178,7]]},"1483":{"position":[[103,7],[148,9],[1262,7]]},"1488":{"position":[[96,7],[128,7],[163,7],[178,7]]}}}],["few",{"_index":2240,"t":{"1354":{"position":[[95,3],[105,5]]},"1454":{"position":[[6996,3]]}}}],["fewer",{"_index":2674,"t":{"1454":{"position":[[7396,5]]}}}],["fewshot",{"_index":2271,"t":{"1354":{"position":[[602,7]]}}}],["ffhq",{"_index":989,"t":{"1059":{"position":[[2130,4],[2166,4],[2210,4],[2235,4],[2272,4],[2299,4],[2322,4],[2486,4],[2513,4],[2539,4],[2564,4],[2583,4],[2610,4],[2633,4]]},"1185":{"position":[[2130,4],[2166,4],[2210,4],[2235,4],[2272,4],[2299,4],[2322,4],[2486,4],[2513,4],[2539,4],[2564,4],[2583,4],[2610,4],[2633,4]]}}}],["fid",{"_index":972,"t":{"1059":{"position":[[1469,3],[1556,3],[1699,3],[1761,3],[1843,3],[1989,4]]},"1185":{"position":[[1469,3],[1556,3],[1699,3],[1761,3],[1843,3],[1989,4]]},"1340":{"position":[[800,3]]},"1347":{"position":[[2447,3]]},"1406":{"position":[[615,5]]},"1408":{"position":[[205,3]]},"1421":{"position":[[800,3]]},"1428":{"position":[[2448,3]]},"1479":{"position":[[615,5]]},"1481":{"position":[[205,3]]}}}],["fid=∥μ1−μ2∥22+tr(σ1+σ2−2(σ1σ2)12)(1)\\mathrm{fid}=\\left\\|\\mu_1",{"_index":759,"t":{"1024":{"position":[[378,61]]}}}],["fidel",{"_index":2025,"t":{"1324":{"position":[[899,9]]}}}],["fid中，做出了如下重要的假设：将生成的图像真实的图像经过cnn输出的representation看作是sample自两个高斯分布的随机变量。然后，通过计算两个特征向量的均值和协方差矩阵来得到两个高斯分布的参数。最后，利用两个高斯分布之间的fréchet",{"_index":758,"t":{"1024":{"position":[[229,148]]}}}],["fid指标需要一定数量的生成图像和真实图像来进行统计估计。这是因为fid",{"_index":763,"t":{"1024":{"position":[[818,93]]}}}],["fid提供一个pr",{"_index":755,"t":{"1024":{"position":[[78,10]]}}}],["fid（fréchet",{"_index":752,"t":{"1024":{"position":[[45,11]]}}}],["file",{"_index":96,"t":{"838":{"position":[[11,6]]},"840":{"position":[[1228,5],[1443,5]]},"844":{"position":[[986,4]]},"846":{"position":[[387,5]]},"1122":{"position":[[458,4]]}}}],["file=/usr/local/share/bochs/bio",{"_index":173,"t":{"840":{"position":[[1597,32]]},"846":{"position":[[541,32]]}}}],["file=/usr/local/share/bochs/vgabio",{"_index":169,"t":{"840":{"position":[[1522,35]]},"846":{"position":[[466,35]]}}}],["filesystem",{"_index":273,"t":{"844":{"position":[[1790,12]]}}}],["filter",{"_index":593,"t":{"979":{"position":[[63,7]]}}}],["filter都对输入图像的所有通道完成一次卷积，filter中的kernel",{"_index":1447,"t":{"1158":{"position":[[46,59]]}}}],["final",{"_index":1636,"t":{"1230":{"position":[[109,5]]},"1454":{"position":[[3744,8]]}}}],["find",{"_index":1308,"t":{"1134":{"position":[[205,4]]},"1138":{"position":[[2677,4]]},"1410":{"position":[[1706,4]]},"1454":{"position":[[7322,4]]},"1483":{"position":[[1706,4]]}}}],["finder小组件中appl",{"_index":2780,"t":{"1512":{"position":[[273,21]]}}}],["finder栏中plasmoid",{"_index":2783,"t":{"1514":{"position":[[0,29]]}}}],["fine",{"_index":473,"t":{"944":{"position":[[570,9]]},"1079":{"position":[[259,7]]},"1316":{"position":[[570,9]]},"1354":{"position":[[150,7],[454,4]]},"1406":{"position":[[141,4]]},"1454":{"position":[[2726,4],[5815,4]]},"1479":{"position":[[141,4]]}}}],["finetun",{"_index":2595,"t":{"1454":{"position":[[3466,9]]}}}],["fine）的方法很自然地给图像暗示了一种顺序。此外，受广泛使用的多尺度（multi",{"_index":2410,"t":{"1413":{"position":[[79,41]]},"1486":{"position":[[79,41]]}}}],["first",{"_index":2015,"t":{"1324":{"position":[[780,5]]},"1406":{"position":[[416,5]]},"1454":{"position":[[278,5],[775,5],[3454,5],[5189,5],[6749,5]]},"1479":{"position":[[416,5]]}}}],["first(该非终结符)减去ϵ\\epsilonϵ的所有终结符元素都加入至follow",{"_index":344,"t":{"874":{"position":[[155,52]]}}}],["first=tru",{"_index":2518,"t":{"1447":{"position":[[308,22]]}}}],["first意味着不同序列中同一个时刻对应的输入单元在内存中是毗邻的，这样才能做到真正的batch",{"_index":2523,"t":{"1447":{"position":[[505,51]]}}}],["first意味着模型的输入（一个tensor）在内存中存储时，先存储第一个sequence，再存储第二个，而如果是seq_len",{"_index":2520,"t":{"1447":{"position":[[375,64]]}}}],["first集、follow集是针对于符号串而言的，而select",{"_index":345,"t":{"876":{"position":[[4,49]]}}}],["first，模型的输入在内存中，先存储每一个sequ",{"_index":2521,"t":{"1447":{"position":[[440,56]]}}}],["fish的配置文件：~/.config/fish/config.fish",{"_index":2764,"t":{"1502":{"position":[[0,36]]}}}],["fit",{"_index":2529,"t":{"1452":{"position":[[202,4]]}}}],["flag",{"_index":36,"t":{"814":{"position":[[93,4],[236,4],[322,5]]},"840":{"position":[[1200,5]]}}}],["flexibl",{"_index":2012,"t":{"1324":{"position":[[600,12],[1019,8]]},"1351":{"position":[[273,12]]}}}],["floppya",{"_index":177,"t":{"840":{"position":[[1676,8],[1714,9]]},"846":{"position":[[620,8],[658,9]]}}}],["flow",{"_index":2466,"t":{"1435":{"position":[[126,4],[172,4]]}}}],["fno",{"_index":158,"t":{"840":{"position":[[1321,3]]}}}],["focal",{"_index":2916,"t":{"1572":{"position":[[0,5]]}}}],["fold",{"_index":1609,"t":{"1226":{"position":[[2,4],[128,4],[198,6],[268,4]]},"1228":{"position":[[23,4],[61,5],[122,5],[204,4]]}}}],["follow",{"_index":342,"t":{"874":{"position":[[4,20],[25,29],[130,24]]},"1118":{"position":[[69,6]]},"1454":{"position":[[3287,6]]}}}],["follow集加入到该非终结符的follow",{"_index":343,"t":{"874":{"position":[[89,40]]}}}],["follow集解决的话则是slr(1",{"_index":365,"t":{"898":{"position":[[222,29]]}}}],["font",{"_index":2747,"t":{"1475":{"position":[[415,4]]}}}],["forcing与mask",{"_index":457,"t":{"940":{"position":[[892,14],[947,14]]},"1312":{"position":[[892,14],[947,14]]}}}],["forcing出现的问题，避免了训练与推理阶段的mismatch",{"_index":485,"t":{"948":{"position":[[133,43]]},"1320":{"position":[[133,43]]}}}],["forcing时却可以得到第i+1i+1i+1个及其之后词汇的注意力信息，如果不添加其他策略显然会对模型的泛化能力造成很大的影响，而且这并不符合自回归（autoregression）的特性。为了解决这个问题，掩码多头注意力机制应运而生，在训练阶段将模型在时间发展顺序的右侧的输入mask",{"_index":481,"t":{"946":{"position":[[298,163]]},"1318":{"position":[[298,163]]}}}],["forcing策略有很大的关系，具体分析见下文《teach",{"_index":456,"t":{"940":{"position":[[860,31]]},"1312":{"position":[[860,31]]}}}],["forcing策略，将ground",{"_index":478,"t":{"946":{"position":[[158,17]]},"1318":{"position":[[158,17]]}}}],["forcing，一次性输入为两个词汇voc1voc_1voc1​与voc2voc_2voc2",{"_index":488,"t":{"948":{"position":[[449,54]]},"1320":{"position":[[449,54]]}}}],["fork",{"_index":819,"t":{"1038":{"position":[[69,4]]},"1164":{"position":[[69,4]]},"1514":{"position":[[179,6]]}}}],["form",{"_index":2537,"t":{"1454":{"position":[[238,4]]}}}],["format",{"_index":1985,"t":{"1324":{"position":[[25,9]]},"1467":{"position":[[137,7]]}}}],["formul",{"_index":1995,"t":{"1324":{"position":[[212,11]]}}}],["forward",{"_index":654,"t":{"1005":{"position":[[31,7],[118,7]]}}}],["forward(self",{"_index":1743,"t":{"1270":{"position":[[368,13],[859,13]]}}}],["found",{"_index":743,"t":{"1022":{"position":[[730,5]]}}}],["foundat",{"_index":2153,"t":{"1340":{"position":[[1371,10]]},"1421":{"position":[[1371,10]]},"1431":{"position":[[50,10]]}}}],["frac",{"_index":1481,"t":{"1203":{"position":[[125,5]]}}}],["frac12dkl​(p,q)=logσ1​σ2​​+2σ22​σ12​+(μ1​−μ2​)2​−21",{"_index":2090,"t":{"1332":{"position":[[882,53]]}}}],["frac1n\\sum_{i=1}^ny_i\\log(p_i)\\tag{1}l=−n1​i=1∑n​yi​log(pi​)(1",{"_index":1118,"t":{"1081":{"position":[[296,64]]}}}],["frac{1",{"_index":698,"t":{"1009":{"position":[[305,7]]},"1287":{"position":[[135,7]]},"1372":{"position":[[135,7]]}}}],["frac{1}{1",{"_index":1462,"t":{"1201":{"position":[[26,10]]}}}],["frac{\\exp\\left(\\frac{\\log(p(x_i))}{t}\\right)}{\\sum_j",{"_index":1174,"t":{"1099":{"position":[[183,53]]}}}],["frac{\\lambda}{2",{"_index":1583,"t":{"1216":{"position":[[74,17],[255,17]]}}}],["frac{\\parti",{"_index":1589,"t":{"1216":{"position":[[299,14],[458,14]]}}}],["frac{\\sum{i(pred_i==y_i)}}{len(i",{"_index":1680,"t":{"1251":{"position":[[160,35]]}}}],["frac{d(u,v",{"_index":610,"t":{"979":{"position":[[1116,12]]},"981":{"position":[[633,12]]}}}],["frac{d(u,v)}{d_0}]^n}\\tag{17}h(u,v)=e−[d0​d(u,v)​]n(17",{"_index":606,"t":{"979":{"position":[[826,57]]}}}],["frac{d_0}{d(u,v)}]^n}\\tag{21}h(u,v)=e−[d(u,v)d0​​]n(21",{"_index":626,"t":{"981":{"position":[[464,57]]}}}],["frac{p(a,b,c)}{\\frac{p(a,b,c)}{p(c",{"_index":2079,"t":{"1332":{"position":[[407,35]]}}}],["frac{p(a,b,c)}{p(a",{"_index":2078,"t":{"1332":{"position":[[379,21]]}}}],["frac{p(b|a",{"_index":2829,"t":{"1530":{"position":[[469,12]]},"1541":{"position":[[469,12]]}}}],["frac{shape_{input",{"_index":1652,"t":{"1235":{"position":[[273,19]]},"1242":{"position":[[343,19]]}}}],["frac{ux}{m}+\\frac{vy}{n})}\\tag{11}f(u,v)=x=0∑m−1​y=0∑n−1​f(x,y)e−j2π(mux​+nvy​)(11",{"_index":572,"t":{"961":{"position":[[1258,85]]}}}],["frac{ux}{m}+\\frac{vy}{n})}\\tag{12}f(x,y)=mn1​u=0∑m−1​v=0∑n−1​f(u,v)ej2π(mux​+nvy​)(12",{"_index":576,"t":{"961":{"position":[[1468,88]]}}}],["frac{w}{2}\\\\1&d_0",{"_index":632,"t":{"983":{"position":[[149,18]]}}}],["frac{w}{2}\\leq",{"_index":633,"t":{"983":{"position":[[168,15]]}}}],["framework",{"_index":2145,"t":{"1340":{"position":[[1162,10]]},"1342":{"position":[[751,9]]},"1421":{"position":[[1162,10]]},"1423":{"position":[[750,9]]}}}],["free",{"_index":251,"t":{"844":{"position":[[895,4]]},"861":{"position":[[54,4]]},"1347":{"position":[[1947,4],[2281,4]]},"1428":{"position":[[1948,4],[2282,4]]}}}],["freebsd",{"_index":135,"t":{"840":{"position":[[891,8]]}}}],["freez",{"_index":916,"t":{"1057":{"position":[[148,6]]},"1183":{"position":[[148,6]]},"1354":{"position":[[406,6]]}}}],["fréchet",{"_index":968,"t":{"1059":{"position":[[1204,7],[1730,7],[2432,7]]},"1185":{"position":[[1204,7],[1730,7],[2432,7]]},"1406":{"position":[[588,7]]},"1479":{"position":[[588,7]]}}}],["ftfi",{"_index":832,"t":{"1043":{"position":[[136,4]]},"1169":{"position":[[136,4]]}}}],["function",{"_index":1472,"t":{"1201":{"position":[[275,10]]},"1203":{"position":[[266,10]]},"1270":{"position":[[80,10]]},"1279":{"position":[[537,8]]},"1364":{"position":[[537,8]]},"1500":{"position":[[31,8],[200,8]]}}}],["function中起到更新的作用，因此不进行nm",{"_index":2889,"t":{"1556":{"position":[[760,25]]}}}],["further",{"_index":2380,"t":{"1406":{"position":[[1066,7]]},"1479":{"position":[[1066,7]]}}}],["f∈rh×w×cf",{"_index":2412,"t":{"1413":{"position":[[413,9]]},"1486":{"position":[[413,9]]}}}],["g",{"_index":2566,"t":{"1454":{"position":[[1699,1]]}}}],["g(x,y)]^2\\tag{4}mse=mn1​x=1∑m​y=1∑n​[f(x,y)−g(x,y)]2(4",{"_index":542,"t":{"957":{"position":[[118,55]]}}}],["g(z)g(z)g(z)是生成器的输出，d(g(z))d(g(z))d(g(z))是生成样本输入到判别器后的输出，zzz是随机噪声，eee",{"_index":396,"t":{"925":{"position":[[901,78]]}}}],["g(z)g(z)g(z)是生成器的输出，zzz是随机噪声，eee",{"_index":391,"t":{"925":{"position":[[462,41]]}}}],["g=ℓu+s(ℓc−ℓu)\\ell_g",{"_index":2210,"t":{"1347":{"position":[[2104,20]]},"1428":{"position":[[2105,20]]}}}],["g\\ell_gℓg",{"_index":2209,"t":{"1347":{"position":[[2080,11]]},"1428":{"position":[[2081,11]]}}}],["gamma",{"_index":2205,"t":{"1347":{"position":[[1770,8]]},"1428":{"position":[[1771,8]]},"1454":{"position":[[6152,8]]}}}],["gamma(p)c=γ(p",{"_index":2645,"t":{"1454":{"position":[[6198,15]]}}}],["gamma+\\betaadaln(x)=σ(x)x−μ(x",{"_index":2202,"t":{"1347":{"position":[[1668,36]]},"1428":{"position":[[1669,36]]}}}],["gan",{"_index":373,"t":{"917":{"position":[[0,9],[268,86]]},"919":{"position":[[296,98]]},"1059":{"position":[[50,8]]},"1185":{"position":[[50,8]]},"1354":{"position":[[466,5],[795,3]]},"1397":{"position":[[17,41],[59,3],[276,3]]},"1433":{"position":[[0,4],[778,3],[826,3],[842,3]]}}}],["gan模型的结构分为generator和discriminator，其中generator接受来自随机分布的向量，产生预期图像；discrimin",{"_index":804,"t":{"1034":{"position":[[0,221]]}}}],["gan（gen",{"_index":374,"t":{"919":{"position":[[0,14]]},"1397":{"position":[[181,14]]}}}],["gan）或变分自动编码器（vae）等生成模型，uncondit",{"_index":380,"t":{"921":{"position":[[269,45]]}}}],["gaussian",{"_index":1334,"t":{"1138":{"position":[[751,9]]},"1454":{"position":[[797,8]]}}}],["gb",{"_index":2633,"t":{"1454":{"position":[[5576,2]]}}}],["gcc",{"_index":119,"t":{"840":{"position":[[654,3],[973,3]]}}}],["geeko",{"_index":79,"t":{"830":{"position":[[110,12],[123,7],[206,25],[270,16]]},"832":{"position":[[70,15]]},"834":{"position":[[0,18]]},"838":{"position":[[0,10],[34,14]]},"840":{"position":[[34,8],[360,8],[584,8],[1149,8],[1371,7],[2132,15]]},"844":{"position":[[0,8],[640,6]]},"846":{"position":[[0,16]]}}}],["geekos!\\n",{"_index":293,"t":{"844":{"position":[[2093,12]]}}}],["geekos/bootinfo.h",{"_index":258,"t":{"844":{"position":[[1014,19]]}}}],["geekos/crc32.h",{"_index":262,"t":{"844":{"position":[[1121,16]]}}}],["geekos/int.h",{"_index":264,"t":{"844":{"position":[[1171,14]]}}}],["geekos/keyboard.h",{"_index":268,"t":{"844":{"position":[[1274,19]]}}}],["geekos/kthread.h",{"_index":265,"t":{"844":{"position":[[1195,18]]}}}],["geekos/mem.h",{"_index":261,"t":{"844":{"position":[[1097,14]]}}}],["geekos/screen.h",{"_index":260,"t":{"844":{"position":[[1070,17]]}}}],["geekos/string.h",{"_index":259,"t":{"844":{"position":[[1043,17]]}}}],["geekos/timer.h",{"_index":267,"t":{"844":{"position":[[1248,16]]}}}],["geekos/trap.h",{"_index":266,"t":{"844":{"position":[[1223,15]]}}}],["geekos/tss.h",{"_index":263,"t":{"844":{"position":[[1147,14]]}}}],["geekos中makefil",{"_index":105,"t":{"840":{"position":[[193,23],[238,19]]}}}],["geekos是一个基于x86",{"_index":76,"t":{"830":{"position":[[0,27]]}}}],["geekos环境的配置，下面我们来验证环境配置的成功与否以及project",{"_index":202,"t":{"842":{"position":[[0,49]]}}}],["geekos设计的7",{"_index":82,"t":{"830":{"position":[[174,20]]}}}],["geekos运行依托于boch",{"_index":89,"t":{"832":{"position":[[21,17]]}}}],["gelu（高斯误差线性单元），⊙\\odot",{"_index":2185,"t":{"1347":{"position":[[872,22]]},"1428":{"position":[[872,22]]}}}],["gen",{"_index":2699,"t":{"1459":{"position":[[79,4]]}}}],["gener",{"_index":136,"t":{"840":{"position":[[907,9]]},"919":{"position":[[106,15]]},"921":{"position":[[4,33],[159,11],[185,83],[315,89],[417,11]]},"1022":{"position":[[114,10]]},"1026":{"position":[[0,10],[18,10],[267,11],[771,10],[934,11]]},"1038":{"position":[[41,10]]},"1071":{"position":[[757,9]]},"1089":{"position":[[23,10]]},"1164":{"position":[[41,10]]},"1197":{"position":[[757,9]]},"1277":{"position":[[68,10]]},"1279":{"position":[[63,10],[753,11],[846,9],[995,10]]},"1324":{"position":[[276,10],[1028,10],[1043,7],[1389,11]]},"1340":{"position":[[45,10],[151,10],[331,10],[455,10],[729,10],[926,10],[1216,10],[1345,10]]},"1342":{"position":[[423,10]]},"1349":{"position":[[84,10]]},"1351":{"position":[[241,9]]},"1354":{"position":[[0,10],[314,10],[616,10]]},"1362":{"position":[[68,10]]},"1364":{"position":[[63,10],[753,11],[846,9],[995,10]]},"1397":{"position":[[129,10]]},"1406":{"position":[[55,10],[386,10],[494,11],[1094,14],[1307,15],[1416,10]]},"1421":{"position":[[45,10],[151,10],[331,10],[455,10],[729,10],[926,10],[1216,10],[1345,10]]},"1423":{"position":[[423,10]]},"1431":{"position":[[39,10]]},"1433":{"position":[[782,11],[1192,10],[2003,10]]},"1437":{"position":[[25,10]]},"1449":{"position":[[56,10],[127,10]]},"1454":{"position":[[3362,10],[3485,8],[3610,8],[3807,9],[3885,11],[5877,11]]},"1457":{"position":[[80,10],[153,10]]},"1479":{"position":[[55,10],[386,10],[494,11],[1094,14],[1307,15],[1416,10]]},"1490":{"position":[[99,29]]}}}],["general_opt",{"_index":113,"t":{"840":{"position":[[490,15],[541,15],[1246,12],[1295,12]]}}}],["generation，可以继续扩展为多模态任务，text",{"_index":2750,"t":{"1490":{"position":[[63,28]]}}}],["generator_train",{"_index":914,"t":{"1057":{"position":[[16,53]]},"1183":{"position":[[16,53]]}}}],["generator）通常接收一个来自潜在空间（lat",{"_index":372,"t":{"913":{"position":[[0,33]]}}}],["geq",{"_index":1209,"t":{"1103":{"position":[[374,4],[383,4],[395,4],[591,4]]},"1203":{"position":[[57,4],[193,4]]}}}],["get_text_featur",{"_index":1087,"t":{"1069":{"position":[[801,17]]},"1195":{"position":[[801,17]]}}}],["ggg",{"_index":2173,"t":{"1347":{"position":[[437,10]]},"1428":{"position":[[437,10]]}}}],["git+https://github.com/openai/clip.git",{"_index":836,"t":{"1043":{"position":[[170,38]]},"1169":{"position":[[170,38]]}}}],["github",{"_index":1244,"t":{"1118":{"position":[[89,6]]},"1327":{"position":[[14,9]]},"1461":{"position":[[30,6]]}}}],["give",{"_index":519,"t":{"950":{"position":[[228,4]]}}}],["given",{"_index":1248,"t":{"1120":{"position":[[24,5]]},"1454":{"position":[[196,5],[747,5]]}}}],["global",{"_index":2789,"t":{"1514":{"position":[[136,6]]}}}],["global_clip_loss",{"_index":911,"t":{"1055":{"position":[[966,27]]},"1071":{"position":[[892,16]]},"1181":{"position":[[966,27]]},"1197":{"position":[[892,16]]}}}],["global_step",{"_index":1230,"t":{"1114":{"position":[[232,14]]},"1116":{"position":[[198,14]]}}}],["glu",{"_index":2179,"t":{"1347":{"position":[[587,3]]},"1428":{"position":[[587,3]]}}}],["glue",{"_index":1125,"t":{"1083":{"position":[[97,5],[851,4]]}}}],["glue（gener",{"_index":1126,"t":{"1083":{"position":[[103,12]]}}}],["glu（gat",{"_index":2175,"t":{"1347":{"position":[[527,9]]},"1428":{"position":[[527,9]]}}}],["gnu",{"_index":141,"t":{"840":{"position":[[999,3]]}}}],["god",{"_index":518,"t":{"950":{"position":[[216,4]]}}}],["good",{"_index":517,"t":{"950":{"position":[[211,4]]}}}],["googl",{"_index":837,"t":{"1045":{"position":[[14,6]]},"1059":{"position":[[13,6]]},"1122":{"position":[[187,6]]},"1171":{"position":[[14,6]]},"1185":{"position":[[13,6]]}}}],["gpt",{"_index":2359,"t":{"1406":{"position":[[434,3]]},"1415":{"position":[[338,3]]},"1454":{"position":[[3476,3],[4093,3]]},"1479":{"position":[[434,3]]},"1488":{"position":[[338,3]]}}}],["gpt、claud",{"_index":1088,"t":{"1071":{"position":[[426,10]]},"1197":{"position":[[426,10]]}}}],["gpt的结构是transformer的decod",{"_index":1168,"t":{"1087":{"position":[[0,39]]}}}],["gpt的自监督学习的特征体现在：在训练过程中，gpt根据输入的token预测输入的下一个token应该是什么，对输出的distribution与ground",{"_index":1169,"t":{"1087":{"position":[[40,78]]}}}],["gpu",{"_index":2004,"t":{"1324":{"position":[[442,3]]},"1454":{"position":[[5595,3],[5750,3],[8755,3]]}}}],["gradient",{"_index":1683,"t":{"1251":{"position":[[297,8],[756,8]]},"1475":{"position":[[242,9]]}}}],["gradual",{"_index":2556,"t":{"1454":{"position":[[891,9]]}}}],["grammar，cfg",{"_index":323,"t":{"861":{"position":[[59,12]]}}}],["grammar，rg",{"_index":330,"t":{"861":{"position":[[265,11]]}}}],["gray",{"_index":294,"t":{"844":{"position":[[2137,7]]}}}],["greatli",{"_index":2022,"t":{"1324":{"position":[[875,7]]},"1351":{"position":[[256,7]]}}}],["green",{"_index":2550,"t":{"1454":{"position":[[644,5]]}}}],["green|bright",{"_index":291,"t":{"844":{"position":[[2059,15]]}}}],["grid",{"_index":2872,"t":{"1556":{"position":[[207,6],[369,8],[472,6]]}}}],["ground",{"_index":2662,"t":{"1454":{"position":[[6669,6],[7882,6]]}}}],["grow",{"_index":1390,"t":{"1138":{"position":[[2845,4]]}}}],["guarante",{"_index":2719,"t":{"1469":{"position":[[177,9]]}}}],["guid",{"_index":1243,"t":{"1118":{"position":[[80,5]]},"1324":{"position":[[237,7]]},"1454":{"position":[[121,6],[227,5]]},"1459":{"position":[[34,6]]}}}],["guidanc",{"_index":2207,"t":{"1347":{"position":[[1952,21]]},"1428":{"position":[[1953,21]]},"1454":{"position":[[0,23],[34,8],[48,8],[73,8],[92,9],[1143,9],[1223,9],[5782,9],[5943,8]]}}}],["guidance的比例因子，用于控制条件logit和无条件logit",{"_index":2219,"t":{"1347":{"position":[[2286,40]]},"1428":{"position":[[2287,40]]}}}],["h",{"_index":237,"t":{"844":{"position":[[703,2]]},"1235":{"position":[[20,1],[155,2],[442,1],[460,2]]},"1447":{"position":[[167,2]]},"1509":{"position":[[54,1]]},"1556":{"position":[[305,2]]},"1564":{"position":[[27,2],[150,2],[193,2]]},"1566":{"position":[[21,2]]},"1568":{"position":[[21,2],[94,2]]},"1574":{"position":[[53,2]]}}}],["h(p",{"_index":1507,"t":{"1208":{"position":[[309,4],[1154,4],[1206,4],[1224,4]]}}}],["h(p)=−∑inpi",{"_index":1504,"t":{"1208":{"position":[[259,11]]}}}],["h(u,v)=11+(2−1)[d(u,v)d0]2n(16)h(u,v)=\\frac{1}{1+(\\sqrt{2",{"_index":599,"t":{"979":{"position":[[401,58]]}}}],["h(u,v)=11+(2−1)[d0d(u,v)]2n(20)h(u,v)=\\frac{1}{1+(\\sqrt{2",{"_index":621,"t":{"981":{"position":[[214,58]]}}}],["h(u,v)=e−[d(u,v)d0]n(17)h(u,v)=",{"_index":605,"t":{"979":{"position":[[791,34]]}}}],["h(u,v)=e−[d0d(u,v)]n(21)h(u,v)=",{"_index":625,"t":{"981":{"position":[[429,34]]}}}],["h(u,v)={0d(u,v)<d0−w21d0−w2≤d(u,v)≤d0+w20d(u,v)≥d0+w2(23)h(u,v)=\\begin{cases}0&d(u,v)<d_0",{"_index":631,"t":{"983":{"position":[[59,89]]}}}],["h(u,v)={1d(u,v)<d0d(u,v)−d1d0−d1d0≤d(u,v)≤d10d(u,v)>d1(18)h(u,v)=\\begin{cases}1&d(u,v)<d_0",{"_index":609,"t":{"979":{"position":[[1020,92]]}}}],["h(u,v)={1d(u,v)<w10w1⩽d(u,v)≤w21d(u,v)>w2(24)h(u,v)=\\begin{cases}1&d(u,v)<w_1\\\\0&w_1\\leqsl",{"_index":637,"t":{"983":{"position":[[420,93]]}}}],["h(u,v)={1d(u,v)>d00d(u,v)≤d0(19)h(u,v)=\\begin{cases}1&d(u,v)>d_0",{"_index":617,"t":{"981":{"position":[[30,66]]}}}],["h(u,v)={1d(u,v)>d0d(u,v)−d1d0−d1d1≤d(u,v)≤d00d(u,v)<d1(22)h(u,v)=\\begin{cases}1&d(u,v)>d_0",{"_index":628,"t":{"981":{"position":[[537,92]]}}}],["h(u,v)={1d(u,v)≤d00d(u,v)>d0(14)h(u,v)=\\begin{cases}1&d(u,v)\\leq",{"_index":594,"t":{"979":{"position":[[71,64]]}}}],["h(u,v)h(u,v)h(u,v)下降到最大值的0.707时的d(u,v)d(u,v)d(u,v",{"_index":623,"t":{"981":{"position":[[345,68]]}}}],["h(u,v)h(u,v)h(u,v)下降到最大值的0.707时的d(u,v)d(u,v)d(u,v)作为截止频率d0d_0d0",{"_index":601,"t":{"979":{"position":[[532,69]]}}}],["h(u,v)h(u,v)h(u,v)下降到最大值的1/2时的d(u,v)d(u,v)d(u,v",{"_index":604,"t":{"979":{"position":[[728,62]]}}}],["h_k",{"_index":2434,"t":{"1413":{"position":[[1018,3]]},"1486":{"position":[[1018,3]]}}}],["han",{"_index":2242,"t":{"1354":{"position":[[220,8]]}}}],["happen",{"_index":1302,"t":{"1134":{"position":[[74,7]]}}}],["hat{f}\\|_2+\\lambda_\\text{p}\\mathcal{l}_\\text{p}(i\\hat{m})+\\lambda_\\text{g}\\mathcal{l}_\\text{g}(i\\hat{m})l=∥im−im^∥2​+∥f−f^​∥2​+λp​lp​(im^)+λg​lg​(im",{"_index":2398,"t":{"1410":{"position":[[691,151]]},"1483":{"position":[[691,151]]}}}],["hat{x}_\\theta",{"_index":2685,"t":{"1454":{"position":[[8280,14],[8389,14]]}}}],["hat{x}_\\theta(\\epsilon",{"_index":2647,"t":{"1454":{"position":[[6247,24]]}}}],["hat{x}_\\theta(z_{t_1",{"_index":2679,"t":{"1454":{"position":[[7923,23]]}}}],["head",{"_index":427,"t":{"936":{"position":[[57,4]]},"940":{"position":[[790,4],[913,4],[968,4],[1363,4]]},"1140":{"position":[[31,4]]},"1144":{"position":[[122,4]]},"1150":{"position":[[24,4]]},"1308":{"position":[[57,4]]},"1312":{"position":[[790,4],[913,4],[968,4],[1363,4]]}}}],["help",{"_index":1387,"t":{"1138":{"position":[[2774,5]]}}}],["hhfq",{"_index":1084,"t":{"1069":{"position":[[705,4]]},"1195":{"position":[[705,4]]}}}],["high",{"_index":2030,"t":{"1324":{"position":[[1106,4]]},"1340":{"position":[[1007,4]]},"1421":{"position":[[1007,4]]},"1452":{"position":[[302,4]]},"1457":{"position":[[58,4],[219,4]]}}}],["higher",{"_index":1498,"t":{"1208":{"position":[[90,6],[132,6]]}}}],["highest",{"_index":1361,"t":{"1138":{"position":[[1911,7],[4207,7]]}}}],["highli",{"_index":2037,"t":{"1324":{"position":[[1310,6]]}}}],["hinton提出的一种卷积神经网络模型，它主要应用于图像分类任务。在当时，alexnet的表现远远超过了其他参赛的网络模型，并且在imagenet",{"_index":1603,"t":{"1219":{"position":[[55,83]]}}}],["hit",{"_index":208,"t":{"844":{"position":[[114,3],[1326,3]]}}}],["hk×wkh_k",{"_index":2418,"t":{"1413":{"position":[[543,8],[862,8]]},"1486":{"position":[[543,8],[862,8]]}}}],["hold",{"_index":1898,"t":{"1279":{"position":[[20,5]]},"1364":{"position":[[20,5]]}}}],["hollings@cs.umd.edu",{"_index":244,"t":{"844":{"position":[[783,21]]}}}],["hollingsworth",{"_index":243,"t":{"844":{"position":[[769,13]]}}}],["honglak",{"_index":2247,"t":{"1354":{"position":[[270,7]]}}}],["hope",{"_index":1925,"t":{"1279":{"position":[[940,4]]},"1364":{"position":[[940,4]]}}}],["host",{"_index":127,"t":{"840":{"position":[[735,4],[805,4]]}}}],["host_cc",{"_index":139,"t":{"840":{"position":[[962,7]]}}}],["hovemey",{"_index":238,"t":{"844":{"position":[[706,9]]}}}],["html",{"_index":1039,"t":{"1064":{"position":[[120,4]]},"1190":{"position":[[120,4]]}}}],["http://127.0.0.1:7890",{"_index":1738,"t":{"1270":{"position":[[199,23]]}}}],["http_proxi",{"_index":2762,"t":{"1500":{"position":[[244,10]]}}}],["http_proxy=http://127.0.0.1:7890",{"_index":2757,"t":{"1500":{"position":[[100,32]]}}}],["https://127.0.0.1:7890",{"_index":1740,"t":{"1270":{"position":[[251,24]]}}}],["https://arxiv.org/pdf/2112.10752",{"_index":1982,"t":{"1322":{"position":[[5,37]]}}}],["https://arxiv.org/pdf/2404.02905",{"_index":2749,"t":{"1477":{"position":[[0,37]]}}}],["https://arxiv.org/pdf/2406.06525",{"_index":2101,"t":{"1338":{"position":[[0,37]]},"1419":{"position":[[0,37]]}}}],["https://arxiv.org/pdf/2406.11838",{"_index":1892,"t":{"1277":{"position":[[0,37]]},"1362":{"position":[[0,37]]}}}],["https://blog.csdn.net/yeziyezi210/article/details/103864518",{"_index":1436,"t":{"1154":{"position":[[4,64]]}}}],["https://www.bilibili.com/video/bv12u411s7us/?spm_id_from=333.788&vd_source=24d8fcf68bc0e2b0003defe0995cf533",{"_index":356,"t":{"893":{"position":[[284,107]]}}}],["https://www.bilibili.com/video/bv13r4y1m7sq/?spm_id_from=333.788&vd_source=24d8fcf68bc0e2b0003defe0995cf533",{"_index":363,"t":{"893":{"position":[[732,107]]}}}],["https://www.bilibili.com/video/bv1pl4y1e7re/?spm_id_from=333.788&vd_source=24d8fcf68bc0e2b0003defe0995cf533",{"_index":353,"t":{"893":{"position":[[121,107]]}}}],["https://www.bilibili.com/video/bv1vm4y1q7xb/?spm_id_from=333.788&vd_source=24d8fcf68bc0e2b0003defe0995cf533",{"_index":359,"t":{"893":{"position":[[458,107]]}}}],["https://www.jianshu.com/p/41c15d301542",{"_index":2513,"t":{"1447":{"position":[[5,38]]}}}],["https_proxi",{"_index":2763,"t":{"1500":{"position":[[261,11]]}}}],["https_proxy=https://127.0.0.1:7890",{"_index":2758,"t":{"1500":{"position":[[140,34]]}}}],["human",{"_index":2602,"t":{"1454":{"position":[[3922,5]]}}}],["hundr",{"_index":2003,"t":{"1324":{"position":[[430,8]]}}}],["hung",{"_index":2278,"t":{"1354":{"position":[[702,4]]}}}],["hw4",{"_index":1288,"t":{"1124":{"position":[[53,4]]}}}],["hw×cihw",{"_index":1638,"t":{"1233":{"position":[[51,15]]}}}],["hyperparamet",{"_index":1635,"t":{"1230":{"position":[[80,14]]}}}],["hyperstyl",{"_index":1018,"t":{"1062":{"position":[[20,10],[180,10]]},"1188":{"position":[[20,10],[180,10]]}}}],["h×wh",{"_index":2420,"t":{"1413":{"position":[[600,4]]},"1486":{"position":[[600,4]]}}}],["h′以及w′h'以及w'h′以及w",{"_index":1650,"t":{"1235":{"position":[[177,24]]}}}],["i'll",{"_index":505,"t":{"950":{"position":[[73,4],[125,4]]}}}],["i)n(0,i",{"_index":1968,"t":{"1287":{"position":[[331,8]]},"1372":{"position":[[331,8]]}}}],["i)xt​∼n(0,i",{"_index":1973,"t":{"1287":{"position":[[410,12]]},"1372":{"position":[[410,12]]}}}],["i,ji,ji,j代表输出神经元的二维索引坐标，h,wh,wh,w",{"_index":1657,"t":{"1240":{"position":[[27,49]]}}}],["i440fxsupport",{"_index":190,"t":{"840":{"position":[[1880,14]]},"846":{"position":[[824,14]]}}}],["i\\hat{m}\\|_2+\\|f",{"_index":2397,"t":{"1410":{"position":[[674,16]]},"1483":{"position":[[674,16]]}}}],["i\\theta_iθi",{"_index":2196,"t":{"1347":{"position":[[1347,13]]},"1428":{"position":[[1348,13]]}}}],["iccv",{"_index":977,"t":{"1059":{"position":[[1795,4]]},"1185":{"position":[[1795,4]]}}}],["iclr",{"_index":2249,"t":{"1354":{"position":[[350,5]]},"1454":{"position":[[102,4],[1153,4]]},"1457":{"position":[[114,4]]}}}],["icom",{"_index":2798,"t":{"1514":{"position":[[289,4]]}}}],["icon",{"_index":2796,"t":{"1514":{"position":[[239,4],[256,5],[304,5]]}}}],["id",{"_index":2730,"t":{"1475":{"position":[[67,3]]}}}],["idea",{"_index":1615,"t":{"1226":{"position":[[114,4]]}}}],["ideal",{"_index":590,"t":{"979":{"position":[[48,5]]}}}],["ident",{"_index":986,"t":{"1059":{"position":[[2040,8]]},"1185":{"position":[[2040,8]]}}}],["identifi",{"_index":2665,"t":{"1454":{"position":[[6733,11],[7138,12],[7172,12],[7230,10],[7288,10],[7544,10]]}}}],["idx",{"_index":2811,"t":{"1525":{"position":[[99,3],[174,4],[554,6]]},"1527":{"position":[[186,3],[261,4],[641,6]]}}}],["if(!((keycod",{"_index":214,"t":{"844":{"position":[[186,13],[1398,13]]}}}],["if((keycod",{"_index":219,"t":{"844":{"position":[[303,11],[1515,11]]}}}],["if(read_key(&keycod",{"_index":213,"t":{"844":{"position":[[161,22],[1373,22]]}}}],["ifm",{"_index":2739,"t":{"1475":{"position":[[266,3],[300,3],[339,3],[379,3]]}}}],["ihpf",{"_index":616,"t":{"981":{"position":[[15,14]]}}}],["iii",{"_index":1176,"t":{"1099":{"position":[[353,3]]},"1347":{"position":[[1338,3],[1365,3]]},"1428":{"position":[[1339,3],[1366,3]]}}}],["illustr",{"_index":1289,"t":{"1126":{"position":[[16,12]]},"1454":{"position":[[594,10]]}}}],["ilpf",{"_index":588,"t":{"979":{"position":[[18,14],[37,4]]}}}],["im^=d(f^)\\hat{im}=\\mathcal{d}(\\hat{f})im^=d(f",{"_index":2395,"t":{"1410":{"position":[[569,48]]},"1483":{"position":[[569,48]]}}}],["imag",{"_index":747,"t":{"1022":{"position":[[845,5]]},"1024":{"position":[[977,5]]},"1057":{"position":[[1104,5]]},"1059":{"position":[[1724,5]]},"1069":{"position":[[282,5],[366,5],[513,5],[844,5]]},"1071":{"position":[[45,35],[128,8]]},"1183":{"position":[[1104,5]]},"1185":{"position":[[1724,5]]},"1195":{"position":[[282,5],[366,5],[513,5],[844,5]]},"1197":{"position":[[45,35],[128,8]]},"1277":{"position":[[62,5]]},"1279":{"position":[[57,5],[840,5]]},"1324":{"position":[[19,5],[169,5],[270,5],[1251,5],[1290,5],[1383,5],[1409,5]]},"1340":{"position":[[39,5],[325,5],[405,5],[449,5],[559,5],[723,5],[920,5],[1031,7],[1210,5]]},"1342":{"position":[[353,5],[382,5],[417,5],[569,5]]},"1345":{"position":[[66,5],[244,5]]},"1351":{"position":[[0,10],[202,5]]},"1354":{"position":[[610,5]]},"1362":{"position":[[62,5]]},"1364":{"position":[[57,5],[840,5]]},"1406":{"position":[[121,6],[488,5],[833,5],[1147,5]]},"1410":{"position":[[180,5],[934,5],[1221,5],[1289,5],[1541,7],[1715,5],[1758,5],[1812,5],[1844,8]]},"1417":{"position":[[224,5],[294,5]]},"1421":{"position":[[39,5],[325,5],[405,5],[449,5],[559,5],[723,5],[920,5],[1031,7],[1210,5]]},"1423":{"position":[[353,5],[382,5],[417,5],[569,5]]},"1426":{"position":[[66,5],[244,5]]},"1431":{"position":[[31,7]]},"1433":{"position":[[1203,5],[1575,5],[2059,5]]},"1452":{"position":[[26,5],[318,6]]},"1454":{"position":[[67,5],[128,5],[211,5],[350,5],[547,6],[704,6],[968,7],[1199,5],[1444,7],[1468,5],[3294,5],[3376,5],[3628,6],[3830,6],[3910,6],[5129,5],[5776,5],[5835,5],[5986,5],[6031,5],[6881,6],[7857,5]]},"1459":{"position":[[131,5]]},"1479":{"position":[[121,6],[488,5],[833,5],[1147,5]]},"1483":{"position":[[180,5],[934,5],[1221,5],[1289,5],[1541,7],[1715,5],[1758,5],[1812,5],[1844,8]]},"1492":{"position":[[224,5],[294,5]]}}}],["imagen",{"_index":2527,"t":{"1452":{"position":[[50,6]]},"1454":{"position":[[7311,7],[7522,6],[8688,6]]},"1457":{"position":[[44,6]]}}}],["imagenet",{"_index":960,"t":{"1059":{"position":[[1009,8],[1327,10],[1364,8]]},"1185":{"position":[[1009,8],[1327,10],[1364,8]]},"1340":{"position":[[669,8],[807,8]]},"1406":{"position":[[509,8]]},"1421":{"position":[[669,8],[807,8]]},"1479":{"position":[[509,8]]}}}],["imagen将压缩版本的图片作为gener",{"_index":727,"t":{"1022":{"position":[[353,28]]}}}],["imagen的实验结果，相对于decoder即diffus",{"_index":731,"t":{"1022":{"position":[[447,34]]}}}],["image数据，在传入visdom时仍需要先转化为numpi",{"_index":1234,"t":{"1114":{"position":[[355,35]]}}}],["image，encod",{"_index":2387,"t":{"1410":{"position":[[83,13]]},"1483":{"position":[[83,13]]}}}],["impact",{"_index":745,"t":{"1022":{"position":[[791,9]]}}}],["implant",{"_index":2666,"t":{"1454":{"position":[[6766,7]]}}}],["implement",{"_index":2722,"t":{"1471":{"position":[[15,10],[63,12]]}}}],["import",{"_index":547,"t":{"961":{"position":[[78,9]]},"975":{"position":[[30,9]]},"1018":{"position":[[232,9]]},"1022":{"position":[[623,9]]},"1114":{"position":[[12,6]]},"1116":{"position":[[12,6]]},"1126":{"position":[[0,9]]},"1129":{"position":[[85,9]]},"1138":{"position":[[0,9],[3234,6],[3247,6],[3279,6],[3298,6]]},"1144":{"position":[[207,9]]},"1201":{"position":[[268,6],[291,6]]},"1203":{"position":[[259,6],[282,6]]},"1208":{"position":[[1468,6]]},"1270":{"position":[[0,6],[13,6],[43,6],[73,6],[105,6],[141,6],[162,6]]},"1406":{"position":[[1250,9]]},"1439":{"position":[[0,9]]},"1441":{"position":[[259,6],[278,6]]},"1479":{"position":[[1250,9]]}}}],["improv",{"_index":740,"t":{"1022":{"position":[[702,8]]},"1406":{"position":[[555,7],[578,9]]},"1479":{"position":[[555,7],[578,9]]}}}],["incept",{"_index":753,"t":{"1024":{"position":[[57,9]]},"1059":{"position":[[67,9],[116,9],[602,9],[855,9],[923,9],[1212,9],[1309,9],[1338,9],[1502,9],[1738,9],[1859,9],[2440,9]]},"1185":{"position":[[67,9],[116,9],[602,9],[855,9],[923,9],[1212,9],[1309,9],[1338,9],[1502,9],[1738,9],[1859,9],[2440,9]]},"1406":{"position":[[596,9],[641,9]]},"1479":{"position":[[596,9],[641,9]]}}}],["includ",{"_index":2,"t":{"803":{"position":[[0,8]]},"814":{"position":[[0,8]]},"824":{"position":[[0,8]]},"844":{"position":[[1005,8],[1034,8],[1061,8],[1088,8],[1112,8],[1138,8],[1162,8],[1186,8],[1214,8],[1239,8],[1265,8]]},"1279":{"position":[[708,9]]},"1324":{"position":[[1359,9]]},"1364":{"position":[[708,9]]},"1406":{"position":[[823,9],[1137,9]]},"1479":{"position":[[823,9],[1137,9]]},"1527":{"position":[[41,8]]}}}],["increas",{"_index":2544,"t":{"1454":{"position":[[381,8]]}}}],["indentifi",{"_index":2671,"t":{"1454":{"position":[[7096,12]]}}}],["induct",{"_index":2110,"t":{"1340":{"position":[[261,9]]},"1421":{"position":[[261,9]]}}}],["infer",{"_index":1135,"t":{"1083":{"position":[[401,37]]},"1324":{"position":[[455,9]]},"1340":{"position":[[1191,9]]},"1406":{"position":[[698,9],[848,9]]},"1421":{"position":[[1191,9]]},"1454":{"position":[[3859,9]]},"1479":{"position":[[698,9],[848,9]]}}}],["inference.pi",{"_index":1023,"t":{"1062":{"position":[[218,27]]},"1188":{"position":[[218,27]]}}}],["inform",{"_index":1501,"t":{"1208":{"position":[[116,11]]}}}],["infty}^{+\\infty}f(x)",{"_index":549,"t":{"961":{"position":[[333,24]]}}}],["infty}^{\\infty}\\int_",{"_index":556,"t":{"961":{"position":[[582,22],[763,22]]}}}],["infty}^{\\infty}f(u)e^{j2\\pi",{"_index":553,"t":{"961":{"position":[[451,28]]}}}],["infty}^{\\infty}f(u,v)e^{j2\\pi",{"_index":560,"t":{"961":{"position":[[786,30]]}}}],["infty}^{\\infty}f(x,y)",{"_index":557,"t":{"961":{"position":[[605,25]]}}}],["init",{"_index":275,"t":{"844":{"position":[[1816,4]]}}}],["init_bss",{"_index":280,"t":{"844":{"position":[[1873,11]]}}}],["init_crc32",{"_index":283,"t":{"844":{"position":[[1920,13]]}}}],["init_interrupt",{"_index":285,"t":{"844":{"position":[[1946,18]]}}}],["init_keyboard",{"_index":289,"t":{"844":{"position":[[2011,16]]}}}],["init_mem(bootinfo",{"_index":282,"t":{"844":{"position":[[1900,19]]}}}],["init_schedul",{"_index":286,"t":{"844":{"position":[[1965,17]]}}}],["init_screen",{"_index":281,"t":{"844":{"position":[[1885,14]]}}}],["init_tim",{"_index":288,"t":{"844":{"position":[[1997,13]]}}}],["init_trap",{"_index":287,"t":{"844":{"position":[[1983,13]]}}}],["init_tss",{"_index":284,"t":{"844":{"position":[[1934,11]]}}}],["init_weights(m",{"_index":1788,"t":{"1270":{"position":[[1431,16]]}}}],["initi",{"_index":270,"t":{"844":{"position":[[1752,11]]},"1138":{"position":[[3486,10]]},"1263":{"position":[[553,10]]},"1406":{"position":[[1223,9]]},"1479":{"position":[[1223,9]]}}}],["initialis",{"_index":1332,"t":{"1138":{"position":[[684,11],[800,14]]}}}],["inlin",{"_index":2793,"t":{"1514":{"position":[[194,6],[317,6]]}}}],["inpaint",{"_index":2036,"t":{"1324":{"position":[[1257,10]]}}}],["input",{"_index":1111,"t":{"1081":{"position":[[8,6]]},"1134":{"position":[[37,6],[157,6]]},"1138":{"position":[[299,6],[334,6],[386,6],[1284,6],[1309,6],[1335,6],[1750,38],[3344,6],[3381,6],[3433,6],[3848,6],[3873,6],[3899,6]]},"1324":{"position":[[1064,6]]},"1454":{"position":[[205,5],[302,6],[4068,5],[4145,5],[4269,5],[5167,5]]}}}],["input_size)，batch_size位于第二维度！虽然可以将batch_size和序列长度seq_len对换位置，此时只需令batch_first=tru",{"_index":2516,"t":{"1447":{"position":[[204,83]]}}}],["inputembedding(pos,i)=wordembedding(pos,i)+positionencoding(pos,i)(4)inputembedding(pos,i)=wordembedding(pos,i)+positionencoding(pos,i",{"_index":422,"t":{"934":{"position":[[1097,135]]},"1306":{"position":[[1097,135]]}}}],["instal",{"_index":827,"t":{"1043":{"position":[[46,7],[128,7],[162,7]]},"1109":{"position":[[4,7]]},"1169":{"position":[[46,7],[128,7],[162,7]]},"1471":{"position":[[176,7]]},"1473":{"position":[[92,7]]}}}],["instanc",{"_index":933,"t":{"1057":{"position":[[872,8]]},"1183":{"position":[[872,8]]},"1454":{"position":[[6786,8]]}}}],["instruct",{"_index":2593,"t":{"1454":{"position":[[3308,13],[3494,12],[3842,13],[3936,13],[4104,11],[4177,11],[4321,11]]}}}],["instructpix2pix",{"_index":2592,"t":{"1454":{"position":[[3258,16],[3757,15]]}}}],["int",{"_index":7,"t":{"803":{"position":[[46,3],[63,4],[96,3]]},"805":{"position":[[157,3],[174,4]]},"814":{"position":[[46,3],[79,3],[134,3],[151,4]]},"824":{"position":[[46,3],[90,3],[231,4],[299,3],[397,4]]},"844":{"position":[[268,3],[1480,3]]},"1051":{"position":[[670,3]]},"1177":{"position":[[670,3]]},"1525":{"position":[[26,3],[95,3],[129,3],[154,3]]},"1527":{"position":[[113,3],[182,3],[216,3],[241,3],[766,3],[825,3],[864,3]]}}}],["interact",{"_index":1307,"t":{"1134":{"position":[[167,8],[303,12]]}}}],["interpol",{"_index":2438,"t":{"1415":{"position":[[143,11]]},"1488":{"position":[[143,11]]}}}],["introduc",{"_index":2026,"t":{"1324":{"position":[[912,11]]},"1340":{"position":[[3,9]]},"1421":{"position":[[3,9]]}}}],["introduct",{"_index":2701,"t":{"1461":{"position":[[52,42]]}}}],["intuit",{"_index":2356,"t":{"1406":{"position":[[281,9]]},"1479":{"position":[[281,9]]}}}],["invers",{"_index":1025,"t":{"1062":{"position":[[355,9]]},"1188":{"position":[[355,9]]}}}],["ios::sync_with_stdio(fals",{"_index":75,"t":{"826":{"position":[[346,28]]}}}],["ipl",{"_index":815,"t":{"1038":{"position":[[9,3],[27,3]]},"1041":{"position":[[16,3],[46,3]]},"1059":{"position":[[1053,3],[1100,3],[1593,3],[1640,3],[2121,3],[2125,4],[2477,3],[2481,4]]},"1069":{"position":[[2,3],[927,3]]},"1071":{"position":[[914,3],[1189,3]]},"1164":{"position":[[9,3],[27,3]]},"1167":{"position":[[16,3],[46,3]]},"1185":{"position":[[1053,3],[1100,3],[1593,3],[1640,3],[2121,3],[2125,4],[2477,3],[2481,4]]},"1195":{"position":[[2,3],[927,3]]},"1197":{"position":[[914,3],[1189,3]]},"1359":{"position":[[0,11]]}}}],["ipl独立于生成模型，可以自由选择diffus",{"_index":2236,"t":{"1351":{"position":[[286,64]]}}}],["is=exp⁡(ex[kl(p(y∣x)∣∣p(y))])is=\\exp\\left(\\mathbb{e}_x[kl(p(y|x)||p(y))]\\right)is=exp(ex​[kl(p(y∣x)∣∣p(i",{"_index":953,"t":{"1059":{"position":[[627,108]]},"1185":{"position":[[627,108]]}}}],["isclos",{"_index":2733,"t":{"1475":{"position":[[128,12]]}}}],["isinstance(net",{"_index":1769,"t":{"1270":{"position":[[955,15]]}}}],["isinstance(train_lay",{"_index":923,"t":{"1057":{"position":[[347,24]]},"1183":{"position":[[347,24]]}}}],["isinstance(x",{"_index":1775,"t":{"1270":{"position":[[1114,13]]}}}],["is（incept",{"_index":988,"t":{"1059":{"position":[[2084,12]]},"1185":{"position":[[2084,12]]}}}],["it'",{"_index":1495,"t":{"1208":{"position":[[62,4]]}}}],["item",{"_index":2508,"t":{"1445":{"position":[[231,4],[440,4]]}}}],["iter",{"_index":1622,"t":{"1228":{"position":[[8,9],[272,9]]}}}],["iulian",{"_index":246,"t":{"844":{"position":[[827,6]]}}}],["j",{"_index":2325,"t":{"1386":{"position":[[617,2]]}}}],["j2\\pi",{"_index":550,"t":{"961":{"position":[[358,5],[631,5],[1252,5]]}}}],["j\\frac{2\\pi",{"_index":564,"t":{"961":{"position":[[953,11]]}}}],["jae",{"_index":2267,"t":{"1354":{"position":[[559,3]]}}}],["jeffrey",{"_index":241,"t":{"844":{"position":[[758,7]]}}}],["jingwan",{"_index":2262,"t":{"1354":{"position":[[526,7]]}}}],["jinwoo",{"_index":2254,"t":{"1354":{"position":[[393,6]]}}}],["j}\\right)p(xi,j​∣x<i,j",{"_index":2327,"t":{"1386":{"position":[[632,24]]}}}],["j}xi,j",{"_index":2329,"t":{"1386":{"position":[[676,7]]}}}],["k",{"_index":242,"t":{"844":{"position":[[766,2]]},"1101":{"position":[[4,1],[42,1],[77,1],[100,1],[237,1],[368,1],[437,1],[496,1]]},"1103":{"position":[[46,1],[423,5],[429,1]]},"1105":{"position":[[66,1]]},"1148":{"position":[[289,1],[338,1]]},"1226":{"position":[[0,1],[126,1]]},"1228":{"position":[[21,1]]}}}],["k_h",{"_index":1645,"t":{"1235":{"position":[[73,3]]}}}],["k_wco​×ci​×kh​×kw",{"_index":1646,"t":{"1235":{"position":[[84,18]]}}}],["kaim",{"_index":1336,"t":{"1138":{"position":[[772,7]]}}}],["kaiyang",{"_index":2293,"t":{"1356":{"position":[[177,7]]}}}],["katex",{"_index":2717,"t":{"1469":{"position":[[126,5],[229,5],[278,6]]}}}],["kernel",{"_index":199,"t":{"840":{"position":[[2050,7]]},"844":{"position":[[1723,6],[1764,6],[2162,6]]},"846":{"position":[[994,7]]}}}],["kernel_size=5",{"_index":1752,"t":{"1270":{"position":[[574,14],[673,15]]}}}],["kernel_thread",{"_index":228,"t":{"844":{"position":[[542,13],[2224,13]]}}}],["key",{"_index":298,"t":{"844":{"position":[[2192,4]]},"1138":{"position":[[929,5],[1252,5],[1277,4],[1369,5],[2751,3],[2835,3],[3513,5],[3816,5],[3841,4],[3933,5]]}}}],["key^{n",{"_index":1406,"t":{"1140":{"position":[[135,6]]}}}],["key_ctrl_flag)==key_ctrl_flag",{"_index":220,"t":{"844":{"position":[[317,29],[1529,29]]}}}],["key_release_flag",{"_index":216,"t":{"844":{"position":[[234,21],[1446,21]]}}}],["key_special_flag",{"_index":215,"t":{"844":{"position":[[202,17],[1414,17]]}}}],["keyboard_serial_delay",{"_index":183,"t":{"840":{"position":[[1776,22]]},"846":{"position":[[720,22]]}}}],["keycod",{"_index":211,"t":{"844":{"position":[[133,7],[141,8],[223,8],[284,7],[1345,7],[1353,8],[1435,8],[1496,7]]}}}],["keys.t",{"_index":1354,"t":{"1138":{"position":[[1706,6],[4076,6]]}}}],["key，query以及valu",{"_index":1313,"t":{"1136":{"position":[[0,140]]},"1138":{"position":[[1222,19]]}}}],["kh=kw=1k_h=k_w=1kh​=kw​=1",{"_index":1637,"t":{"1233":{"position":[[0,50]]}}}],["kien",{"_index":2281,"t":{"1354":{"position":[[736,4]]}}}],["kkk",{"_index":1618,"t":{"1226":{"position":[[179,3],[239,3]]},"1228":{"position":[[57,3],[183,3]]},"1345":{"position":[[225,6]]},"1413":{"position":[[472,3],[895,3],[991,3],[1065,3]]},"1415":{"position":[[686,4]]},"1426":{"position":[[225,6]]},"1433":{"position":[[1340,3],[1437,3],[1490,3]]},"1486":{"position":[[472,3],[895,3],[991,3],[1065,3]]},"1488":{"position":[[686,5]]}}}],["kl",{"_index":956,"t":{"1059":{"position":[[825,2]]},"1185":{"position":[[825,2]]},"1208":{"position":[[471,5],[477,15],[687,11]]},"1332":{"position":[[598,10]]}}}],["kl(p(y∣x)∣∣p(y))kl(p(y|x)||p(y))kl(p(y∣x)∣∣p(i",{"_index":955,"t":{"1059":{"position":[[757,59]]},"1185":{"position":[[757,59]]}}}],["knife",{"_index":513,"t":{"950":{"position":[[173,6]]}}}],["kpple",{"_index":2784,"t":{"1514":{"position":[[30,5]]}}}],["krizhevsky、ilya",{"_index":1601,"t":{"1219":{"position":[[20,15]]}}}],["kruskal",{"_index":2805,"t":{"1519":{"position":[[90,9]]}}}],["kullback–leibl",{"_index":1514,"t":{"1208":{"position":[[493,16]]}}}],["k}r≤k",{"_index":2437,"t":{"1413":{"position":[[1148,11]]},"1486":{"position":[[1148,11]]}}}],["k−1k",{"_index":1624,"t":{"1228":{"position":[[112,4]]}}}],["l",{"_index":367,"t":{"898":{"position":[[348,1],[461,1]]},"900":{"position":[[84,1]]},"902":{"position":[[76,1]]},"973":{"position":[[208,1]]},"1270":{"position":[[2035,1]]}}}],["l(w",{"_index":1571,"t":{"1214":{"position":[[43,4]]},"1216":{"position":[[314,4]]}}}],["l(w,b",{"_index":1568,"t":{"1214":{"position":[[4,6],[124,6]]}}}],["l(w,b)+λ2∥w∥12(2)l(w",{"_index":1582,"t":{"1216":{"position":[[47,21]]}}}],["l(w_t",{"_index":1594,"t":{"1216":{"position":[[473,6]]}}}],["l(x,e,θ)=et,ϵ[∥ϵ−fθ(xt,t,e)∥22]\\mathcal{l}(\\mathbf{x",{"_index":2570,"t":{"1454":{"position":[[1829,54]]}}}],["l(y,z)=max(0,−y∗z)(1)l(y,z)=max(0",{"_index":1717,"t":{"1263":{"position":[[366,34]]}}}],["l(z,x)=eε,t[∥ε−εθ(xt∣t,z)∥2]l(z",{"_index":1938,"t":{"1285":{"position":[[179,32]]},"1370":{"position":[[179,32]]}}}],["l.backward",{"_index":1818,"t":{"1270":{"position":[[2054,12]]}}}],["l1正则化(硬性限制)、l2",{"_index":1565,"t":{"1212":{"position":[[204,35]]}}}],["l1正则化会使得一部分参数变为0，从而实现特征选择的效果；l2正则化则会使得模型参数尽量接近0",{"_index":1566,"t":{"1212":{"position":[[240,93]]}}}],["l1正则化限制权重参数的l1",{"_index":1579,"t":{"1214":{"position":[[154,26]]}}}],["l2",{"_index":938,"t":{"1057":{"position":[[1146,2],[1269,2]]},"1183":{"position":[[1146,2],[1269,2]]}}}],["l2l_2l2",{"_index":2165,"t":{"1345":{"position":[[161,8]]},"1426":{"position":[[161,8]]}}}],["l2正则化是指在模型的损失函数中，加入对模型参数的l2",{"_index":1581,"t":{"1216":{"position":[[0,46]]}}}],["l2范数是对元素求平方和后再开根号，需要.pow(2",{"_index":1485,"t":{"1206":{"position":[[0,36]]}}}],["l=5l=5l=5",{"_index":1430,"t":{"1152":{"position":[[13,19]]}}}],["l=ee(x),e(ci),ct,ε∼n(0,1),t[∥ε−ϵθ(zt,t,e(ci),ct))∥22]\\left.l=\\mathbb{e}_{\\mathcal{e}(x",{"_index":2614,"t":{"1454":{"position":[[4709,88]]}}}],["l=−1n∑i=1nyilog⁡(pi)(1)l",{"_index":1117,"t":{"1081":{"position":[[270,25]]}}}],["l=∥im−im^∥2+∥f−f^∥2+λplp(im^)+λglg(im^)\\mathcal{l}=\\|im",{"_index":2396,"t":{"1410":{"position":[[618,55]]},"1483":{"position":[[618,55]]}}}],["label",{"_index":1556,"t":{"1208":{"position":[[1626,5],[1672,6],[1809,5],[1858,6]]}}}],["laion",{"_index":2138,"t":{"1340":{"position":[[992,5]]},"1342":{"position":[[649,5]]},"1421":{"position":[[992,5]]},"1423":{"position":[[649,5]]}}}],["lalr(1",{"_index":361,"t":{"893":{"position":[[620,10]]}}}],["lambda",{"_index":1585,"t":{"1216":{"position":[[137,23],[337,7],[555,7]]},"1454":{"position":[[8371,7],[8564,37]]}}}],["lambda)w_t",{"_index":1593,"t":{"1216":{"position":[[439,11]]}}}],["lambda_l",{"_index":912,"t":{"1055":{"position":[[1219,9]]},"1181":{"position":[[1219,9]]}}}],["lambda_src",{"_index":913,"t":{"1055":{"position":[[1265,10]]},"1181":{"position":[[1265,10]]}}}],["landmark",{"_index":1030,"t":{"1062":{"position":[[539,8]]},"1188":{"position":[[539,8]]}}}],["languag",{"_index":749,"t":{"1022":{"position":[[878,8]]},"1024":{"position":[[968,8]]},"1081":{"position":[[193,8]]},"1083":{"position":[[116,8],[392,8]]},"1340":{"position":[[125,8]]},"1421":{"position":[[125,8]]},"1454":{"position":[[7735,32]]}}}],["lantent",{"_index":2295,"t":{"1359":{"position":[[34,19]]}}}],["laplac",{"_index":648,"t":{"999":{"position":[[187,10]]},"1001":{"position":[[339,22],[381,51]]}}}],["larg",{"_index":1391,"t":{"1138":{"position":[[2854,5],[2899,5]]},"1340":{"position":[[119,5]]},"1421":{"position":[[119,5]]}}}],["latent",{"_index":376,"t":{"919":{"position":[[122,35]]},"1022":{"position":[[220,17]]},"1026":{"position":[[456,11],[484,11],[1048,11],[1076,12]]},"1055":{"position":[[256,9]]},"1062":{"position":[[79,6],[462,6],[624,6]]},"1181":{"position":[[256,9]]},"1188":{"position":[[79,6],[462,6],[624,6]]},"1324":{"position":[[634,6],[1180,6]]},"1359":{"position":[[12,13],[223,11],[380,11],[587,12]]},"1454":{"position":[[4681,6]]},"1457":{"position":[[210,8],[256,6]]}}}],["later",{"_index":121,"t":{"840":{"position":[[668,5]]}}}],["latest",{"_index":171,"t":{"840":{"position":[[1563,6],[1636,6]]},"846":{"position":[[507,6],[580,6]]}}}],["latex",{"_index":2721,"t":{"1469":{"position":[[238,5]]}}}],["law",{"_index":2373,"t":{"1406":{"position":[[942,3],[954,4],[1288,4]]},"1408":{"position":[[61,4],[140,4],[190,4]]},"1413":{"position":[[1322,4]]},"1417":{"position":[[535,4]]},"1479":{"position":[[942,3],[954,4],[1288,4]]},"1481":{"position":[[61,4],[140,4],[190,4]]},"1486":{"position":[[1322,4]]},"1492":{"position":[[535,4]]}}}],["layer",{"_index":447,"t":{"940":{"position":[[485,34]]},"1055":{"position":[[588,6]]},"1057":{"position":[[325,5]]},"1067":{"position":[[399,6]]},"1181":{"position":[[588,6]]},"1183":{"position":[[325,5]]},"1193":{"position":[[399,6]]},"1312":{"position":[[485,34]]},"1324":{"position":[[940,6]]},"1347":{"position":[[157,5],[1483,5]]},"1410":{"position":[[1574,6],[1668,6]]},"1428":{"position":[[157,5],[1484,5]]},"1454":{"position":[[5209,6]]},"1483":{"position":[[1574,6],[1668,6]]}}}],["layernorm",{"_index":2168,"t":{"1347":{"position":[[210,9],[248,9],[288,9]]},"1428":{"position":[[210,9],[248,9],[288,9]]}}}],["layers.append",{"_index":904,"t":{"1055":{"position":[[634,14]]},"1181":{"position":[[634,14]]}}}],["layers.append(pixelnorm",{"_index":1066,"t":{"1067":{"position":[[780,26]]},"1193":{"position":[[780,26]]}}}],["layers.append(self.final_linear",{"_index":1074,"t":{"1067":{"position":[[1025,32]]},"1193":{"position":[[1025,32]]}}}],["layers.append(self.linear",{"_index":1071,"t":{"1067":{"position":[[882,26]]},"1193":{"position":[[882,26]]}}}],["layers.append(self.transformer_encod",{"_index":1064,"t":{"1067":{"position":[[683,39]]},"1193":{"position":[[683,39]]}}}],["layman’",{"_index":1303,"t":{"1134":{"position":[[101,8]]}}}],["ld",{"_index":142,"t":{"840":{"position":[[1003,2]]}}}],["ldm",{"_index":2034,"t":{"1324":{"position":[[1204,6]]},"1340":{"position":[[887,4]]},"1421":{"position":[[887,4]]},"1457":{"position":[[287,5]]}}}],["ldot",{"_index":1185,"t":{"1101":{"position":[[175,7]]},"1103":{"position":[[125,7],[388,6]]},"1386":{"position":[[163,7],[322,7]]},"1413":{"position":[[507,7],[680,7],[732,7],[936,7]]},"1486":{"position":[[507,7],[680,7],[732,7],[936,7]]}}}],["ldots,\\left(w_v",{"_index":1206,"t":{"1103":{"position":[[286,17]]}}}],["learn",{"_index":1103,"t":{"1071":{"position":[[1136,8]]},"1120":{"position":[[40,5]]},"1197":{"position":[[1136,8]]},"1226":{"position":[[55,8]]},"1356":{"position":[[138,18],[242,47]]},"1359":{"position":[[546,8]]},"1406":{"position":[[109,8],[346,5],[1439,9]]},"1433":{"position":[[2046,8]]},"1449":{"position":[[67,8]]},"1454":{"position":[[3275,8]]},"1479":{"position":[[109,8],[346,5],[1439,9]]}}}],["learning（ipl）方法来解决风格迁移任务中生成模型从源域到目标域的适应问题。一个lat",{"_index":2230,"t":{"1351":{"position":[[27,50]]}}}],["lecun等人于1998年提出的卷积神经网络结构，该结构由卷积层、池化层和全连接层组成，可以高效地处理手写数字图像，并在mnist",{"_index":1730,"t":{"1268":{"position":[[12,78]]}}}],["lee",{"_index":2248,"t":{"1354":{"position":[[278,4],[563,4]]}}}],["left",{"_index":2657,"t":{"1454":{"position":[[6489,6],[8266,6]]}}}],["left(\\theta_i\\right)+x_{i+1",{"_index":2193,"t":{"1347":{"position":[[1221,29]]},"1428":{"position":[[1222,29]]}}}],["left(\\theta_i\\right)rope(xi​)=xi​⋅cos(θi​)+xi+1​⋅sin(θi",{"_index":2194,"t":{"1347":{"position":[[1262,58]]},"1428":{"position":[[1263,58]]}}}],["legend=[\"curve_name_1",{"_index":1237,"t":{"1116":{"position":[[110,23]]}}}],["legend=['train",{"_index":1803,"t":{"1270":{"position":[[1736,14]]}}}],["len",{"_index":2820,"t":{"1527":{"position":[[829,3],[950,3]]}}}],["len(train_it",{"_index":1807,"t":{"1270":{"position":[[1818,15]]}}}],["lenet",{"_index":1731,"t":{"1268":{"position":[[91,5]]},"1270":{"position":[[2745,5]]}}}],["lenet5",{"_index":1840,"t":{"1270":{"position":[[2753,8]]}}}],["lenet5(nn.modul",{"_index":1746,"t":{"1270":{"position":[[424,18]]}}}],["lenetreshap",{"_index":1750,"t":{"1270":{"position":[[526,15]]}}}],["lenetreshape(nn.modul",{"_index":1741,"t":{"1270":{"position":[[282,24]]}}}],["lenet是由yann",{"_index":1729,"t":{"1268":{"position":[[0,11]]}}}],["length",{"_index":1270,"t":{"1122":{"position":[[312,6]]}}}],["leq",{"_index":1575,"t":{"1214":{"position":[[101,4]]}}}],["less",{"_index":1500,"t":{"1208":{"position":[[111,4]]}}}],["leverag",{"_index":2403,"t":{"1410":{"position":[[1549,9]]},"1483":{"position":[[1549,9]]}}}],["lg=−ez∼pz(z)[log⁡d(g(z))](4)l_g",{"_index":393,"t":{"925":{"position":[[800,32]]}}}],["lgpl",{"_index":170,"t":{"840":{"position":[[1558,4]]},"846":{"position":[[502,4]]}}}],["li",{"_index":509,"t":{"950":{"position":[[119,5]]},"1354":{"position":[[522,3]]}}}],["life",{"_index":520,"t":{"950":{"position":[[240,5]]}}}],["lighter",{"_index":2742,"t":{"1475":{"position":[[284,9],[318,8]]}}}],["lightest",{"_index":2744,"t":{"1475":{"position":[[357,9],[397,9]]}}}],["limit",{"_index":2009,"t":{"1324":{"position":[[534,7]]}}}],["line",{"_index":166,"t":{"840":{"position":[[1479,5]]},"846":{"position":[[423,5]]}}}],["linear",{"_index":2176,"t":{"1347":{"position":[[537,6]]},"1406":{"position":[[999,6]]},"1428":{"position":[[537,6]]},"1475":{"position":[[235,6]]},"1479":{"position":[[999,6]]}}}],["linguist",{"_index":1143,"t":{"1083":{"position":[[597,10]]}}}],["linker",{"_index":140,"t":{"840":{"position":[[991,7]]}}}],["linux",{"_index":134,"t":{"840":{"position":[[881,5]]}}}],["linux操作系统后需要安装bochs以及nasm",{"_index":90,"t":{"832":{"position":[[39,30]]}}}],["linux自带的编译环境以及编译命令对特定的geeko",{"_index":91,"t":{"834":{"position":[[47,30]]}}}],["list",{"_index":924,"t":{"1057":{"position":[[372,6]]},"1071":{"position":[[799,5],[849,5],[884,5]]},"1183":{"position":[[372,6]]},"1197":{"position":[[799,5],[849,5],[884,5]]},"1270":{"position":[[1128,6]]},"1275":{"position":[[526,16]]},"1592":{"position":[[316,4]]}}}],["list.end",{"_index":46,"t":{"816":{"position":[[58,12],[138,13]]}}}],["list1",{"_index":2503,"t":{"1445":{"position":[[92,5]]}}}],["list2",{"_index":2504,"t":{"1445":{"position":[[110,5],[202,6]]}}}],["list3",{"_index":2505,"t":{"1445":{"position":[[134,5],[209,6]]}}}],["list和tensor",{"_index":1859,"t":{"1275":{"position":[[409,17]]}}}],["list的*乘法是复制元素，改变list的shap",{"_index":1860,"t":{"1275":{"position":[[427,26]]}}}],["llama",{"_index":2109,"t":{"1340":{"position":[[246,6]]},"1342":{"position":[[602,5]]},"1347":{"position":[[34,5]]},"1421":{"position":[[246,6]]},"1423":{"position":[[602,5]]},"1428":{"position":[[34,5]]}}}],["llamagen",{"_index":2102,"t":{"1340":{"position":[[13,9]]},"1347":{"position":[[971,13],[1916,8]]},"1421":{"position":[[13,9]]},"1428":{"position":[[977,8],[1917,8]]}}}],["lll",{"_index":584,"t":{"973":{"position":[[243,17]]},"1144":{"position":[[187,13]]},"1146":{"position":[[16,12]]}}}],["llm",{"_index":2143,"t":{"1340":{"position":[[1150,3]]},"1342":{"position":[[211,4],[288,4],[500,4],[739,3]]},"1347":{"position":[[112,4],[1444,4]]},"1406":{"position":[[988,5],[1274,5]]},"1408":{"position":[[84,4]]},"1413":{"position":[[1305,4]]},"1417":{"position":[[518,4]]},"1421":{"position":[[1150,3]]},"1423":{"position":[[211,4],[288,4],[500,4],[738,3]]},"1428":{"position":[[112,4],[1445,4]]},"1454":{"position":[[3958,4],[6982,4]]},"1479":{"position":[[988,5],[1274,5]]},"1481":{"position":[[84,4]]},"1486":{"position":[[1305,4]]},"1490":{"position":[[265,4],[281,4]]},"1492":{"position":[[518,4]]}}}],["ll×l",{"_index":1428,"t":{"1148":{"position":[[366,4]]}}}],["log",{"_index":181,"t":{"840":{"position":[[1757,4]]},"846":{"position":[[701,4]]}}}],["log2(1pi)\\begin{align",{"_index":1506,"t":{"1208":{"position":[[286,22]]}}}],["log2(pi)=∑inpi",{"_index":1505,"t":{"1208":{"position":[[271,14]]}}}],["log2(pi)−log2(qi)](9)d_{kl}(p",{"_index":1527,"t":{"1208":{"position":[[839,30]]}}}],["log2(qi)\\begin{align",{"_index":1539,"t":{"1208":{"position":[[1184,21]]}}}],["log2​(pi​)=i∑n​pi",{"_index":1512,"t":{"1208":{"position":[[434,18]]}}}],["log2​(pi​)−log2​(qi​)](9",{"_index":1531,"t":{"1208":{"position":[[956,26]]}}}],["log2​(pi​1​)​(7",{"_index":1513,"t":{"1208":{"position":[[453,17]]}}}],["log2​(qi​)​(10",{"_index":1545,"t":{"1208":{"position":[[1350,16]]}}}],["log_2(p_i",{"_index":1529,"t":{"1208":{"position":[[907,11]]}}}],["log_2(q_i)]}\\tag{9}dkl​(p",{"_index":1530,"t":{"1208":{"position":[[919,25]]}}}],["logist",{"_index":1673,"t":{"1245":{"position":[[0,8]]}}}],["logit",{"_index":2208,"t":{"1347":{"position":[[2074,5]]},"1428":{"position":[[2075,5]]}}}],["logit提供的通用信息，同时保留条件logit",{"_index":2220,"t":{"1347":{"position":[[2327,80]]},"1428":{"position":[[2328,80]]}}}],["logit，即不考虑任何条件信息时生成的logit",{"_index":2217,"t":{"1347":{"position":[[2233,31]]},"1428":{"position":[[2234,31]]}}}],["logit，即基于输入文本提示的信息生成的logit",{"_index":2215,"t":{"1347":{"position":[[2189,31]]},"1428":{"position":[[2190,31]]}}}],["long",{"_index":2807,"t":{"1521":{"position":[[4,17],[22,32]]}}}],["loss",{"_index":934,"t":{"1057":{"position":[[1024,4]]},"1069":{"position":[[790,4],[886,4]]},"1071":{"position":[[24,8],[40,4]]},"1087":{"position":[[139,10]]},"1114":{"position":[[280,8]]},"1183":{"position":[[1024,4]]},"1195":{"position":[[790,4],[886,4]]},"1197":{"position":[[24,8],[40,4]]},"1270":{"position":[[1623,4],[1751,6]]},"1279":{"position":[[504,5],[532,4]]},"1285":{"position":[[127,4],[893,4]]},"1359":{"position":[[890,39]]},"1364":{"position":[[504,5],[532,4]]},"1370":{"position":[[127,4],[893,4]]},"1454":{"position":[[2547,4],[2719,4],[7729,5],[7831,5]]},"1549":{"position":[[129,4]]},"1572":{"position":[[6,43],[87,14]]}}}],["loss(y_hat",{"_index":1817,"t":{"1270":{"position":[[2039,11]]}}}],["loss.to(devic",{"_index":1799,"t":{"1270":{"position":[[1658,15]]}}}],["lossdiscriminator=lossreal+lossfake(3)loss_{discriminator}=loss_{real}+loss_{fake}\\tag{3}lossdiscriminator​=lossreal​+lossfake​(3",{"_index":392,"t":{"925":{"position":[[532,130]]}}}],["lossfake=−ez∼pz(z)[log(1−d(g(z)))](2)loss_{fak",{"_index":387,"t":{"925":{"position":[[332,49]]}}}],["lossmse=∑[y−f(x)]2(5)loss_{ms",{"_index":1486,"t":{"1206":{"position":[[57,31]]}}}],["lossreal=−ex∼pdata(x)[log⁡d(x)](1)loss_{r",{"_index":382,"t":{"925":{"position":[[160,46]]}}}],["loss得到6",{"_index":2856,"t":{"1549":{"position":[[74,11]]}}}],["loss来训练目标域生成器，使源于生成器向目标域迁移学习。需要输入源域以及目标域图像、源域以及目标域的prompt描述。源域图像的lat",{"_index":2308,"t":{"1359":{"position":[[644,71]]}}}],["loss的输入来约束从源域中学习到的prompt",{"_index":2303,"t":{"1359":{"position":[[346,33]]}}}],["loss的输入，约束学习到的prompt",{"_index":2307,"t":{"1359":{"position":[[555,31]]}}}],["lot",{"_index":196,"t":{"840":{"position":[[1992,3]]},"846":{"position":[[936,3]]}}}],["low",{"_index":591,"t":{"979":{"position":[[54,3]]}}}],["lr",{"_index":348,"t":{"888":{"position":[[29,9]]},"1270":{"position":[[1414,3],[2720,3],[2810,3]]}}}],["lr(0",{"_index":352,"t":{"893":{"position":[[98,8]]},"898":{"position":[[139,6],[160,13]]}}}],["lr(1",{"_index":358,"t":{"893":{"position":[[435,8]]},"898":{"position":[[154,5]]}}}],["lr=lr",{"_index":1797,"t":{"1270":{"position":[[1616,6]]}}}],["lr_mul=0.01",{"_index":1069,"t":{"1067":{"position":[[843,12],[986,12]]},"1193":{"position":[[843,12],[986,12]]}}}],["lr_mul=lr_mlp",{"_index":906,"t":{"1055":{"position":[[684,14]]},"1181":{"position":[[684,14]]}}}],["lu",{"_index":2263,"t":{"1354":{"position":[[534,3]]},"1536":{"position":[[678,57]]},"1539":{"position":[[818,57]]}}}],["l×dkl",{"_index":1424,"t":{"1148":{"position":[[246,5]]}}}],["l×dl",{"_index":1415,"t":{"1146":{"position":[[54,4]]}}}],["l×ll",{"_index":1427,"t":{"1148":{"position":[[354,4]]}}}],["m",{"_index":148,"t":{"840":{"position":[[1086,1]]},"1112":{"position":[[52,1]]}}}],["m32",{"_index":126,"t":{"840":{"position":[[729,3],[978,3]]}}}],["machin",{"_index":1611,"t":{"1226":{"position":[[47,7]]}}}],["mae",{"_index":1978,"t":{"1290":{"position":[[15,3]]},"1375":{"position":[[15,3]]}}}],["mage",{"_index":2462,"t":{"1433":{"position":[[1985,10]]}}}],["main",{"_index":8,"t":{"803":{"position":[[50,6]]},"814":{"position":[[50,6]]},"824":{"position":[[50,6]]},"844":{"position":[[503,31]]},"1527":{"position":[[770,6]]}}}],["main(struct",{"_index":277,"t":{"844":{"position":[[1838,11]]}}}],["make",{"_index":86,"t":{"830":{"position":[[338,7]]},"834":{"position":[[144,7]]},"840":{"position":[[147,19]]},"846":{"position":[[70,4],[82,4]]},"1138":{"position":[[2783,6]]},"1406":{"position":[[428,5]]},"1457":{"position":[[125,4]]},"1469":{"position":[[84,4]]},"1479":{"position":[[428,5]]}}}],["man",{"_index":2283,"t":{"1354":{"position":[[758,3]]}}}],["manifold",{"_index":2558,"t":{"1454":{"position":[[948,8]]}}}],["manipul",{"_index":2538,"t":{"1454":{"position":[[246,12]]}}}],["manner",{"_index":2033,"t":{"1324":{"position":[[1168,7]]}}}],["map",{"_index":898,"t":{"1055":{"position":[[470,12],[566,10]]},"1059":{"position":[[1940,3]]},"1156":{"position":[[83,3]]},"1162":{"position":[[43,21]]},"1181":{"position":[[470,12],[566,10]]},"1185":{"position":[[1940,3]]},"1410":{"position":[[111,4],[1270,3]]},"1413":{"position":[[213,3],[265,4],[289,4],[386,14]]},"1415":{"position":[[104,3],[136,3],[171,6],[186,3]]},"1417":{"position":[[236,14]]},"1483":{"position":[[111,4],[1270,3]]},"1486":{"position":[[213,3],[265,4],[289,4],[386,14]]},"1488":{"position":[[104,3],[136,3],[171,6],[186,3]]},"1492":{"position":[[236,14]]}}}],["mapper",{"_index":881,"t":{"1053":{"position":[[2,6]]},"1055":{"position":[[994,13],[1114,15]]},"1059":{"position":[[1106,6],[1646,6]]},"1067":{"position":[[0,6],[61,6]]},"1069":{"position":[[257,20],[341,6],[348,12],[492,16],[935,6],[1110,6]]},"1071":{"position":[[121,6],[187,6],[245,6],[330,6],[378,6],[1063,6]]},"1179":{"position":[[2,6]]},"1181":{"position":[[994,13],[1114,15]]},"1185":{"position":[[1106,6],[1646,6]]},"1193":{"position":[[0,6],[61,6]]},"1195":{"position":[[257,20],[341,6],[348,12],[492,16],[935,6],[1110,6]]},"1197":{"position":[[121,6],[187,6],[245,6],[330,6],[378,6],[1063,6]]},"1359":{"position":[[26,7],[600,20]]}}}],["mapper接收源域图像的lat",{"_index":2297,"t":{"1359":{"position":[[91,19]]}}}],["mapper接收源域图像的隐式表示后输出再分别与源域和目标域标签concat而得到。分别将源域图像、生成的目标域图像以及源域、目标域的图片prompt描述一起输入至direct",{"_index":2310,"t":{"1359":{"position":[[791,93]]}}}],["mapper来为每一个训练集的源域图片生成一组prompt。lat",{"_index":2296,"t":{"1359":{"position":[[54,36]]}}}],["mapper来从源域图像中学习出包含图像特征且适应目标域的prompt",{"_index":2231,"t":{"1351":{"position":[[78,50]]}}}],["mapper输出的prompt与源域标签concat后送入来自clip的text",{"_index":2304,"t":{"1359":{"position":[[392,40]]}}}],["mapper输出的prompt与目标域标签concat后送入来自clip的text",{"_index":2300,"t":{"1359":{"position":[[235,41]]}}}],["mapper，增加多头注意力，减小transform",{"_index":1043,"t":{"1067":{"position":[[129,28]]},"1193":{"position":[[129,28]]}}}],["mapping)。具体来说，w",{"_index":930,"t":{"1057":{"position":[[787,15]]},"1183":{"position":[[787,15]]}}}],["mapping.json",{"_index":1282,"t":{"1122":{"position":[[522,12]]}}}],["map与ground",{"_index":2854,"t":{"1549":{"position":[[53,10],[108,10]]}}}],["map进行concat拼接，得到6通道的融合featur",{"_index":2850,"t":{"1547":{"position":[[300,29]]}}}],["map通道数也为1。将每一层的featur",{"_index":2849,"t":{"1547":{"position":[[277,22]]}}}],["map，quant",{"_index":2389,"t":{"1410":{"position":[[158,13]]},"1483":{"position":[[158,13]]}}}],["map，最后使用1x1卷积以及sigmoid",{"_index":2851,"t":{"1547":{"position":[[330,38]]}}}],["mar",{"_index":1917,"t":{"1279":{"position":[[787,5]]},"1297":{"position":[[3,3]]},"1364":{"position":[[787,5]]},"1382":{"position":[[3,3]]}}}],["mardown",{"_index":2706,"t":{"1467":{"position":[[32,7]]}}}],["markdown",{"_index":2709,"t":{"1467":{"position":[[126,10]]},"1471":{"position":[[43,8]]}}}],["marr",{"_index":649,"t":{"999":{"position":[[224,36]]}}}],["mask",{"_index":452,"t":{"940":{"position":[[766,17]]},"948":{"position":[[1565,4]]},"1081":{"position":[[0,7]]},"1279":{"position":[[765,6]]},"1312":{"position":[[766,17]]},"1320":{"position":[[1565,4]]},"1342":{"position":[[165,6]]},"1364":{"position":[[765,6]]},"1410":{"position":[[1435,4],[1603,5]]},"1423":{"position":[[165,6]]},"1433":{"position":[[1124,6],[1185,6],[1368,6],[1391,6],[1414,6],[1611,6],[1996,6]]},"1454":{"position":[[1218,4]]},"1483":{"position":[[1435,4],[1603,5]]}}}],["maskgit",{"_index":1979,"t":{"1290":{"position":[[21,7]]},"1342":{"position":[[152,7]]},"1375":{"position":[[21,7]]},"1423":{"position":[[152,7]]},"1433":{"position":[[1171,13],[1515,7],[1587,10],[1807,7],[1860,24],[1885,69]]}}}],["mask置为负无穷是因为这是在经过softmax之前进行的掩码，在经过softmax之后负无穷小就变成了0",{"_index":501,"t":{"948":{"position":[[1660,60]]},"1320":{"position":[[1660,60]]}}}],["math",{"_index":2712,"t":{"1469":{"position":[[32,4],[108,4],[263,4]]}}}],["mathbb{r}^dx∈rd",{"_index":1932,"t":{"1285":{"position":[[29,26]]},"1370":{"position":[[29,26]]}}}],["mathbb{r}^dz∈rd",{"_index":1934,"t":{"1285":{"position":[[91,18]]},"1370":{"position":[[91,18]]}}}],["mathbb{r}^dε∈rd",{"_index":1945,"t":{"1285":{"position":[[384,16]]},"1370":{"position":[[384,16]]}}}],["mathbb{r}^{h",{"_index":2413,"t":{"1413":{"position":[[427,13]]},"1486":{"position":[[427,13]]}}}],["mathbb{r}^{t",{"_index":2567,"t":{"1454":{"position":[[1708,13]]}}}],["mathbf{",{"_index":2571,"t":{"1454":{"position":[[1884,11]]}}}],["mathbf{c",{"_index":2655,"t":{"1454":{"position":[[6464,11],[6562,11],[8230,11],[8337,11]]}}}],["mathbf{c})xgen​=x^θ​(ϵ,c",{"_index":2648,"t":{"1454":{"position":[[6272,52]]}}}],["mathbf{e}\\right)\\right\\|_2^2\\right]l(x,e,θ)=et,ϵ​[∥ϵ−fθ​(xt​,t,e)∥22",{"_index":2575,"t":{"1454":{"position":[[1994,71]]}}}],["mathbf{i})n(0,i",{"_index":1947,"t":{"1285":{"position":[[425,17]]},"1370":{"position":[[425,17]]}}}],["mathbf{i})zt1​​∼n(0,i",{"_index":2682,"t":{"1454":{"position":[[8023,23]]}}}],["mathbf{i})ϵ∼n(0,i",{"_index":2643,"t":{"1454":{"position":[[6124,19]]}}}],["mathbf{x",{"_index":2651,"t":{"1454":{"position":[[6352,10],[6530,10],[6576,10],[8305,10],[8351,10]]}}}],["mathcal{e}\\left(c_i\\right",{"_index":2615,"t":{"1454":{"position":[[4798,28],[4909,28]]}}}],["mathcal{n}(0",{"_index":1972,"t":{"1287":{"position":[[395,14]]},"1372":{"position":[[395,14]]},"1454":{"position":[[6109,14],[8008,14]]}}}],["mathcal{n}(\\mu_1,\\sigma_1^2)p∼n(μ1​,σ12​)和q∼n(μ2,σ22)q\\sim",{"_index":2086,"t":{"1332":{"position":[[639,59]]}}}],["mathcal{n}(\\mu_2,\\sigma_2^2)q∼n(μ2​,σ22​)而言，它们的kl",{"_index":2087,"t":{"1332":{"position":[[699,56]]}}}],["mathrm{t}t∈t",{"_index":2608,"t":{"1454":{"position":[[4472,27]]}}}],["matlab",{"_index":502,"t":{"950":{"position":[[4,49]]}}}],["matplotlib",{"_index":1733,"t":{"1270":{"position":[[130,10]]}}}],["matplotlib.pyplot",{"_index":1403,"t":{"1138":{"position":[[3305,17]]}}}],["max(maxx",{"_index":41,"t":{"814":{"position":[[253,9]]}}}],["maxpool",{"_index":1606,"t":{"1221":{"position":[[24,14]]}}}],["maxx",{"_index":35,"t":{"814":{"position":[[83,4],[246,4],[344,4]]}}}],["mco",{"_index":2791,"t":{"1514":{"position":[[186,4]]}}}],["md",{"_index":2707,"t":{"1467":{"position":[[69,3]]}}}],["mdx",{"_index":2705,"t":{"1467":{"position":[[14,3],[59,3],[79,4]]},"1469":{"position":[[71,3],[159,3]]}}}],["mean",{"_index":589,"t":{"979":{"position":[[42,5]]},"1208":{"position":[[105,5]]},"1558":{"position":[[126,23]]}}}],["measur",{"_index":1496,"t":{"1208":{"position":[[69,7]]}}}],["mechan",{"_index":1305,"t":{"1134":{"position":[[136,9]]},"1144":{"position":[[22,70]]},"1324":{"position":[[245,9]]}}}],["meg",{"_index":174,"t":{"840":{"position":[[1660,5]]},"846":{"position":[[604,5]]}}}],["mel",{"_index":1259,"t":{"1122":{"position":[[122,3],[480,3]]}}}],["mention",{"_index":1377,"t":{"1138":{"position":[[2610,9]]}}}],["menu",{"_index":2785,"t":{"1514":{"position":[[36,4],[143,4],[299,4]]}}}],["messag",{"_index":194,"t":{"840":{"position":[[1951,8]]},"846":{"position":[[895,8]]}}}],["metadata",{"_index":1268,"t":{"1122":{"position":[[285,9]]}}}],["metadata.json",{"_index":1283,"t":{"1122":{"position":[[539,13]]}}}],["metadata_path",{"_index":1267,"t":{"1122":{"position":[[254,14]]}}}],["method",{"_index":2549,"t":{"1454":{"position":[[632,7],[3332,6]]}}}],["methodolog",{"_index":2357,"t":{"1406":{"position":[[291,11]]},"1479":{"position":[[291,11]]}}}],["metric",{"_index":1773,"t":{"1270":{"position":[[1060,6],[1834,6]]}}}],["metric.add(d2l.accuracy(net(x",{"_index":1778,"t":{"1270":{"position":[[1205,31]]}}}],["metric.add(l",{"_index":1820,"t":{"1270":{"position":[[2084,12]]}}}],["metric[0",{"_index":1780,"t":{"1270":{"position":[[1275,9],[2165,9]]}}}],["metric[1",{"_index":1781,"t":{"1270":{"position":[[1287,9],[2199,9]]}}}],["metric[2",{"_index":1824,"t":{"1270":{"position":[[2177,9],[2211,9]]}}}],["mid",{"_index":1191,"t":{"1101":{"position":[[404,4]]},"1103":{"position":[[569,4]]},"1285":{"position":[[162,4],[295,4],[718,4]]},"1287":{"position":[[21,4],[203,4],[470,4]]},"1332":{"position":[[368,4],[443,4],[463,4],[483,4],[501,4]]},"1370":{"position":[[162,4],[295,4],[718,4]]},"1372":{"position":[[21,4],[203,4],[470,4]]},"1386":{"position":[[238,4],[476,4],[620,4]]},"1413":{"position":[[717,4]]},"1486":{"position":[[717,4]]}}}],["million",{"_index":2162,"t":{"1342":{"position":[[665,7],[688,7]]},"1423":{"position":[[665,7],[688,7]]}}}],["min",{"_index":1567,"t":{"1214":{"position":[[0,3]]},"1454":{"position":[[8711,10],[8765,4]]}}}],["minsu",{"_index":2252,"t":{"1354":{"position":[[378,5]]}}}],["mit",{"_index":1034,"t":{"1064":{"position":[[3,3]]},"1190":{"position":[[3,3]]}}}],["mixing_noise(args.batch_mapp",{"_index":888,"t":{"1055":{"position":[[33,31]]},"1181":{"position":[[33,31]]}}}],["ml2021spring",{"_index":1287,"t":{"1124":{"position":[[40,12]]}}}],["mlp",{"_index":1955,"t":{"1285":{"position":[[673,3],[785,8]]},"1370":{"position":[[673,3],[785,8]]}}}],["mnli（multinli",{"_index":1128,"t":{"1083":{"position":[[254,53]]}}}],["mo",{"_index":2251,"t":{"1354":{"position":[[374,3]]}}}],["modal",{"_index":2530,"t":{"1452":{"position":[[234,5]]}}}],["mode",{"_index":2287,"t":{"1354":{"position":[[1018,69]]}}}],["model",{"_index":653,"t":{"1003":{"position":[[58,28]]},"1005":{"position":[[823,7]]},"1014":{"position":[[107,97]]},"1022":{"position":[[690,5],[861,6]]},"1026":{"position":[[11,6]]},"1038":{"position":[[52,5]]},"1057":{"position":[[634,5]]},"1059":{"position":[[43,6]]},"1164":{"position":[[52,5]]},"1183":{"position":[[634,5]]},"1185":{"position":[[43,6]]},"1226":{"position":[[97,6],[233,5]]},"1230":{"position":[[24,5]]},"1279":{"position":[[46,6],[271,9],[309,5],[423,6],[549,5],[742,6],[927,9]]},"1322":{"position":[[147,6]]},"1324":{"position":[[110,6],[336,6],[733,6],[956,5],[994,6],[1197,6]]},"1327":{"position":[[103,5]]},"1340":{"position":[[56,6],[134,6],[232,7],[466,7],[740,6],[872,6],[937,5],[1227,6],[1282,6],[1382,7]]},"1342":{"position":[[183,5],[260,14],[434,23]]},"1347":{"position":[[19,5]]},"1349":{"position":[[95,5]]},"1354":{"position":[[11,5]]},"1364":{"position":[[46,6],[271,9],[309,5],[423,6],[549,5],[742,6],[927,9]]},"1384":{"position":[[21,137]]},"1406":{"position":[[33,8],[447,6],[914,6],[1344,6],[1398,6]]},"1410":{"position":[[1511,6]]},"1421":{"position":[[56,6],[134,6],[232,7],[466,7],[740,6],[872,6],[937,5],[1227,6],[1282,6],[1382,7]]},"1423":{"position":[[183,5],[260,14],[434,23]]},"1428":{"position":[[19,5]]},"1431":{"position":[[61,6]]},"1433":{"position":[[2086,6]]},"1435":{"position":[[83,6]]},"1449":{"position":[[46,7],[138,6],[166,5]]},"1452":{"position":[[42,5],[161,6]]},"1454":{"position":[[1497,7],[2359,9],[2601,5],[2810,9],[3068,7],[3146,12],[3241,5],[3424,5],[3783,5],[3879,5],[4003,16],[4046,6],[5851,6],[6002,6],[6047,5],[6825,5],[6858,5],[7873,5],[8626,6]]},"1457":{"position":[[30,7],[106,7],[273,6]]},"1459":{"position":[[72,6]]},"1479":{"position":[[33,8],[447,6],[914,6],[1344,6],[1398,6]]},"1483":{"position":[[1511,6]]},"1490":{"position":[[221,5]]}}}],["model'",{"_index":1632,"t":{"1228":{"position":[[346,7]]}}}],["models）模型为例，模型在denoise时为每个denoise步骤赋予一个编号，越早进行denoise的步骤编号越大，因此，这个编号也代表着图像中噪声的严重程度。在denoise模块中，模型根据输入的带有噪声的图片、文字prompt以及噪声的严重程度（即denoise的步骤）预测出该图片中噪声的分布，然后将输入的图片中减去预测出的噪声得到denois",{"_index":810,"t":{"1036":{"position":[[177,184]]}}}],["model以及gan",{"_index":799,"t":{"1028":{"position":[[35,23]]}}}],["model其实就是训练一个nois",{"_index":775,"t":{"1026":{"position":[[226,18]]}}}],["model或gan",{"_index":2237,"t":{"1351":{"position":[[351,11]]}}}],["model时的训练资料可以通过对数据集中的原始图片添加与图像大小一致地从已知随机分布中sample出的噪声来获得。此时加入噪声后的图像可以作为压缩图像输入至nois",{"_index":777,"t":{"1026":{"position":[[323,83]]}}}],["model概率扩散模型理论与完整pytorch",{"_index":2044,"t":{"1327":{"position":[[57,29]]}}}],["model生成的压缩的图片或lat",{"_index":789,"t":{"1026":{"position":[[846,20]]}}}],["model的中间产物是压缩图像时，如diffusion模型，在训练gener",{"_index":776,"t":{"1026":{"position":[[279,43]]}}}],["model的中间产物，st",{"_index":728,"t":{"1022":{"position":[[382,17]]}}}],["model的具体数学推导，可以参考胡老师推荐的论文understand",{"_index":674,"t":{"1005":{"position":[[774,38]]}}}],["model的最终输出是中间产物，这个中间产物可以是图像的压缩版本，也可以是一个lat",{"_index":773,"t":{"1026":{"position":[[149,45]]}}}],["model的模型大小，text",{"_index":732,"t":{"1022":{"position":[[482,15]]}}}],["model的生成过程其实就是denoise的过程。具体来讲，输入文字prompt以及从随机分布中sample出的与预期生成图像具有相同大小的噪声矩阵，预测出输入图片中的噪声分布，在输入图像中减去噪声，输出去噪后的图像。gener",{"_index":772,"t":{"1026":{"position":[[29,119]]}}}],["model的训练需要大量成对的（pair",{"_index":787,"t":{"1026":{"position":[[782,23]]}}}],["model的输出是压缩版本的图像时，decoder的训练资料可以将从互联网上fetch到的图像作为label，并对这些图像做down",{"_index":791,"t":{"1026":{"position":[[946,66]]}}}],["model等生成模型，都不只是单独使用文字作为输入来生成图像，而是使用了从已知的随机分布（e.g",{"_index":713,"t":{"1018":{"position":[[19,49]]}}}],["model非常相似：vae对训练集中的原始图像使用encoder将其变换为某种lat",{"_index":670,"t":{"1005":{"position":[[606,45]]}}}],["model（mlm）任务的交叉熵损失，通过最小化encoder输出的概率分布与ground",{"_index":1116,"t":{"1081":{"position":[[202,45]]}}}],["model（non",{"_index":2459,"t":{"1433":{"position":[[1142,9]]}}}],["model：接受text",{"_index":722,"t":{"1022":{"position":[[125,12]]}}}],["modifi",{"_index":255,"t":{"844":{"position":[[956,6]]}}}],["modul",{"_index":1301,"t":{"1134":{"position":[[17,6],[90,7]]}}}],["monitor",{"_index":2790,"t":{"1514":{"position":[[171,7]]}}}],["more",{"_index":737,"t":{"1022":{"position":[[618,4],[786,4]]},"1134":{"position":[[234,4]]},"1228":{"position":[[316,4]]},"1351":{"position":[[145,4]]}}}],["motherfuck",{"_index":269,"t":{"844":{"position":[[1613,13]]}}}],["motiv",{"_index":1926,"t":{"1279":{"position":[[960,8]]},"1364":{"position":[[960,8]]}}}],["mount",{"_index":272,"t":{"844":{"position":[[1783,6]]}}}],["mous",{"_index":187,"t":{"840":{"position":[[1833,6]]},"846":{"position":[[777,6]]}}}],["move(t,a)move(t",{"_index":337,"t":{"867":{"position":[[292,18]]}}}],["mrpc（microsoft",{"_index":1145,"t":{"1083":{"position":[[653,14]]}}}],["mse",{"_index":540,"t":{"957":{"position":[[0,29]]},"1247":{"position":[[72,50]]}}}],["mse=1mn∑x=1m∑y=1n[f(x,y)−g(x,y)]2(4)mse=\\frac{1}{mn}\\sum_{x=1}^{m}\\sum_{y=1}^{n}[f(x,i",{"_index":541,"t":{"957":{"position":[[30,87]]}}}],["mse作为损失函数，其目标是让输出的概率更接近于1",{"_index":1676,"t":{"1249":{"position":[[0,56]]}}}],["mu(x)}{\\sigma(x",{"_index":2201,"t":{"1347":{"position":[[1643,18]]},"1428":{"position":[[1644,18]]}}}],["mu_2)^2}{2\\sigma_2^2",{"_index":2089,"t":{"1332":{"position":[[859,22]]}}}],["mu_2\\right\\|_2^2+\\mathrm{tr}\\left(\\sigma_1+\\sigma_2",{"_index":760,"t":{"1024":{"position":[[440,52]]}}}],["multi",{"_index":453,"t":{"940":{"position":[[784,5],[907,5],[962,5]]},"1144":{"position":[[105,16]]},"1150":{"position":[[0,23]]},"1312":{"position":[[784,5],[907,5],[962,5]]},"1413":{"position":[[1229,5]]},"1415":{"position":[[0,5]]},"1452":{"position":[[228,5]]},"1486":{"position":[[1229,5]]},"1488":{"position":[[0,5]]}}}],["multihead",{"_index":482,"t":{"948":{"position":[[0,14]]},"1320":{"position":[[0,14]]}}}],["multimod",{"_index":2152,"t":{"1340":{"position":[[1360,10]]},"1421":{"position":[[1360,10]]}}}],["multipl",{"_index":2368,"t":{"1406":{"position":[[803,8]]},"1479":{"position":[[803,8]]}}}],["mutil",{"_index":464,"t":{"940":{"position":[[1357,5]]},"1312":{"position":[[1357,5]]}}}],["mux=σ∗z",{"_index":2100,"t":{"1336":{"position":[[128,10]]}}}],["muμ，背景像素的正态分布概率密度函数的均值为ν\\nu",{"_index":641,"t":{"995":{"position":[[7,102]]}}}],["mvtm",{"_index":2461,"t":{"1433":{"position":[[1352,7]]}}}],["n",{"_index":224,"t":{"844":{"position":[[409,5],[474,4],[1635,5],[1689,4]]},"1041":{"position":[[14,1]]},"1138":{"position":[[2325,4],[4484,4]]},"1167":{"position":[[14,1]]},"1517":{"position":[[9,1],[18,1]]},"1527":{"position":[[19,13]]},"1536":{"position":[[165,12],[178,26],[205,27]]},"1539":{"position":[[305,12],[318,26],[345,27]]}}}],["n\"代替endl",{"_index":74,"t":{"826":{"position":[[304,41]]}}}],["n(0,1",{"_index":2618,"t":{"1454":{"position":[[4849,7]]}}}],["n(0,i)\\epsilon",{"_index":2642,"t":{"1454":{"position":[[6087,16]]}}}],["n(0,i)\\mathcal{n}(0",{"_index":1946,"t":{"1285":{"position":[[404,20]]},"1287":{"position":[[310,20]]},"1370":{"position":[[404,20]]},"1372":{"position":[[310,20]]}}}],["n(μ,σ2)\\mathcal{n}(\\mu,\\sigma^2)n(μ,σ2)采样xxx时，可以先从标准高斯分布n(0,1)\\mathcal{n}(0,1)n(0,1)中sample出zzz",{"_index":2098,"t":{"1336":{"position":[[0,105]]}}}],["n2",{"_index":2804,"t":{"1517":{"position":[[88,2]]}}}],["n2n^2n2",{"_index":2443,"t":{"1417":{"position":[[284,7]]},"1492":{"position":[[284,7]]}}}],["n_dim",{"_index":1049,"t":{"1067":{"position":[[311,7],[393,5],[966,5]]},"1138":{"position":[[87,5],[117,18]]},"1144":{"position":[[284,5],[314,18]]},"1193":{"position":[[311,7],[393,5],[966,5]]}}}],["n_latent",{"_index":928,"t":{"1057":{"position":[[672,8],[817,8]]},"1183":{"position":[[672,8],[817,8]]}}}],["nada",{"_index":944,"t":{"1059":{"position":[[5,4],[38,4],[1045,4],[1585,4],[2116,4],[2472,4]]},"1185":{"position":[[5,4],[38,4],[1045,4],[1585,4],[2116,4],[2472,4]]}}}],["nada率先引入了clip模型来获取必须的先验知识，通过预训练大模型的语言理解能力实现在目标域只需要文字标签而不需要图片，将源域和目标域之间的差距编码为在clip",{"_index":2285,"t":{"1354":{"position":[[831,112]]}}}],["namespac",{"_index":5,"t":{"803":{"position":[[31,9]]},"814":{"position":[[31,9]]},"824":{"position":[[31,9]]},"1527":{"position":[[72,9]]}}}],["nar",{"_index":711,"t":{"1016":{"position":[[0,63]]}}}],["nasm",{"_index":98,"t":{"838":{"position":[[27,6],[100,4]]}}}],["nat",{"_index":438,"t":{"938":{"position":[[91,9]]},"1310":{"position":[[91,9]]}}}],["natur",{"_index":1134,"t":{"1083":{"position":[[384,7]]},"1454":{"position":[[960,7]]}}}],["neamtiu",{"_index":247,"t":{"844":{"position":[[834,7]]}}}],["neamtiu@cs.umd.edu",{"_index":248,"t":{"844":{"position":[[842,20]]}}}],["near",{"_index":2017,"t":{"1324":{"position":[[802,4]]},"1406":{"position":[[1031,4]]},"1479":{"position":[[1031,4]]}}}],["necess",{"_index":1905,"t":{"1279":{"position":[[242,9]]},"1364":{"position":[[242,9]]}}}],["need",{"_index":164,"t":{"840":{"position":[[1460,4]]},"846":{"position":[[404,4]]},"1279":{"position":[[611,4]]},"1364":{"position":[[611,4]]}}}],["neq",{"_index":1521,"t":{"1208":{"position":[[754,4]]}}}],["net",{"_index":739,"t":{"1022":{"position":[[640,3],[698,3],[812,3]]},"1435":{"position":[[40,3]]},"1452":{"position":[[5,3]]},"1547":{"position":[[7,8]]}}}],["net(x",{"_index":1816,"t":{"1270":{"position":[[2028,6]]}}}],["net.apply(init_weight",{"_index":1793,"t":{"1270":{"position":[[1531,23]]}}}],["net.ev",{"_index":1771,"t":{"1270":{"position":[[989,10]]}}}],["net.generator_frozen.style(sample_z",{"_index":893,"t":{"1055":{"position":[[122,36]]},"1181":{"position":[[122,36]]}}}],["net.to(devic",{"_index":1794,"t":{"1270":{"position":[[1555,14]]}}}],["net.train",{"_index":1809,"t":{"1270":{"position":[[1862,11]]}}}],["network",{"_index":378,"t":{"921":{"position":[[38,48]]},"923":{"position":[[42,53]]},"1055":{"position":[[506,7],[577,10]]},"1059":{"position":[[1869,7]]},"1138":{"position":[[626,7]]},"1181":{"position":[[506,7],[577,10]]},"1185":{"position":[[1869,7]]},"1354":{"position":[[337,9]]},"1397":{"position":[[152,12],[208,26]]},"1433":{"position":[[806,8]]}}}],["network进行映射得到的，map",{"_index":899,"t":{"1055":{"position":[[483,22]]},"1181":{"position":[[483,22]]}}}],["network），fcn通过最后通过反卷积将tensor还原到原始图像尺寸，即在cnn中，输入与输出（下游任务的要求）都影响着cnn",{"_index":1293,"t":{"1129":{"position":[[212,72]]}}}],["network，生成对抗网络）包括两个主要的组件：生成器（generator）和判别器（discrimin",{"_index":375,"t":{"919":{"position":[[27,78]]}}}],["neural",{"_index":1326,"t":{"1138":{"position":[[619,6]]}}}],["neurip",{"_index":2695,"t":{"1457":{"position":[[0,7]]}}}],["new",{"_index":2035,"t":{"1324":{"position":[[1219,3]]},"1340":{"position":[[25,3]]},"1406":{"position":[[51,3]]},"1421":{"position":[[25,3]]},"1479":{"position":[[51,3]]}}}],["next",{"_index":1121,"t":{"1081":{"position":[[428,4]]},"1340":{"position":[[83,5]]},"1406":{"position":[[146,5],[173,5],[243,5]]},"1408":{"position":[[32,4]]},"1410":{"position":[[21,4],[892,4],[929,4],[985,4]]},"1413":{"position":[[155,4],[298,4],[326,4],[1205,4]]},"1417":{"position":[[57,4]]},"1421":{"position":[[83,5]]},"1433":{"position":[[1096,4]]},"1479":{"position":[[146,5],[173,5],[243,5]]},"1481":{"position":[[32,4]]},"1483":{"position":[[21,4],[892,4],[929,4],[985,4]]},"1486":{"position":[[155,4],[298,4],[326,4],[1205,4]]},"1490":{"position":[[181,4]]},"1492":{"position":[[57,4]]}}}],["next(iter(net.parameters())).devic",{"_index":1772,"t":{"1270":{"position":[[1024,35]]}}}],["next_permutation(list.begin",{"_index":48,"t":{"816":{"position":[[106,31]]}}}],["next_permutation(s.begin",{"_index":42,"t":{"814":{"position":[[279,28]]}}}],["nfa",{"_index":334,"t":{"867":{"position":[[194,14]]}}}],["nfa的确定化，即将nfa转换为dfa",{"_index":335,"t":{"867":{"position":[[209,24]]}}}],["nfa终态的集合要标*，代表其为等价dfa",{"_index":336,"t":{"867":{"position":[[251,40]]}}}],["ngai",{"_index":2282,"t":{"1354":{"position":[[753,4]]}}}],["ngoc",{"_index":2274,"t":{"1354":{"position":[[675,9],[713,4]]}}}],["nguyen",{"_index":2280,"t":{"1354":{"position":[[722,7],[741,7]]}}}],["nhead=4",{"_index":1058,"t":{"1067":{"position":[[541,8]]},"1193":{"position":[[541,8]]}}}],["night/128x128/apps(deppend",{"_index":2777,"t":{"1512":{"position":[[109,29]]}}}],["ninja",{"_index":835,"t":{"1043":{"position":[[152,5]]},"1169":{"position":[[152,5]]}}}],["nli",{"_index":1154,"t":{"1083":{"position":[[795,16]]}}}],["nllloss",{"_index":1551,"t":{"1208":{"position":[[1492,11],[1504,7]]}}}],["nllloss(predict",{"_index":1558,"t":{"1208":{"position":[[1655,16]]}}}],["nlp",{"_index":2154,"t":{"1342":{"position":[[9,3],[282,3]]},"1408":{"position":[[76,3]]},"1410":{"position":[[884,3]]},"1423":{"position":[[9,3],[282,3]]},"1481":{"position":[[76,3]]},"1483":{"position":[[884,3]]}}}],["nlp任务中很重要的一个benchmark：glu",{"_index":1123,"t":{"1083":{"position":[[47,38]]}}}],["nlp领域的，但是bert是seq2seq模型，图片、语音等信号也都可以作为sequence输入至bert中，因此bert",{"_index":1164,"t":{"1083":{"position":[[1394,88]]}}}],["nm",{"_index":651,"t":{"999":{"position":[[293,32]]},"1556":{"position":[[590,9]]}}}],["nn",{"_index":1550,"t":{"1208":{"position":[[1487,2]]},"1270":{"position":[[50,2],[56,2]]}}}],["nn.avgpool2d(kernel_size=2",{"_index":1755,"t":{"1270":{"position":[[617,27],[705,27]]}}}],["nn.conv2d",{"_index":1791,"t":{"1270":{"position":[[1486,10]]}}}],["nn.conv2d(1",{"_index":1751,"t":{"1270":{"position":[[558,12]]}}}],["nn.conv2d(6",{"_index":1757,"t":{"1270":{"position":[[656,12]]}}}],["nn.crossentropyloss",{"_index":1562,"t":{"1208":{"position":[[1740,21]]}}}],["nn.flatten",{"_index":1759,"t":{"1270":{"position":[[744,13]]}}}],["nn.init.xavier_uniform_(m.weight",{"_index":1792,"t":{"1270":{"position":[[1497,33]]}}}],["nn.leakyrelu",{"_index":1754,"t":{"1270":{"position":[[601,15],[689,15],[786,15]]}}}],["nn.linear",{"_index":1790,"t":{"1270":{"position":[[1462,9]]}}}],["nn.linear(120",{"_index":1762,"t":{"1270":{"position":[[802,14]]}}}],["nn.linear(16",{"_index":1760,"t":{"1270":{"position":[[758,12]]}}}],["nn.linear(84",{"_index":1764,"t":{"1270":{"position":[[836,13]]}}}],["nn.nllloss",{"_index":1552,"t":{"1208":{"position":[[1514,12]]}}}],["nn.sequential(*lay",{"_index":909,"t":{"1055":{"position":[[741,22]]},"1181":{"position":[[741,22]]}}}],["nn.sequential(*layers).to(devic",{"_index":1076,"t":{"1067":{"position":[[1073,33]]},"1193":{"position":[[1073,33]]}}}],["nn.sigmoid",{"_index":1763,"t":{"1270":{"position":[[822,13]]}}}],["nnn",{"_index":493,"t":{"948":{"position":[[1079,20]]},"1024":{"position":[[712,30],[743,74]]},"1081":{"position":[[365,17]]},"1134":{"position":[[33,3],[56,3]]},"1140":{"position":[[0,29]]},"1320":{"position":[[1079,20]]},"1347":{"position":[[427,9]]},"1428":{"position":[[427,9]]}}}],["node_modul",{"_index":2727,"t":{"1473":{"position":[[56,12]]}}}],["nois",{"_index":657,"t":{"1005":{"position":[[165,19]]},"1007":{"position":[[164,20]]},"1285":{"position":[[579,5],[600,5],[832,5]]},"1370":{"position":[[579,5],[600,5],[832,5]]},"1454":{"position":[[289,5],[806,5],[841,5]]}}}],["none",{"_index":495,"t":{"948":{"position":[[1577,5]]},"1138":{"position":[[2265,5],[2295,5],[4424,5],[4454,5]]},"1270":{"position":[[2360,6],[2440,6],[2447,5]]},"1320":{"position":[[1577,5]]},"1441":{"position":[[394,5],[509,5]]}}}],["norm",{"_index":939,"t":{"1057":{"position":[[1149,4],[1272,4]]},"1067":{"position":[[271,10]]},"1183":{"position":[[1149,4],[1272,4]]},"1193":{"position":[[271,10]]}}}],["normal",{"_index":406,"t":{"932":{"position":[[73,14]]},"1009":{"position":[[0,7]]},"1018":{"position":[[69,6]]},"1030":{"position":[[178,6]]},"1057":{"position":[[881,14]]},"1067":{"position":[[224,15]]},"1138":{"position":[[1826,9],[4122,9]]},"1183":{"position":[[881,14]]},"1193":{"position":[[224,15]]},"1304":{"position":[[73,14]]},"1345":{"position":[[170,16]]},"1347":{"position":[[163,13]]},"1426":{"position":[[170,16]]},"1428":{"position":[[163,13]]}}}],["normalization:\\n",{"_index":1363,"t":{"1138":{"position":[[1959,18],[4255,18]]}}}],["normalization（adaln",{"_index":2440,"t":{"1415":{"position":[[380,21]]},"1488":{"position":[[380,21]]}}}],["normalization）是一种自适应层归一化方法，旨在为不同的样本动态调整归一化参数。与传统的层归一化方法不同，adaln",{"_index":2199,"t":{"1347":{"position":[[1489,63]]},"1428":{"position":[[1490,63]]}}}],["normalization，不需要进行pixelnorm",{"_index":1055,"t":{"1067":{"position":[[432,28]]},"1193":{"position":[[432,28]]}}}],["normalization，之后送入fcn并进行residu",{"_index":431,"t":{"936":{"position":[[119,32]]},"1308":{"position":[[119,32]]}}}],["normalization，以上构成了一个encod",{"_index":433,"t":{"936":{"position":[[186,28]]},"1308":{"position":[[186,28]]}}}],["notebook",{"_index":2732,"t":{"1475":{"position":[[116,11]]}}}],["noun",{"_index":2672,"t":{"1454":{"position":[[7158,13],[7647,5],[8117,6]]}}}],["novel",{"_index":2668,"t":{"1454":{"position":[[6875,5]]}}}],["now",{"_index":39,"t":{"814":{"position":[[138,3],[184,3],[190,3],[218,4],[263,5]]},"844":{"position":[[2315,3]]},"1471":{"position":[[11,3]]}}}],["np",{"_index":1402,"t":{"1138":{"position":[[3295,2]]},"1270":{"position":[[29,2]]},"1441":{"position":[[275,2]]},"1530":{"position":[[789,24]]},"1541":{"position":[[789,24]]}}}],["npm",{"_index":2724,"t":{"1471":{"position":[[172,3]]},"1473":{"position":[[88,3]]}}}],["np完全问题是一类特殊的np问题，没有已知的高效解决算法，并且可以在多项式时间内归约到任何其他的np",{"_index":2837,"t":{"1530":{"position":[[814,52]]},"1541":{"position":[[814,52]]}}}],["ntfsfix",{"_index":2770,"t":{"1509":{"position":[[18,9],[35,7],[64,7]]}}}],["ntfsfix修复ntf",{"_index":2769,"t":{"1509":{"position":[[0,17]]}}}],["nucleu",{"_index":1202,"t":{"1103":{"position":[[13,7]]}}}],["num)(batch,num",{"_index":1365,"t":{"1138":{"position":[[2113,16],[2467,16]]}}}],["num_batch",{"_index":1805,"t":{"1270":{"position":[[1791,11],[2238,12],[2274,11],[2326,12]]}}}],["num_epoch",{"_index":1787,"t":{"1270":{"position":[[1402,11],[1723,12],[2566,10],[2724,10],[2798,11]]}}}],["num_head",{"_index":1317,"t":{"1138":{"position":[[152,28]]},"1144":{"position":[[349,28]]}}}],["num_layers=2",{"_index":1063,"t":{"1067":{"position":[[669,13]]},"1193":{"position":[[669,13]]}}}],["number",{"_index":1331,"t":{"1138":{"position":[[675,8]]},"1251":{"position":[[790,6]]}}}],["numnumnum的注意力分数向量，考虑到有batchbatchbatch个输入向量，因此最终的注意力分数矩阵的形状为(batch,num)(batch",{"_index":1374,"t":{"1138":{"position":[[2358,108]]}}}],["numpi",{"_index":1401,"t":{"1138":{"position":[[3286,5]]},"1270":{"position":[[20,5]]},"1441":{"position":[[266,5]]}}}],["nu}{2}\\tag{25}t=2μ+ν​(25",{"_index":643,"t":{"995":{"position":[[132,27]]}}}],["nvidia",{"_index":824,"t":{"1043":{"position":[[4,6],[117,6]]},"1169":{"position":[[4,6],[117,6]]},"1454":{"position":[[5583,6],[5738,6],[8743,6]]}}}],["n})x=(x1​,x2​,…,xn×n",{"_index":2407,"t":{"1410":{"position":[[1970,22]]},"1483":{"position":[[1970,22]]}}}],["n位整数求余10可以得到最后一位，再除以10可以得到除去上述最后一位之后的n",{"_index":20,"t":{"805":{"position":[[21,41]]}}}],["n是一个四位数，它的9倍恰好是其反序数（例如：1234的反序数是4321），求n",{"_index":1,"t":{"801":{"position":[[4,43]]}}}],["n维向量w=(w1,w2,...,wn)w=(w_1,w_2,...,w_n)w=(w1​,w2​,...,wn",{"_index":1714,"t":{"1263":{"position":[[89,88]]}}}],["n维向量x=(x1,x2,...,xn)x=(x_1,x_2,...,x_n)x=(x1​,x2​,...,xn",{"_index":1713,"t":{"1263":{"position":[[0,88]]}}}],["o",{"_index":155,"t":{"840":{"position":[[1263,1],[1312,1]]}}}],["o(co×ci×h×w×h′×w′)o(c_o",{"_index":1655,"t":{"1235":{"position":[[394,29]]}}}],["o(log(n))\\mathcal{o}(log(n))o(log(n",{"_index":2446,"t":{"1417":{"position":[[421,37]]},"1492":{"position":[[421,37]]}}}],["o(n2)\\mathcal{o}(n^2)o(n2",{"_index":2444,"t":{"1417":{"position":[[336,26]]},"1492":{"position":[[336,26]]}}}],["o(n2)\\mathcal{o}(n^{2})o(n2",{"_index":2408,"t":{"1410":{"position":[[2007,28]]},"1483":{"position":[[2007,28]]}}}],["o(n4)\\mathcal{o}(n^4)o(n4",{"_index":2447,"t":{"1417":{"position":[[464,27]]},"1492":{"position":[[464,27]]}}}],["o(n6)\\mathcal{o}(n^6)o(n6",{"_index":2445,"t":{"1417":{"position":[[370,26]]},"1492":{"position":[[370,26]]}}}],["o(n6)\\mathcal{o}(n^{6})o(n6",{"_index":2409,"t":{"1410":{"position":[[2045,28]]},"1483":{"position":[[2045,28]]}}}],["o1]=[α1,1′][v1](5)\\begin{bmatrix}o_1\\end{bmatrix}=\\begin{bmatrix}\\alpha_{1,1}^{\\prime}\\end{bmatrix}\\begin{bmatrix}v_1\\end{bmatrix}\\tag{5}[o1​​]=[α1,1′​​][v1​​](5",{"_index":487,"t":{"948":{"position":[[265,163]]},"1320":{"position":[[265,163]]}}}],["o1o2]=[α1,1′0α1,2′α2,2′][v1v2](7)\\begin{bmatrix}o_1\\\\o_2\\end{bmatrix}=\\begin{bmatrix}\\alpha_{1,1}^{\\prime}&0\\\\\\alpha_{1,2}^{\\prime}&\\alpha_{2,2}^{\\prime}\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix",{"_index":491,"t":{"948":{"position":[[822,203]]},"1320":{"position":[[822,203]]}}}],["o1o2]=[α1,1′α2,1′α1,2′α2,2′][v1v2](6)\\begin{bmatrix}o_1\\\\o_2\\end{bmatrix}=\\begin{bmatrix}\\alpha_{1,1}^{\\prime}&\\alpha_{2,1}^{\\prime}\\\\\\alpha_{1,2}^{\\prime}&\\alpha_{2,2}^{\\prime}\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix",{"_index":489,"t":{"948":{"position":[[504,227]]},"1320":{"position":[[504,227]]}}}],["o1o2⋮on]=[α1,1′0⋯0α1,2′α2′⋯0⋮⋮⋮α1,n′α2,n′⋯αn,n′][v1v2⋮vn](8)\\begin{bmatrix}o_1\\\\o_2\\\\\\vdots\\\\o_n\\end{bmatrix}=\\begin{bmatrix}\\alpha'_{1,1}&0&\\cdots&0\\\\\\alpha'_{1,2}&\\alpha'_2&\\cdots&0\\\\\\vdots&\\vdots&&\\vdots\\\\\\alpha'_{1,n}&\\alpha'_{2,n}&\\cdots&\\alpha'_{n,n}\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{bmatrix}\\tag{8}​o1​o2​⋮on​​​=​α1,1′​α1,2′​⋮α1,n′​​0α2′​⋮α2,n′​​⋯⋯⋯​00⋮αn,n′​​​​v1​v2​⋮vn​​​(8",{"_index":494,"t":{"948":{"position":[[1100,405]]},"1320":{"position":[[1100,405]]}}}],["object",{"_index":2613,"t":{"1454":{"position":[[4698,10]]},"1545":{"position":[[15,6]]}}}],["observ",{"_index":1901,"t":{"1279":{"position":[[131,7]]},"1364":{"position":[[131,7]]},"1406":{"position":[[976,8]]},"1479":{"position":[[976,8]]}}}],["odena",{"_index":2246,"t":{"1354":{"position":[[259,6]]}}}],["offer",{"_index":514,"t":{"950":{"position":[[180,5]]}}}],["offer的也不用着急，因为每个学生只能上一个学校，928/929当天就算手里一堆offer的学生也是只能选择最想去的学校而释放掉其他offer。稳住心态沉住气，一般10月20",{"_index":2928,"t":{"1592":{"position":[[80,112]]}}}],["ojha",{"_index":2260,"t":{"1354":{"position":[[510,5]]}}}],["on",{"_index":144,"t":{"840":{"position":[[1026,3]]},"1228":{"position":[[46,3]]},"1452":{"position":[[186,3]]}}}],["onc",{"_index":1337,"t":{"1138":{"position":[[823,4]]},"1228":{"position":[[228,4]]},"1230":{"position":[[131,4]]}}}],["open",{"_index":2150,"t":{"1340":{"position":[[1313,4]]},"1349":{"position":[[18,4]]},"1421":{"position":[[1313,4]]}}}],["openai",{"_index":2349,"t":{"1401":{"position":[[14,6]]}}}],["oper",{"_index":1998,"t":{"1324":{"position":[[353,7]]}}}],["operatorname{uniform}[1",{"_index":2577,"t":{"1454":{"position":[[2091,25]]}}}],["opt",{"_index":1048,"t":{"1067":{"position":[[305,5],[375,4]]},"1193":{"position":[[305,5],[375,4]]}}}],["optim",{"_index":1795,"t":{"1270":{"position":[[1570,9]]},"1324":{"position":[[386,12],[807,7]]},"1340":{"position":[[1176,10]]},"1421":{"position":[[1176,10]]},"1454":{"position":[[2431,12]]}}}],["optimizer.step",{"_index":1819,"t":{"1270":{"position":[[2067,16]]}}}],["optimizer.zero_grad",{"_index":1814,"t":{"1270":{"position":[[1964,21]]}}}],["opts.n_ctx",{"_index":1073,"t":{"1067":{"position":[[974,11]]},"1193":{"position":[[974,11]]}}}],["opts=dic(title=\"win_titl",{"_index":1236,"t":{"1116":{"position":[[82,27]]}}}],["opts=dict(title=\"win_titl",{"_index":1228,"t":{"1114":{"position":[[151,29]]}}}],["opt}}eopt",{"_index":2581,"t":{"1454":{"position":[[2269,13]]}}}],["origin",{"_index":1251,"t":{"1122":{"position":[[4,8]]},"1340":{"position":[[74,8]]},"1421":{"position":[[74,8]]}}}],["os",{"_index":1736,"t":{"1270":{"position":[[169,2]]}}}],["os.environ['http_proxi",{"_index":1737,"t":{"1270":{"position":[[172,24]]}}}],["os.environ['https_proxi",{"_index":1739,"t":{"1270":{"position":[[223,25]]}}}],["out",{"_index":1309,"t":{"1134":{"position":[[210,3]]},"1406":{"position":[[1166,3]]},"1479":{"position":[[1166,3]]}}}],["outcom",{"_index":2118,"t":{"1340":{"position":[[511,7]]},"1421":{"position":[[511,7]]}}}],["outperform",{"_index":2133,"t":{"1340":{"position":[[836,13]]},"1406":{"position":[[756,11]]},"1421":{"position":[[836,13]]},"1479":{"position":[[756,11]]}}}],["output",{"_index":197,"t":{"840":{"position":[[1999,7]]},"846":{"position":[[943,7]]},"1071":{"position":[[859,6]]},"1134":{"position":[[60,8],[271,7]]},"1138":{"position":[[2558,7],[2566,7],[2922,6],[4516,7],[4524,7]]},"1197":{"position":[[859,6]]},"1208":{"position":[[1681,7],[1867,7]]},"1275":{"position":[[885,7],[939,7]]},"1441":{"position":[[602,6]]},"1454":{"position":[[6804,6]]}}}],["over",{"_index":2600,"t":{"1454":{"position":[[3708,4]]}}}],["overfitting）以及语言漂移（languag",{"_index":2669,"t":{"1454":{"position":[[7005,41]]}}}],["p",{"_index":853,"t":{"1051":{"position":[[89,1],[364,1]]},"1103":{"position":[[4,1],[67,1]]},"1105":{"position":[[74,1]]},"1177":{"position":[[89,1],[364,1]]},"1530":{"position":[[740,1],[770,18]]},"1541":{"position":[[740,1],[770,18]]}}}],["p(a)=∑i=1np(a∣bi)⋅p(bi)p(a)=\\sum_{i=1}^np(a\\mid",{"_index":2054,"t":{"1330":{"position":[[389,47]]}}}],["p(a)p(a)p(a)和p(b)p(b)p(b",{"_index":2833,"t":{"1530":{"position":[[627,30]]},"1541":{"position":[[627,30]]}}}],["p(a)p(a,b,c)=p(c∣a,b)⋅p(a,b)=p(c∣a,b)⋅p(b∣a)⋅p(a)p(b,c∣a)=p(b∣a)⋅p(c∣a,b)p(b,c\\mid",{"_index":2072,"t":{"1332":{"position":[[124,82]]}}}],["p(a)p(a,b,c)=p(c∣a,b)⋅p(a,b)=p(c∣b)⋅p(b∣a)⋅p(a)p(b,c∣a)=p(b∣a)⋅p(c∣b)p(b,c\\mid",{"_index":2096,"t":{"1334":{"position":[[169,78]]}}}],["p(a)}{p(b)}p(a∣b)=p(b)p(b∣a)∗p(a",{"_index":2830,"t":{"1530":{"position":[[484,34]]},"1541":{"position":[[484,34]]}}}],["p(a,b)=p(c\\mid",{"_index":2069,"t":{"1332":{"position":[[83,14]]},"1334":{"position":[[130,14]]}}}],["p(a,b)p(a,b)p(a,b)是事件aaa、bbb",{"_index":2050,"t":{"1330":{"position":[[180,42]]}}}],["p(a,b,c)=p(c∣a,b)⋅p(a,b)=p(c∣a,b)⋅p(b∣a)⋅p(a)p(a,b,c)=p(c\\mid",{"_index":2067,"t":{"1332":{"position":[[11,61]]}}}],["p(a,b,c)=p(c∣a,b)⋅p(a,b)=p(c∣b)⋅p(b∣a)⋅p(a)p(a,b,c",{"_index":2094,"t":{"1334":{"position":[[59,51]]}}}],["p(a∣b)=p(a,b)p(b)p(a\\mid",{"_index":2048,"t":{"1330":{"position":[[110,24]]}}}],["p(a∣b)=p(b∣a)∗p(a)p(b)p(a|b",{"_index":2828,"t":{"1530":{"position":[[438,28]]},"1541":{"position":[[438,28]]}}}],["p(a∣b)p(a\\mid",{"_index":2046,"t":{"1330":{"position":[[9,53]]}}}],["p(a∣b)p(a|b)p(a∣b",{"_index":2831,"t":{"1530":{"position":[[523,26]]},"1541":{"position":[[523,26]]}}}],["p(a∣bi)p(a\\mid",{"_index":2065,"t":{"1330":{"position":[[829,14]]}}}],["p(b",{"_index":2082,"t":{"1332":{"position":[[459,3],[479,3]]}}}],["p(b)p(b)p(b)是事件bbb",{"_index":2051,"t":{"1330":{"position":[[223,25]]}}}],["p(b,c",{"_index":2077,"t":{"1332":{"position":[[362,5]]}}}],["p(b,c∣a)=p(a,b,c)p(a)=p(a,b,c)p(a,b,c)p(c∣a,b)⋅p(b∣a)=p(b∣a)⋅p(c∣a,b)\\begin{align",{"_index":2076,"t":{"1332":{"position":[[278,83]]}}}],["p(b\\mid",{"_index":2070,"t":{"1332":{"position":[[108,7]]},"1334":{"position":[[153,7]]}}}],["p(b_i)p(a)=i=1∑n​p(a∣bi​)⋅p(bi",{"_index":2056,"t":{"1330":{"position":[[447,32]]}}}],["p(b_i)}{p(a)}p(bi​∣a)=p(a)p(a∣bi​)⋅p(bi",{"_index":2061,"t":{"1330":{"position":[[710,42]]}}}],["p(bi)p(b_i)p(bi​)以及p(a)p(a)p(a",{"_index":2062,"t":{"1330":{"position":[[757,37]]}}}],["p(bi∣a)=p(a∣bi)⋅p(bi)p(a)p(b_i\\mid",{"_index":2059,"t":{"1330":{"position":[[648,34]]}}}],["p(bi∣a)p(b_i\\mid",{"_index":2063,"t":{"1330":{"position":[[795,16]]}}}],["p(b∣a)p(b|a)p(b∣a",{"_index":2832,"t":{"1530":{"position":[[577,23]]},"1541":{"position":[[577,23]]}}}],["p(c",{"_index":2083,"t":{"1332":{"position":[[497,3]]}}}],["p(c\\mid",{"_index":2074,"t":{"1332":{"position":[[226,7]]},"1334":{"position":[[111,8],[267,7]]}}}],["p(r1,r2,…,rk)=∏k=1kp(rk∣r1,r2,…,rk−1)p(r_1",{"_index":2422,"t":{"1413":{"position":[[631,43]]},"1486":{"position":[[631,43]]}}}],["p(r_k",{"_index":2425,"t":{"1413":{"position":[[711,5]]},"1486":{"position":[[711,5]]}}}],["p(x",{"_index":1976,"t":{"1287":{"position":[[466,3]]},"1372":{"position":[[466,3]]}}}],["p(x)=∏t=1tp(xt∣x<t)p(x)=\\prod_{t=1}^t",{"_index":2320,"t":{"1386":{"position":[[427,37]]}}}],["p(xi)p(x_i)p(xi",{"_index":1172,"t":{"1099":{"position":[[89,38],[326,26]]}}}],["p(xi,j∣x<i,j)p\\left(x_{i",{"_index":2324,"t":{"1386":{"position":[[591,25]]}}}],["p(xt∣x<t)p\\left(x_t",{"_index":2315,"t":{"1386":{"position":[[218,19]]}}}],["p(x∣y)p(x|y)p(x∣y)，而图像生成模型需要完成的任务就是将输入的从某一随机分布中sampl",{"_index":715,"t":{"1018":{"position":[[112,119]]}}}],["p(x∣z)p(x",{"_index":1936,"t":{"1285":{"position":[[152,9]]},"1287":{"position":[[11,9]]},"1370":{"position":[[152,9]]},"1372":{"position":[[11,9]]}}}],["p(y)p(y)p(i",{"_index":951,"t":{"1059":{"position":[[383,12],[449,33],[483,13],[991,17]]},"1185":{"position":[[383,12],[449,33],[483,13],[991,17]]}}}],["p(y∣x)p(y|x)p(y∣x",{"_index":949,"t":{"1059":{"position":[[218,18],[428,18]]},"1185":{"position":[[218,18],[428,18]]}}}],["p)(8)d_{kl}(p",{"_index":1520,"t":{"1208":{"position":[[720,13]]}}}],["p)(8)dkl(p",{"_index":1525,"t":{"1208":{"position":[[818,10]]}}}],["p)\\tag{8}dkl​(p",{"_index":1523,"t":{"1208":{"position":[[785,15]]}}}],["p1≥p2≥…≥pvp_1",{"_index":1208,"t":{"1103":{"position":[[360,13]]}}}],["p={p1,p2,…,pv}p=\\left\\{p_1",{"_index":1183,"t":{"1101":{"position":[[142,27]]},"1103":{"position":[[92,27]]}}}],["p\\left(x_t",{"_index":2321,"t":{"1386":{"position":[[465,10]]}}}],["p\\right\\}ck​={wi​∣pi",{"_index":1193,"t":{"1101":{"position":[[456,22]]}}}],["p\\right\\}cp​={wi​∣j=1∑i​pj​≥p}pi′=pi∑wj∈cppj",{"_index":1215,"t":{"1103":{"position":[[596,44]]}}}],["p^(y)=1n∑i=1np(y∣x(i))\\hat{p}(y)=\\frac{1}{n}\\sum_{i=1}^{n}p(y|\\mathbf{x}^{(i)})p^​(y)=n1​∑i=1n​p(y∣x(i",{"_index":952,"t":{"1059":{"position":[[497,104]]},"1185":{"position":[[497,104]]}}}],["p_1\\right),\\left(w_2",{"_index":1204,"t":{"1103":{"position":[[252,21]]}}}],["p_2",{"_index":1184,"t":{"1101":{"position":[[170,4]]},"1103":{"position":[[120,4],[379,3]]}}}],["p_2\\right",{"_index":1205,"t":{"1103":{"position":[[274,11]]}}}],["p_\\text{data}{(x)}}[\\log",{"_index":384,"t":{"925":{"position":[[216,24]]}}}],["p_attn",{"_index":499,"t":{"948":{"position":[[1628,6]]},"1320":{"position":[[1628,6]]}}}],["p_i",{"_index":1192,"t":{"1101":{"position":[[409,3]]}}}],["p_j",{"_index":1197,"t":{"1101":{"position":[[584,4]]},"1103":{"position":[[587,3],[693,4]]}}}],["p_v\\right)\\right\\}{(w1​,p1​),(w2​,p2​),…,(wv​,pv",{"_index":1207,"t":{"1103":{"position":[[304,51]]}}}],["p_v\\right\\}p={p1​,p2​,…,pv",{"_index":1186,"t":{"1101":{"position":[[183,28]]},"1103":{"position":[[133,28]]}}}],["p_vp1​≥p2​≥…≥pv",{"_index":1210,"t":{"1103":{"position":[[400,16]]}}}],["p_z(z)}[\\log",{"_index":394,"t":{"925":{"position":[[842,12]]}}}],["p_z(z)}[log(1",{"_index":389,"t":{"925":{"position":[[391,13]]}}}],["package.json",{"_index":2726,"t":{"1473":{"position":[[12,12]]}}}],["pad",{"_index":1437,"t":{"1154":{"position":[[107,22]]}}}],["padding=2",{"_index":1753,"t":{"1270":{"position":[[589,11]]}}}],["paint",{"_index":995,"t":{"1059":{"position":[[2183,8],[2226,8],[2530,8],[2555,8]]},"1185":{"position":[[2183,8],[2226,8],[2530,8],[2555,8]]},"1406":{"position":[[1156,9],[1170,9]]},"1454":{"position":[[438,95],[722,10],[762,9],[932,8],[1020,9],[1051,8],[1113,8]]},"1479":{"position":[[1156,9],[1170,9]]}}}],["pair",{"_index":1131,"t":{"1083":{"position":[[327,32]]},"1351":{"position":[[208,5]]},"1454":{"position":[[3619,5],[3640,5]]}}}],["paper",{"_index":980,"t":{"1059":{"position":[[1810,5]]},"1185":{"position":[[1810,5]]}}}],["paradigm",{"_index":2104,"t":{"1340":{"position":[[107,8]]},"1406":{"position":[[66,8]]},"1421":{"position":[[107,8]]},"1479":{"position":[[66,8]]}}}],["paragraph",{"_index":1379,"t":{"1138":{"position":[[2633,10]]}}}],["paramet",{"_index":1250,"t":{"1120":{"position":[[87,10]]},"1230":{"position":[[37,9]]},"1340":{"position":[[773,11],[953,11]]},"1421":{"position":[[773,11],[953,11]]}}}],["paraphras",{"_index":1147,"t":{"1083":{"position":[[677,10]]}}}],["part",{"_index":2594,"t":{"1454":{"position":[[3355,6]]}}}],["parti",{"_index":2156,"t":{"1342":{"position":[[90,5]]},"1403":{"position":[[0,5],[6,71],[103,10],[272,14]]},"1423":{"position":[[90,5]]}}}],["partit",{"_index":1619,"t":{"1226":{"position":[[183,11]]}}}],["pass",{"_index":592,"t":{"979":{"position":[[58,4]]}}}],["patch",{"_index":1980,"t":{"1290":{"position":[[68,5]]},"1297":{"position":[[26,5]]},"1375":{"position":[[68,5]]},"1382":{"position":[[26,5]]},"1452":{"position":[[107,5]]}}}],["path",{"_index":1264,"t":{"1122":{"position":[[226,4],[273,4]]}}}],["path/terminal_proxy.sh",{"_index":2767,"t":{"1502":{"position":[[96,23]]}}}],["pay",{"_index":1310,"t":{"1134":{"position":[[230,3]]}}}],["pc",{"_index":78,"t":{"830":{"position":[[102,7]]}}}],["pe(k,2i)\\\\pe(pos+k,2i+1)=pe(pos,2i+1)\\tim",{"_index":415,"t":{"934":{"position":[[835,43]]},"1306":{"position":[[835,43]]}}}],["pe(k,2i)\\end{array",{"_index":418,"t":{"934":{"position":[[907,19]]},"1306":{"position":[[907,19]]}}}],["pe(k,2i+1",{"_index":416,"t":{"934":{"position":[[879,10]]},"1306":{"position":[[879,10]]}}}],["pe(k,2i+1)+pe(pos,2i+1)\\tim",{"_index":414,"t":{"934":{"position":[[805,29]]},"1306":{"position":[[805,29]]}}}],["pe(pos+k,2i)=pe(pos,2i)×pe(k,2i+1)+pe(pos,2i+1)×pe(k,2i)pe(pos+k,2i+1)=pe(pos,2i+1)×pe(k,2i+1)−pe(pos,2i)×pe(k,2i)(3)\\begin{array}{l}pe(pos+k,2i)=pe(pos,2i)\\tim",{"_index":413,"t":{"934":{"position":[[642,162]]},"1306":{"position":[[642,162]]}}}],["pe(pos,2i)=sin(pos100002i/dmodel)(1)pe_{(pos,2i)}=sin(\\frac{pos}{10000^{2i/d_{\\mathrm{model",{"_index":409,"t":{"934":{"position":[[212,96]]},"1306":{"position":[[212,96]]}}}],["pe(pos,2i)\\tim",{"_index":417,"t":{"934":{"position":[[890,16]]},"1306":{"position":[[890,16]]}}}],["penalti",{"_index":1564,"t":{"1212":{"position":[[151,52]]}}}],["per",{"_index":1907,"t":{"1279":{"position":[[319,3],[559,3]]},"1364":{"position":[[319,3],[559,3]]}}}],["perform",{"_index":1613,"t":{"1226":{"position":[[80,11]]},"1228":{"position":[[354,12]]},"1324":{"position":[[1329,11]]},"1340":{"position":[[342,11],[1065,11]]},"1421":{"position":[[342,11],[1065,11]]}}}],["permit",{"_index":253,"t":{"844":{"position":[[918,9]]}}}],["perspect",{"_index":677,"t":{"1005":{"position":[[841,12]]}}}],["perturb",{"_index":2553,"t":{"1454":{"position":[[781,7]]}}}],["photo",{"_index":844,"t":{"1049":{"position":[[44,5],[225,16],[244,5],[255,18]]},"1051":{"position":[[139,5],[150,5],[414,5]]},"1053":{"position":[[72,5],[94,5]]},"1069":{"position":[[59,5],[687,5]]},"1071":{"position":[[621,5],[649,6]]},"1175":{"position":[[44,5],[225,16],[244,5],[255,18]]},"1177":{"position":[[139,5],[150,5],[414,5]]},"1179":{"position":[[72,5],[94,5]]},"1195":{"position":[[59,5],[687,5]]},"1197":{"position":[[621,5],[649,6]]}}}],["photo.’或‘a",{"_index":883,"t":{"1053":{"position":[[83,10]]},"1179":{"position":[[83,10]]}}}],["photorealist",{"_index":746,"t":{"1022":{"position":[[822,14]]}}}],["photo→anim",{"_index":994,"t":{"1059":{"position":[[2171,11],[2518,11]]},"1185":{"position":[[2171,11],[2518,11]]}}}],["photo→cartoon",{"_index":1013,"t":{"1059":{"position":[[2365,13],[2658,13]]},"1185":{"position":[[2365,13],[2658,13]]}}}],["photo→cub",{"_index":1015,"t":{"1059":{"position":[[2407,12],[2700,12]]},"1185":{"position":[[2407,12],[2700,12]]}}}],["photo→disney",{"_index":990,"t":{"1059":{"position":[[2135,12],[2491,12]]},"1185":{"position":[[2135,12],[2491,12]]}}}],["photo→pixar",{"_index":1004,"t":{"1059":{"position":[[2277,11],[2588,11]]},"1185":{"position":[[2277,11],[2588,11]]}}}],["photo→pointil",{"_index":1014,"t":{"1059":{"position":[[2384,17],[2677,17]]},"1185":{"position":[[2384,17],[2677,17]]}}}],["photo→tolkien",{"_index":1006,"t":{"1059":{"position":[[2304,13],[2615,13]]},"1185":{"position":[[2304,13],[2615,13]]}}}],["photo→ukiyo",{"_index":1000,"t":{"1059":{"position":[[2240,11],[2569,11]]},"1185":{"position":[[2240,11],[2569,11]]}}}],["photo→wal",{"_index":999,"t":{"1059":{"position":[[2215,10],[2544,10]]},"1185":{"position":[[2215,10],[2544,10]]}}}],["photo→werewolf",{"_index":1008,"t":{"1059":{"position":[[2327,14],[2638,14]]},"1185":{"position":[[2327,14],[2638,14]]}}}],["pip",{"_index":831,"t":{"1043":{"position":[[124,3],[158,3]]},"1109":{"position":[[0,3]]},"1169":{"position":[[124,3],[158,3]]}}}],["pip_{i}pi",{"_index":1120,"t":{"1081":{"position":[[404,23]]}}}],["piscart",{"_index":2225,"t":{"1349":{"position":[[30,10]]}}}],["pix2video",{"_index":2700,"t":{"1459":{"position":[[100,10]]}}}],["pixel",{"_index":2000,"t":{"1324":{"position":[[373,5],[1516,5]]},"1454":{"position":[[28,5],[86,5],[263,7]]}}}],["pixelcnn",{"_index":2332,"t":{"1391":{"position":[[14,14]]},"1393":{"position":[[114,8],[141,8]]}}}],["pixelnorm",{"_index":900,"t":{"1055":{"position":[[516,9],[597,13]]},"1181":{"position":[[516,9],[597,13]]}}}],["pixelnorm以及全连接层，将每个点归一化（除以模长），避免输入nois",{"_index":1065,"t":{"1067":{"position":[[725,54]]},"1193":{"position":[[725,54]]}}}],["pixelnorm，防止与transformer中的lay",{"_index":1044,"t":{"1067":{"position":[[185,38]]},"1193":{"position":[[185,38]]}}}],["pixelrnn",{"_index":2330,"t":{"1389":{"position":[[0,8],[52,19]]},"1391":{"position":[[5,8]]},"1393":{"position":[[125,8],[152,8]]}}}],["pi′p_i^{\\prime}pi",{"_index":1201,"t":{"1101":{"position":[[652,20]]},"1103":{"position":[[757,22]]}}}],["plasmpkg2",{"_index":2801,"t":{"1514":{"position":[[336,9]]}}}],["platform",{"_index":130,"t":{"840":{"position":[[810,9],[841,9]]}}}],["pleas",{"_index":1367,"t":{"1138":{"position":[[2198,6],[4357,6]]}}}],["plot",{"_index":1404,"t":{"1138":{"position":[[3326,4]]},"1454":{"position":[[667,5]]}}}],["plt",{"_index":1735,"t":{"1270":{"position":[[158,3]]}}}],["plt.show",{"_index":1836,"t":{"1270":{"position":[[2628,10]]}}}],["plugin",{"_index":2714,"t":{"1469":{"position":[[75,8]]}}}],["point",{"_index":233,"t":{"844":{"position":[[660,5],[1743,6]]},"1324":{"position":[[815,5]]}}}],["polynomi",{"_index":2836,"t":{"1530":{"position":[[753,10]]},"1541":{"position":[[753,10]]}}}],["popular",{"_index":2134,"t":{"1340":{"position":[[854,7]]},"1421":{"position":[[854,7]]}}}],["portrait",{"_index":1097,"t":{"1071":{"position":[[738,10]]},"1197":{"position":[[738,10]]}}}],["pos+kpos+kpos+k位置的嵌入向量的某一维度（2i2i2i或2i+12i+12i+1）而言，可以表示为pospospos位置与kkk位置的嵌入向量的2i2i2i与2i+12i+12i+1",{"_index":412,"t":{"934":{"position":[[505,136]]},"1306":{"position":[[505,136]]}}}],["posit",{"_index":2189,"t":{"1347":{"position":[[1017,8]]},"1428":{"position":[[1018,8]]}}}],["pospospos指的是该词汇在整个输入句子中的位置，2i2i2i以及2i+12i+12i+1指的是该词汇的嵌入向量中的维度，dmodeld_{model}dmodel​指的是在嵌入层之后嵌入向量的总维度。即对于每个输入词汇，都要计算dmodeld_{model}dmodel",{"_index":408,"t":{"934":{"position":[[57,154]]},"1306":{"position":[[57,154]]}}}],["possibl",{"_index":2032,"t":{"1324":{"position":[[1140,8]]}}}],["power",{"_index":2001,"t":{"1324":{"position":[[402,8],[650,8],[1006,8]]},"1406":{"position":[[936,5]]},"1479":{"position":[[936,5]]}}}],["ppp",{"_index":1211,"t":{"1103":{"position":[[446,14]]},"1454":{"position":[[6167,3],[6704,4]]}}}],["ppp的坐标为(x,y)(x",{"_index":528,"t":{"955":{"position":[[118,19]]}}}],["practic",{"_index":1242,"t":{"1118":{"position":[[44,8]]}}}],["pre",{"_index":766,"t":{"1024":{"position":[[983,3]]},"1083":{"position":[[1023,6]]}}}],["pre_stylegan",{"_index":841,"t":{"1045":{"position":[[54,14]]},"1171":{"position":[[54,14]]}}}],["precis",{"_index":2232,"t":{"1351":{"position":[[150,7]]}}}],["predict",{"_index":1122,"t":{"1081":{"position":[[442,11]]},"1208":{"position":[[1527,7],[1574,7],[1762,7]]},"1340":{"position":[[95,11]]},"1342":{"position":[[172,10]]},"1406":{"position":[[158,11],[190,12],[255,12]]},"1410":{"position":[[32,10],[946,10],[996,11]]},"1413":{"position":[[166,11],[309,10],[337,10],[1216,10]]},"1421":{"position":[[95,11]]},"1423":{"position":[[172,10]]},"1433":{"position":[[1112,11],[1131,10]]},"1479":{"position":[[158,11],[190,12],[255,12]]},"1483":{"position":[[32,10],[946,10],[996,11]]},"1486":{"position":[[166,11],[309,10],[337,10],[1216,10]]},"1490":{"position":[[192,11]]}}}],["prediction，图（b",{"_index":2399,"t":{"1410":{"position":[[903,25]]},"1483":{"position":[[903,25]]}}}],["predictor",{"_index":658,"t":{"1005":{"position":[[185,46]]},"1007":{"position":[[185,10]]},"1026":{"position":[[245,10]]},"1036":{"position":[[568,14]]}}}],["predictor中。从随机分布中sample出的噪声就是nois",{"_index":784,"t":{"1026":{"position":[[703,34]]}}}],["predictor中，而需要预测出的噪声分布的ground",{"_index":778,"t":{"1026":{"position":[[407,29]]}}}],["predictor对噪声denois",{"_index":807,"t":{"1036":{"position":[[107,29]]}}}],["predictor的ground",{"_index":785,"t":{"1026":{"position":[[738,16]]}}}],["predictor预测出的噪声，αˉ1,αˉ2,...αˉt\\bar{\\alpha}_1,\\bar{\\alpha}_2,...\\bar{\\alpha}_tαˉ1​,αˉ2​,...αˉt​以及α1,α2,...αt\\alpha_1,\\alpha_2,...\\alpha_tα1​,α2​,...αt",{"_index":703,"t":{"1009":{"position":[[592,158]]}}}],["predictor，其输入是加入噪声的图像以及sample出的ttt，而ϵθ\\epsilon_\\thetaϵθ​训练的ground",{"_index":689,"t":{"1007":{"position":[[713,65]]}}}],["predictor，根据文字prompt对从随机分布中sampl",{"_index":665,"t":{"1005":{"position":[[490,54]]}}}],["predictor；在生成图片时，输入从该随机分布中sample出的向量，使用训练出的nois",{"_index":806,"t":{"1036":{"position":[[58,48]]}}}],["predit",{"_index":971,"t":{"1059":{"position":[[1402,9]]},"1185":{"position":[[1402,9]]}}}],["preprocess",{"_index":1256,"t":{"1122":{"position":[[88,10],[161,12]]}}}],["present",{"_index":2350,"t":{"1406":{"position":[[3,7]]},"1479":{"position":[[3,7]]}}}],["preserv",{"_index":2021,"t":{"1324":{"position":[[861,13]]},"1454":{"position":[[7716,12],[7818,12]]}}}],["press",{"_index":297,"t":{"844":{"position":[[2184,7]]}}}],["pretrain",{"_index":1109,"t":{"1079":{"position":[[208,28]]},"1324":{"position":[[659,10]]}}}],["previou",{"_index":2014,"t":{"1324":{"position":[[699,8]]}}}],["prewitt",{"_index":647,"t":{"999":{"position":[[116,10]]}}}],["prim",{"_index":2806,"t":{"1519":{"position":[[100,6]]}}}],["primari",{"_index":2741,"t":{"1475":{"position":[[276,7],[310,7],[349,7],[389,7]]}}}],["print",{"_index":299,"t":{"844":{"position":[[2201,5]]}}}],["print(\"%c\",(asciicode=='\\r",{"_index":226,"t":{"844":{"position":[[443,28],[1658,28]]}}}],["print(\"\\n",{"_index":222,"t":{"844":{"position":[[376,9],[1588,9]]}}}],["print(\"argmax",{"_index":1879,"t":{"1275":{"position":[[871,13]]}}}],["print(\"attent",{"_index":1355,"t":{"1138":{"position":[[1713,16],[1929,16],[4083,16],[4225,16]]}}}],["print(\"keys:\\n",{"_index":1348,"t":{"1138":{"position":[[1352,16],[3916,16]]}}}],["print(\"queries:\\n",{"_index":1349,"t":{"1138":{"position":[[1384,19],[3948,19]]}}}],["print(\"to",{"_index":206,"t":{"844":{"position":[[99,9],[1311,9]]}}}],["print(\"values:\\n",{"_index":1350,"t":{"1138":{"position":[[1422,18],[3986,18]]}}}],["print(\"weight",{"_index":1373,"t":{"1138":{"position":[[2301,15],[4460,15]]}}}],["print(\"welcom",{"_index":292,"t":{"844":{"position":[[2075,14]]}}}],["print(a",{"_index":1863,"t":{"1275":{"position":[[543,7]]}}}],["print(b",{"_index":1865,"t":{"1275":{"position":[[581,7]]}}}],["print(c.shap",{"_index":1852,"t":{"1275":{"position":[[252,14]]}}}],["print(d.shap",{"_index":1857,"t":{"1275":{"position":[[373,14]]}}}],["print(f'loss",{"_index":1828,"t":{"1270":{"position":[[2464,12]]}}}],["print(f'{metric[2",{"_index":1832,"t":{"1270":{"position":[[2545,18]]}}}],["print(i",{"_index":1884,"t":{"1275":{"position":[[1121,8],[1315,8]]}}}],["print(i)16'''17(tensor([1",{"_index":1704,"t":{"1251":{"position":[[651,26]]}}}],["print(i)6'''7(tensor(1",{"_index":1695,"t":{"1251":{"position":[[450,24]]}}}],["print(item",{"_index":2509,"t":{"1445":{"position":[[253,11],[464,11]]}}}],["print(output",{"_index":2480,"t":{"1441":{"position":[[619,13]]}}}],["print(x",{"_index":2474,"t":{"1441":{"position":[[372,8]]}}}],["print(x1",{"_index":2476,"t":{"1441":{"position":[[412,9]]}}}],["print(x2",{"_index":2479,"t":{"1441":{"position":[[527,9]]}}}],["printf(\"%d\\n",{"_index":68,"t":{"824":{"position":[[430,14]]}}}],["prior",{"_index":2543,"t":{"1454":{"position":[[372,5],[7710,5],[7812,5]]}}}],["private_colormap",{"_index":189,"t":{"840":{"position":[[1850,17]]},"846":{"position":[[794,17]]}}}],["probabilist",{"_index":809,"t":{"1036":{"position":[[163,13]]}}}],["probabl",{"_index":143,"t":{"840":{"position":[[1009,8]]},"1101":{"position":[[370,13],[439,13],[498,13]]},"1279":{"position":[[329,11],[569,12]]},"1364":{"position":[[329,11],[569,12]]}}}],["probing（知识探测）方法，给定完形填空（cloz",{"_index":2290,"t":{"1356":{"position":[[23,28]]}}}],["procedur",{"_index":1908,"t":{"1279":{"position":[[372,10]]},"1364":{"position":[[372,10]]},"1454":{"position":[[3671,9]]}}}],["process",{"_index":276,"t":{"844":{"position":[[1821,8]]},"1005":{"position":[[146,18]]},"1036":{"position":[[469,7]]},"1228":{"position":[[163,7]]},"1324":{"position":[[35,7],[287,7]]},"1454":{"position":[[617,7],[883,7]]}}}],["process。此时将加入噪声后的图片、文字prompt以及denoise的步骤序号作为输入，sample出的噪声作为ground",{"_index":813,"t":{"1036":{"position":[[490,65]]}}}],["process又叫做diffus",{"_index":656,"t":{"1005":{"position":[[126,19]]}}}],["process训练的nois",{"_index":664,"t":{"1005":{"position":[[474,15]]}}}],["process：使用diffus",{"_index":663,"t":{"1005":{"position":[[454,19]]}}}],["process：对训练集中的图片不断加入与图片shape相同的、从某随机分布中sample出的噪声，直至图片可以被认为是从该随机分布中sampl",{"_index":655,"t":{"1005":{"position":[[39,78]]}}}],["prod_{k=1}^{k",{"_index":2424,"t":{"1413":{"position":[[695,15]]},"1486":{"position":[[695,15]]}}}],["produc",{"_index":195,"t":{"840":{"position":[[1981,8]]},"846":{"position":[[925,8]]},"1228":{"position":[[306,7]]},"1351":{"position":[[134,8]]}}}],["product",{"_index":1370,"t":{"1138":{"position":[[2229,7],[2666,7],[2808,8],[2879,7],[2981,8],[4388,7]]},"1140":{"position":[[387,8]]},"1148":{"position":[[309,19]]}}}],["product后，为了避免点积运算经过softmax后的输出太小，在点积后除以key",{"_index":1394,"t":{"1138":{"position":[[2997,56]]}}}],["product），得到的scalar数量与输入向量个数相同，都为batchbatchbatch，即scores矩阵的形状应为(batch,batch)(batch",{"_index":1352,"t":{"1138":{"position":[[1547,82]]}}}],["product，在经过softmax",{"_index":1163,"t":{"1083":{"position":[[1322,65]]}}}],["product，而不考虑aia^iai之后的输入的key",{"_index":462,"t":{"940":{"position":[[1305,29]]},"1312":{"position":[[1305,29]]}}}],["program",{"_index":128,"t":{"840":{"position":[[776,8]]}}}],["progress",{"_index":2554,"t":{"1454":{"position":[[816,13]]}}}],["project",{"_index":92,"t":{"834":{"position":[[78,14]]},"840":{"position":[[2148,7]]},"1454":{"position":[[901,8]]}}}],["project0",{"_index":101,"t":{"840":{"position":[[7,26],[95,17]]},"844":{"position":[[87,11],[1299,11]]}}}],["project0实现检测键盘输入ctrl+d",{"_index":204,"t":{"844":{"position":[[50,31]]}}}],["project6这7",{"_index":81,"t":{"830":{"position":[[158,15]]}}}],["promot",{"_index":2385,"t":{"1406":{"position":[[1364,7]]},"1479":{"position":[[1364,7]]}}}],["prompt",{"_index":721,"t":{"1022":{"position":[[101,12]]},"1049":{"position":[[17,15]]},"1051":{"position":[[598,6],[712,6]]},"1053":{"position":[[12,7],[24,7]]},"1055":{"position":[[1014,7],[1035,7],[1054,7],[1137,7],[1174,7],[1196,7],[1229,35]]},"1057":{"position":[[1220,7]]},"1067":{"position":[[47,8]]},"1069":{"position":[[21,7],[297,7],[380,7],[528,7],[859,7],[897,7],[947,7],[968,7],[1000,7],[1033,7],[1053,7],[1073,7],[1155,7],[1173,7],[1212,7]]},"1071":{"position":[[90,16],[146,7],[163,23],[308,7],[393,7],[416,7],[437,8],[674,7],[770,7],[815,6],[934,7],[967,7],[1052,10],[1078,7],[1095,7],[1112,6],[1129,6]]},"1175":{"position":[[17,15]]},"1177":{"position":[[598,6],[712,6]]},"1179":{"position":[[12,7],[24,7]]},"1181":{"position":[[1014,7],[1035,7],[1054,7],[1137,7],[1174,7],[1196,7],[1229,35]]},"1183":{"position":[[1220,7]]},"1193":{"position":[[47,8]]},"1195":{"position":[[21,7],[297,7],[380,7],[528,7],[859,7],[897,7],[947,7],[968,7],[1000,7],[1033,7],[1053,7],[1073,7],[1155,7],[1173,7],[1212,7]]},"1197":{"position":[[90,16],[146,7],[163,23],[308,7],[393,7],[416,7],[437,8],[674,7],[770,7],[815,6],[934,7],[967,7],[1052,10],[1078,7],[1095,7],[1112,6],[1129,6]]},"1351":{"position":[[20,6]]},"1454":{"position":[[3585,6],[3595,6]]},"1490":{"position":[[92,6]]}}}],["prompt2prompt",{"_index":2604,"t":{"1454":{"position":[[4252,13]]}}}],["prompt_prefix",{"_index":846,"t":{"1049":{"position":[[73,14],[132,14]]},"1175":{"position":[[73,14],[132,14]]}}}],["prompt工程最初是一种knowledg",{"_index":2289,"t":{"1356":{"position":[[0,22]]}}}],["prompt期待生成的图像并不是固定的，可以认为生成的图片在目标域（target",{"_index":659,"t":{"1005":{"position":[[240,72]]},"1018":{"position":[[242,49]]}}}],["prompt通常不是最优的，可能提供不准确的适应方向。为了解决这个问题，在nlp领域的prompt",{"_index":2292,"t":{"1356":{"position":[[81,56]]}}}],["prompt（‘a",{"_index":882,"t":{"1053":{"position":[[62,9]]},"1179":{"position":[[62,9]]}}}],["pronounc",{"_index":369,"t":{"910":{"position":[[12,9]]}}}],["propag",{"_index":476,"t":{"946":{"position":[[132,13]]},"1318":{"position":[[132,13]]}}}],["properli",{"_index":2113,"t":{"1340":{"position":[[365,9]]},"1421":{"position":[[365,9]]}}}],["properti",{"_index":2117,"t":{"1340":{"position":[[435,10]]},"1406":{"position":[[1260,10]]},"1421":{"position":[[435,10]]},"1479":{"position":[[1260,10]]}}}],["propos",{"_index":1906,"t":{"1279":{"position":[[298,7]]},"1364":{"position":[[298,7]]}}}],["protector",{"_index":160,"t":{"840":{"position":[[1331,9]]}}}],["provid",{"_index":1094,"t":{"1071":{"position":[[656,7]]},"1197":{"position":[[656,7]]}}}],["proxy_off",{"_index":2759,"t":{"1500":{"position":[[209,12]]},"1504":{"position":[[43,9]]}}}],["proxy_on",{"_index":2754,"t":{"1500":{"position":[[40,11]]},"1504":{"position":[[17,8]]}}}],["pr曲线所围成的面积即使该类的ap",{"_index":2862,"t":{"1554":{"position":[[0,18]]}}}],["psnr：峰值信噪比，图像所允许的最大像素值的平方与均方误差的比值的对数的10",{"_index":544,"t":{"957":{"position":[[217,81]]}}}],["psp",{"_index":1032,"t":{"1062":{"position":[[645,3]]},"1188":{"position":[[645,3]]}}}],["put",{"_index":1098,"t":{"1071":{"position":[[749,3]]},"1197":{"position":[[749,3]]}}}],["pyplot",{"_index":1734,"t":{"1270":{"position":[[148,6]]}}}],["pyramid",{"_index":2751,"t":{"1490":{"position":[[166,11]]}}}],["python",{"_index":1100,"t":{"1071":{"position":[[792,6],[877,6]]},"1112":{"position":[[44,6]]},"1197":{"position":[[792,6],[877,6]]},"1275":{"position":[[986,6]]},"1441":{"position":[[197,46]]},"1443":{"position":[[0,61],[62,85]]}}}],["python=3.8",{"_index":822,"t":{"1041":{"position":[[20,10]]},"1167":{"position":[[20,10]]}}}],["pytorch",{"_index":826,"t":{"1043":{"position":[[22,7],[54,7],[85,7],[106,7]]},"1064":{"position":[[12,7]]},"1122":{"position":[[445,7]]},"1169":{"position":[[22,7],[54,7],[85,7],[106,7]]},"1190":{"position":[[12,7]]}}}],["pytorch中的crossentropyloss",{"_index":1546,"t":{"1208":{"position":[[1367,26]]}}}],["pytorch框架，可以使用visdom或tensorboardx实现可视化，本篇主要讲述visdom",{"_index":1221,"t":{"1107":{"position":[[41,54]]}}}],["p}c_k=\\left\\{w_i",{"_index":1190,"t":{"1101":{"position":[[387,16]]}}}],["p}pi′=pi∑wj∈ckpj",{"_index":1194,"t":{"1101":{"position":[[515,16]]}}}],["p′(xi)=exp⁡(log⁡(p(xi))t)∑jexp⁡(log⁡(p(xj))t)p'(x_i",{"_index":1173,"t":{"1099":{"position":[[128,52]]}}}],["p∼n(μ1,σ12)p\\sim",{"_index":2085,"t":{"1332":{"position":[[609,29]]}}}],["p和q",{"_index":1536,"t":{"1208":{"position":[[1123,30]]}}}],["p和q，其离散型随机变量xxx的可能取值为x=x1,x2,...,xnx=x_1,x_2,...,x_nx=x1​,x2​,...,xn​，而取值事件xix_ixi​发生的概率分别为pi,qip_i,q_ipi​,qi",{"_index":1517,"t":{"1208":{"position":[[567,119]]}}}],["p问题、np问题以及np",{"_index":2834,"t":{"1530":{"position":[[719,16]]},"1541":{"position":[[719,16]]}}}],["q",{"_index":1426,"t":{"1148":{"position":[[285,1],[329,1]]},"1208":{"position":[[751,2],[887,2],[1218,2],[1256,2]]},"1454":{"position":[[1403,1]]}}}],["q(i,j)=(arg⁡min⁡v∈[v]∥lookup(z,v)−f(i,j)∥2)∈[v]q^{(i,j)}=\\left(\\arg\\min_{v\\in[v]}\\|\\text{lookup}(z,v",{"_index":2391,"t":{"1410":{"position":[[276,101]]},"1483":{"position":[[276,101]]}}}],["q)=h(p)+dkl(p",{"_index":1537,"t":{"1208":{"position":[[1159,13]]}}}],["q)=∑pi",{"_index":1526,"t":{"1208":{"position":[[832,6],[948,7]]}}}],["q)=−∑pi",{"_index":1538,"t":{"1208":{"position":[[1176,7],[1341,8]]}}}],["q)​=h(p)+dkl​(p",{"_index":1544,"t":{"1208":{"position":[[1322,15]]}}}],["q)≠dkl(q",{"_index":1519,"t":{"1208":{"position":[[708,8]]}}}],["q)=dkl​(q",{"_index":1524,"t":{"1208":{"position":[[804,10]]}}}],["q=q(f)q=\\mathcal{q}(f)q=q(f",{"_index":2390,"t":{"1410":{"position":[[193,28]]},"1483":{"position":[[193,28]]}}}],["q_{pred})dkl​(plabel",{"_index":1534,"t":{"1208":{"position":[[1039,21]]}}}],["qnli（quest",{"_index":1132,"t":{"1083":{"position":[[360,13]]}}}],["qpred)d_{kl}(p_{label",{"_index":1533,"t":{"1208":{"position":[[1000,22]]}}}],["qpred​)表示当用概率分布q来拟合真实分布p时，产生的信息损耗，其中p表示真实分布，q表示p",{"_index":1535,"t":{"1208":{"position":[[1063,54]]}}}],["qqp（quora",{"_index":1129,"t":{"1083":{"position":[[308,9]]}}}],["qr",{"_index":2840,"t":{"1536":{"position":[[736,52]]},"1539":{"position":[[876,52]]}}}],["quadex,c,ϵ,t​[wt​∥x~θ​(αt​x+σt​ϵ,c)−x∥22",{"_index":2661,"t":{"1454":{"position":[[6602,43]]}}}],["qualiti",{"_index":742,"t":{"1022":{"position":[[718,8]]},"1324":{"position":[[588,7]]},"1340":{"position":[[498,8],[619,7],[1023,7],[1087,7]]},"1406":{"position":[[839,8]]},"1421":{"position":[[498,8],[619,7],[1023,7],[1087,7]]},"1479":{"position":[[839,8]]}}}],["quantiz",{"_index":1895,"t":{"1277":{"position":[[94,13]]},"1279":{"position":[[110,9],[822,13]]},"1281":{"position":[[225,35]]},"1345":{"position":[[23,9]]},"1362":{"position":[[94,13]]},"1364":{"position":[[110,9],[822,13]]},"1366":{"position":[[225,35]]},"1393":{"position":[[188,9],[329,9]]},"1397":{"position":[[119,9]]},"1399":{"position":[[141,12]]},"1413":{"position":[[1244,16],[1264,9]]},"1415":{"position":[[15,10],[210,9]]},"1417":{"position":[[197,12]]},"1426":{"position":[[23,9]]},"1433":{"position":[[701,13]]},"1486":{"position":[[1244,16],[1264,9]]},"1488":{"position":[[15,10],[210,9]]},"1492":{"position":[[197,12]]}}}],["quantizer（var",{"_index":2439,"t":{"1415":{"position":[[230,13]]},"1488":{"position":[[230,13]]}}}],["queri",{"_index":1340,"t":{"1138":{"position":[[935,7],[1258,7],[1299,7],[1404,8],[1696,7],[2825,5],[3519,7],[3822,7],[3863,7],[3968,8],[4066,7]]},"1454":{"position":[[1312,5],[6848,5]]}}}],["query^{n",{"_index":1408,"t":{"1140":{"position":[[195,8]]}}}],["query、key、valu",{"_index":1419,"t":{"1148":{"position":[[107,19]]}}}],["query分别与所有向量的key做内积（dot",{"_index":1351,"t":{"1138":{"position":[[1504,42]]}}}],["query来自解码器，key和value来自编码器。解码器每个向量的查询（query）与编码器位置的键（key）进行点积得到了注意力分数，通过softmax操作后转换为注意力权重，再与编码器位置的值（value）weight",{"_index":469,"t":{"940":{"position":[[1591,132]]},"1312":{"position":[[1591,132]]}}}],["query）向量、键（key）向量和值（valu",{"_index":2924,"t":{"1576":{"position":[[49,55]]}}}],["query，q）、键（key，k）和值（value，v",{"_index":1418,"t":{"1148":{"position":[[0,106]]}}}],["question",{"_index":1130,"t":{"1083":{"position":[[318,8],[1177,8]]}}}],["quotient",{"_index":2810,"t":{"1525":{"position":[[80,9],[293,29],[657,9]]},"1527":{"position":[[167,9],[380,29],[744,9]]}}}],["quotient.assign(\"0",{"_index":2818,"t":{"1525":{"position":[[626,21]]},"1527":{"position":[[713,21]]}}}],["quotient.empti",{"_index":2816,"t":{"1525":{"position":[[418,19],[576,18]]},"1527":{"position":[[505,19],[663,18]]}}}],["quotient.push_back('0",{"_index":2817,"t":{"1525":{"position":[[477,24]]},"1527":{"position":[[564,24]]}}}],["quotient.push_back(temp",{"_index":2815,"t":{"1525":{"position":[[323,23]]},"1527":{"position":[[410,23]]}}}],["q、k",{"_index":1314,"t":{"1138":{"position":[[75,3]]},"1144":{"position":[[272,3]]}}}],["q、k、v",{"_index":1420,"t":{"1148":{"position":[[164,5],[234,5]]},"1150":{"position":[[82,40]]}}}],["r",{"_index":2561,"t":{"1454":{"position":[[1339,1],[1389,1]]}}}],["r1,...,rkr_1,...,r_kr1​,...,rk",{"_index":2441,"t":{"1415":{"position":[[444,31]]},"1488":{"position":[[444,31]]}}}],["r1,r2,…,rk)(r_1",{"_index":2415,"t":{"1413":{"position":[[484,17]]},"1486":{"position":[[484,17]]}}}],["r1,r2,…,rk−1)(r_1",{"_index":2432,"t":{"1413":{"position":[[911,19]]},"1486":{"position":[[911,19]]}}}],["r_1",{"_index":2426,"t":{"1413":{"position":[[722,4]]},"1486":{"position":[[722,4]]}}}],["r_2",{"_index":2416,"t":{"1413":{"position":[[502,4],[675,4],[727,4],[931,4]]},"1486":{"position":[[502,4],[675,4],[727,4],[931,4]]}}}],["r_k",{"_index":2423,"t":{"1413":{"position":[[688,4]]},"1486":{"position":[[688,4]]}}}],["r_k)(r1​,r2​,…,rk",{"_index":2417,"t":{"1413":{"position":[[515,27]]},"1486":{"position":[[515,27]]}}}],["r_{k",{"_index":2427,"t":{"1413":{"position":[[740,4],[944,4]]},"1486":{"position":[[740,4],[944,4]]}}}],["random",{"_index":1277,"t":{"1122":{"position":[[415,7],[580,7]]},"1138":{"position":[[726,6]]},"1454":{"position":[[7346,6]]}}}],["randomli",{"_index":1254,"t":{"1122":{"position":[[38,8]]},"1138":{"position":[[696,8]]}}}],["rang",{"_index":1914,"t":{"1279":{"position":[[692,5]]},"1340":{"position":[[747,7]]},"1364":{"position":[[692,5]]},"1421":{"position":[[747,7]]},"1454":{"position":[[7479,5]]}}}],["range(n_mlp",{"_index":903,"t":{"1055":{"position":[[620,13]]},"1181":{"position":[[620,13]]}}}],["range(num_epoch",{"_index":1810,"t":{"1270":{"position":[[1887,18]]}}}],["rank",{"_index":2939,"t":{"1594":{"position":[[177,27]]}}}],["rank：前1%（非rank",{"_index":2932,"t":{"1594":{"position":[[31,14]]}}}],["raster",{"_index":2354,"t":{"1406":{"position":[[231,6]]},"1479":{"position":[[231,6]]}}}],["ratio",{"_index":2121,"t":{"1340":{"position":[[591,5]]},"1421":{"position":[[591,5]]}}}],["raw",{"_index":1257,"t":{"1122":{"position":[[103,3]]},"1138":{"position":[[1661,3],[4032,3]]},"1410":{"position":[[79,3]]},"1483":{"position":[[79,3]]}}}],["rd\\varepsilon",{"_index":1944,"t":{"1285":{"position":[[364,15]]},"1370":{"position":[[364,15]]}}}],["re",{"_index":2169,"t":{"1347":{"position":[[261,2]]},"1428":{"position":[[261,2]]},"1435":{"position":[[0,5]]}}}],["reach",{"_index":2016,"t":{"1324":{"position":[[794,5]]}}}],["real",{"_index":2562,"t":{"1454":{"position":[[1463,4],[3905,4]]}}}],["realism",{"_index":2545,"t":{"1454":{"position":[[394,8]]}}}],["reboot",{"_index":2774,"t":{"1509":{"position":[[95,6]]}}}],["reconstrcut",{"_index":2584,"t":{"1454":{"position":[[2532,14]]}}}],["reconstruct",{"_index":2122,"t":{"1340":{"position":[[604,14]]},"1410":{"position":[[1529,11]]},"1421":{"position":[[604,14]]},"1454":{"position":[[2704,14]]},"1483":{"position":[[1529,11]]}}}],["rectifi",{"_index":2465,"t":{"1435":{"position":[[116,9]]}}}],["redefin",{"_index":2351,"t":{"1406":{"position":[[80,9]]},"1479":{"position":[[80,9]]}}}],["redistribut",{"_index":254,"t":{"844":{"position":[[938,13]]}}}],["reduc",{"_index":1868,"t":{"1275":{"position":[[680,6]]},"1324":{"position":[[1468,8]]}}}],["reduce项时只写前向搜索符对应的act",{"_index":360,"t":{"893":{"position":[[566,53]]}}}],["reduce项时只写前向搜索符集对应的act",{"_index":362,"t":{"893":{"position":[[645,86]]}}}],["reduce项时，act",{"_index":354,"t":{"893":{"position":[[229,30]]}}}],["reduce项时，只写产生式左部的follow集对应的act",{"_index":357,"t":{"893":{"position":[[392,42]]}}}],["reduct",{"_index":2019,"t":{"1324":{"position":[[840,9]]}}}],["reexamin",{"_index":2114,"t":{"1340":{"position":[[378,9]]},"1421":{"position":[[378,9]]}}}],["refer",{"_index":151,"t":{"840":{"position":[[1111,9]]},"1454":{"position":[[57,9],[5766,9]]}}}],["reflect",{"_index":167,"t":{"840":{"position":[[1488,7]]},"846":{"position":[[432,7]]}}}],["regex",{"_index":833,"t":{"1043":{"position":[[141,5]]},"1169":{"position":[[141,5]]}}}],["regress",{"_index":1674,"t":{"1245":{"position":[[9,51]]}}}],["regular",{"_index":1086,"t":{"1069":{"position":[[775,14]]},"1071":{"position":[[997,15],[1022,14]]},"1195":{"position":[[775,14]]},"1197":{"position":[[997,15],[1022,14]]},"1212":{"position":[[0,150]]},"1354":{"position":[[295,14]]}}}],["rehyp",{"_index":2716,"t":{"1469":{"position":[[119,6],[271,6]]}}}],["relat",{"_index":2702,"t":{"1461":{"position":[[98,7]]}}}],["relax",{"_index":2228,"t":{"1349":{"position":[[114,7]]}}}],["releas",{"_index":2149,"t":{"1340":{"position":[[1270,7]]},"1406":{"position":[[1331,8]]},"1421":{"position":[[1270,7]]},"1479":{"position":[[1331,8]]}}}],["relev",{"_index":1381,"t":{"1138":{"position":[[2682,10]]}}}],["relu",{"_index":1604,"t":{"1221":{"position":[[0,8]]},"1347":{"position":[[649,21]]},"1428":{"position":[[649,21]]},"1574":{"position":[[141,4]]}}}],["relu激活函数。因为relu的范围是[0",{"_index":1843,"t":{"1272":{"position":[[0,43]]}}}],["remain",{"_index":1623,"t":{"1228":{"position":[[102,9]]}}}],["remaind",{"_index":2812,"t":{"1525":{"position":[[133,9],[217,9],[365,9],[504,9]]},"1527":{"position":[[220,9],[304,9],[452,9],[591,9],[868,9]]}}}],["remark",{"_index":2715,"t":{"1469":{"position":[[101,6],[256,6]]},"1471":{"position":[[81,6],[122,6],[191,6]]}}}],["remov",{"_index":1919,"t":{"1279":{"position":[[806,8]]},"1364":{"position":[[806,8]]},"1454":{"position":[[830,6]]}}}],["render",{"_index":2711,"t":{"1469":{"position":[[25,6]]}}}],["repeat",{"_index":1626,"t":{"1228":{"position":[[174,8]]},"1263":{"position":[[580,6]]},"1475":{"position":[[225,9]]}}}],["repo",{"_index":1245,"t":{"1118":{"position":[[96,5]]}}}],["repres",{"_index":1279,"t":{"1122":{"position":[[434,10]]},"1279":{"position":[[189,12]]},"1364":{"position":[[189,12]]},"1454":{"position":[[673,9]]}}}],["represent",{"_index":724,"t":{"1022":{"position":[[238,15],[425,21]]},"1026":{"position":[[468,15],[588,15],[1060,15]]},"1079":{"position":[[77,15]]},"1324":{"position":[[750,14]]},"1433":{"position":[[2031,14]]}}}],["representation。因此，训练gener",{"_index":774,"t":{"1026":{"position":[[195,30]]}}}],["representation分别输入至源域生成器和目标域生成器中得到对应的图像，同时指导风格迁移方向的源域以及目标域的prompt描述由lat",{"_index":2309,"t":{"1359":{"position":[[716,74]]}}}],["representation时，同样采取从已知随机分布中sample出噪声再添加到网络的输入作为生成ground",{"_index":780,"t":{"1026":{"position":[[496,56]]}}}],["representation时，需要训练一个auto",{"_index":793,"t":{"1026":{"position":[[1089,26]]}}}],["representation的分布也是符合某种随机分布的，vae再通过decod",{"_index":672,"t":{"1005":{"position":[[676,58]]}}}],["representation），输出的是还原出的高分辨率的图像，它的训练是不需要额外pair",{"_index":790,"t":{"1026":{"position":[[867,49]]}}}],["representation，decoder再吃encoder的输出，最终输出还原出的高清label图片，训练的方向是让输出的图片与输入的图片越接近越好。在这个过程中，不需要额外的标注，auto",{"_index":797,"t":{"1026":{"position":[[1209,97]]}}}],["representation，生成一组prompt向量。第一阶段需要解决两个问题，即在zero",{"_index":2298,"t":{"1359":{"position":[[111,47]]}}}],["representation，经过从随机分布中sample出的噪声的加入，输入至nois",{"_index":783,"t":{"1026":{"position":[[657,45]]}}}],["representation，这种lat",{"_index":671,"t":{"1005":{"position":[[652,23]]}}}],["requir",{"_index":2041,"t":{"1324":{"position":[[1491,12]]}}}],["research",{"_index":1146,"t":{"1083":{"position":[[668,8]]},"1349":{"position":[[44,8]]}}}],["residu",{"_index":2843,"t":{"1547":{"position":[[20,8],[113,11]]}}}],["resolut",{"_index":2031,"t":{"1324":{"position":[[1111,10],[1436,11]]},"1406":{"position":[[179,10]]},"1433":{"position":[[1101,10]]},"1452":{"position":[[307,10]]},"1457":{"position":[[224,10]]},"1479":{"position":[[179,10]]}}}],["resourc",{"_index":2010,"t":{"1324":{"position":[[556,9]]},"1514":{"position":[[161,9]]}}}],["respect",{"_index":2552,"t":{"1454":{"position":[[733,13]]}}}],["restyl",{"_index":1029,"t":{"1062":{"position":[[433,7]]},"1064":{"position":[[186,7]]},"1188":{"position":[[433,7]]},"1190":{"position":[[186,7]]}}}],["restyle_e4e_encod",{"_index":1026,"t":{"1062":{"position":[[382,26],[582,19]]},"1188":{"position":[[382,26],[582,19]]}}}],["result",{"_index":1628,"t":{"1228":{"position":[[254,7]]},"1279":{"position":[[872,7]]},"1324":{"position":[[158,7]]},"1364":{"position":[[872,7]]},"1406":{"position":[[1199,7]]},"1454":{"position":[[340,9]]},"1479":{"position":[[1199,7]]}}}],["retain",{"_index":2011,"t":{"1324":{"position":[[572,9]]}}}],["retrain",{"_index":1997,"t":{"1324":{"position":[[303,11]]}}}],["return",{"_index":19,"t":{"803":{"position":[[203,6]]},"814":{"position":[[358,6]]},"824":{"position":[[461,6]]},"1134":{"position":[[48,7]]},"1270":{"position":[[386,6],[877,6],[1268,6],[1321,6]]},"1525":{"position":[[650,6]]},"1527":{"position":[[737,6],[1097,6]]}}}],["revers",{"_index":662,"t":{"1005":{"position":[[446,7]]},"1454":{"position":[[865,7]]}}}],["revis",{"_index":249,"t":{"844":{"position":[[865,10]]}}}],["rfid",{"_index":2124,"t":{"1340":{"position":[[635,4]]},"1421":{"position":[[635,4]]}}}],["rgb",{"_index":1295,"t":{"1129":{"position":[[445,3]]},"1454":{"position":[[24,3],[82,3],[259,3],[421,3]]}}}],["richard",{"_index":2270,"t":{"1354":{"position":[[587,7]]}}}],["right",{"_index":2660,"t":{"1454":{"position":[[6594,7]]}}}],["right]ex,c,ϵ,ϵ′,t​[wt​∥x^θ​(αt​x+σt​ϵ,c)−x∥22​+λwt′​∥x^θ​(αt′​xpr​+σt′​ϵ′,cpr​)−xpr​∥22",{"_index":2691,"t":{"1454":{"position":[[8473,90]]}}}],["rightarrow",{"_index":310,"t":{"857":{"position":[[89,13]]},"859":{"position":[[16,13]]},"861":{"position":[[135,11]]}}}],["rkr_krk",{"_index":579,"t":{"973":{"position":[[67,26],[524,74]]},"1413":{"position":[[580,8],[973,8],[1006,8],[1047,8],[1120,8]]},"1486":{"position":[[580,8],[973,8],[1006,8],[1047,8],[1120,8]]}}}],["rk∈[v]hk×wkr_k",{"_index":2429,"t":{"1413":{"position":[[804,14]]},"1486":{"position":[[804,14]]}}}],["rmsnorm",{"_index":2167,"t":{"1347":{"position":[[132,7],[140,13],[184,7],[236,7],[457,7]]},"1428":{"position":[[132,7],[140,13],[184,7],[236,7],[457,7]]}}}],["rmsnorm(x)=x1n∑i=1nxi2∗grmsnorm(x)=\\frac",{"_index":2171,"t":{"1347":{"position":[[313,40]]},"1428":{"position":[[313,40]]}}}],["rnn",{"_index":2331,"t":{"1389":{"position":[[9,38],[48,3]]}}}],["rnn的输入是(seq_len",{"_index":2515,"t":{"1447":{"position":[[174,17]]}}}],["rnn输入默认不是batch",{"_index":2517,"t":{"1447":{"position":[[288,19]]}}}],["robert",{"_index":645,"t":{"999":{"position":[[0,9]]}}}],["romimag",{"_index":172,"t":{"840":{"position":[[1587,9]]},"846":{"position":[[531,9]]}}}],["root",{"_index":1385,"t":{"1138":{"position":[[2743,4]]}}}],["rope",{"_index":2187,"t":{"1347":{"position":[[999,5],[1393,4]]},"1428":{"position":[[1000,5],[1394,4]]}}}],["rope⁡(xi)=xi⋅cos⁡(θi)+xi+1⋅sin⁡(θi)\\operatorname{rope}\\left(x_i\\right)=x_i",{"_index":2191,"t":{"1347":{"position":[[1135,74]]},"1428":{"position":[[1136,74]]}}}],["rope（rotari",{"_index":2188,"t":{"1347":{"position":[[1005,11]]},"1428":{"position":[[1006,11]]}}}],["rq",{"_index":2345,"t":{"1399":{"position":[[112,7]]}}}],["rqtransform",{"_index":2344,"t":{"1399":{"position":[[0,13],[14,60]]}}}],["rte（recogn",{"_index":1136,"t":{"1083":{"position":[[439,15]]}}}],["run_loop",{"_index":1031,"t":{"1062":{"position":[[569,8]]},"1188":{"position":[[569,8]]}}}],["rɪˈzɪdjuəl",{"_index":2844,"t":{"1547":{"position":[[29,12]]}}}],["r≤kr_{\\leq",{"_index":2436,"t":{"1413":{"position":[[1137,10]]},"1486":{"position":[[1137,10]]}}}],["s",{"_index":22,"t":{"805":{"position":[[142,14]]},"814":{"position":[[66,2],[76,2]]},"838":{"position":[[92,1]]},"857":{"position":[[67,5],[87,1]]},"859":{"position":[[0,5]]},"898":{"position":[[307,1],[432,1],[440,15]]},"900":{"position":[[43,1]]},"902":{"position":[[47,1],[55,15]]},"1509":{"position":[[33,1]]},"1525":{"position":[[23,2]]},"1527":{"position":[[110,2],[786,2],[803,2],[851,2],[930,1]]}}}],["s(\\ell_c",{"_index":2212,"t":{"1347":{"position":[[2136,8]]},"1428":{"position":[[2137,8]]}}}],["s(k)=ceil(sk×l−1)(13)s(k)=\\mathbf{ceil}(s_k\\tim",{"_index":582,"t":{"973":{"position":[[158,49]]}}}],["s(k)s(k)s(k",{"_index":581,"t":{"973":{"position":[[119,38],[366,29]]}}}],["s(k)s(k)s(k)和z(k)z(k)z(k)都是归一化后的均匀分布，使用二者之间的就近原则将s(k)s(k)s(k",{"_index":586,"t":{"973":{"position":[[430,78]]}}}],["s)^2+(i",{"_index":531,"t":{"955":{"position":[[219,7]]}}}],["s.end",{"_index":38,"t":{"814":{"position":[[119,9],[308,10]]}}}],["s.size",{"_index":24,"t":{"805":{"position":[[190,9]]},"814":{"position":[[167,9]]},"1525":{"position":[[181,9]]},"1527":{"position":[[268,9],[835,9],[956,9]]}}}],["s[i",{"_index":25,"t":{"805":{"position":[[224,5]]},"814":{"position":[[201,4]]}}}],["s[idx",{"_index":2814,"t":{"1525":{"position":[[234,7]]},"1527":{"position":[[321,7]]}}}],["s[len",{"_index":2821,"t":{"1527":{"position":[[880,6]]}}}],["s\\vert+\\vert",{"_index":534,"t":{"955":{"position":[[310,12]]}}}],["s\\vert,\\vert",{"_index":538,"t":{"955":{"position":[[416,12]]}}}],["salient",{"_index":2842,"t":{"1545":{"position":[[0,14]]}}}],["same",{"_index":137,"t":{"840":{"position":[[924,4]]},"1071":{"position":[[787,4]]},"1197":{"position":[[787,4]]}}}],["sampl",{"_index":741,"t":{"1022":{"position":[[711,6]]},"1062":{"position":[[126,6]]},"1089":{"position":[[0,13]]},"1091":{"position":[[0,101]]},"1188":{"position":[[126,6]]},"1454":{"position":[[7353,8]]}}}],["sample_w",{"_index":892,"t":{"1055":{"position":[[111,8]]},"1181":{"position":[[111,8]]}}}],["sample_z",{"_index":887,"t":{"1055":{"position":[[22,8]]},"1181":{"position":[[22,8]]}}}],["sample出原始图像x0\\mathbf{x}_0x0",{"_index":679,"t":{"1007":{"position":[[13,36]]}}}],["sample出的向量，并在这个过程中训练出nois",{"_index":805,"t":{"1036":{"position":[[0,57]]}}}],["sampling来获得压缩版本的图像作为decod",{"_index":792,"t":{"1026":{"position":[[1013,34]]}}}],["sangwoo",{"_index":2250,"t":{"1354":{"position":[[362,11]]}}}],["save",{"_index":2725,"t":{"1471":{"position":[[186,4]]}}}],["scalabl",{"_index":2116,"t":{"1340":{"position":[[423,11]]},"1342":{"position":[[408,8]]},"1406":{"position":[[886,12]]},"1421":{"position":[[423,11]]},"1423":{"position":[[408,8]]},"1435":{"position":[[64,8]]},"1452":{"position":[[142,8]]},"1479":{"position":[[886,12]]}}}],["scale",{"_index":735,"t":{"1022":{"position":[[589,7],[656,7],[736,7]]},"1138":{"position":[[2700,5],[2967,5]]},"1340":{"position":[[357,7]]},"1406":{"position":[[152,5],[899,7],[946,7],[1280,7]]},"1408":{"position":[[53,7],[132,7],[182,7]]},"1410":{"position":[[990,5]]},"1413":{"position":[[121,33],[160,5],[331,5],[1210,5],[1235,5],[1314,7]]},"1415":{"position":[[6,5]]},"1417":{"position":[[62,5],[527,7]]},"1421":{"position":[[357,7]]},"1452":{"position":[[253,5]]},"1479":{"position":[[152,5],[899,7],[946,7],[1280,7]]},"1481":{"position":[[53,7],[132,7],[182,7]]},"1483":{"position":[[990,5]]},"1486":{"position":[[121,33],[160,5],[331,5],[1210,5],[1235,5],[1314,7]]},"1488":{"position":[[6,5]]},"1490":{"position":[[186,5]]},"1492":{"position":[[62,5],[527,7]]}}}],["scan",{"_index":2355,"t":{"1406":{"position":[[238,4]]},"1479":{"position":[[238,4]]}}}],["scanf(\"%d",{"_index":59,"t":{"824":{"position":[[106,10],[321,10]]}}}],["schedul",{"_index":1953,"t":{"1285":{"position":[[585,8],[606,8],[838,18]]},"1370":{"position":[[585,8],[606,8],[838,18]]}}}],["score",{"_index":496,"t":{"948":{"position":[[1583,6]]},"1024":{"position":[[38,6],[917,6]]},"1059":{"position":[[612,5],[865,5],[933,34],[2097,7]]},"1134":{"position":[[330,7]]},"1138":{"position":[[1680,6],[1687,6],[1742,7],[1850,5],[1946,6],[2317,7],[4051,5],[4057,6],[4112,7],[4146,5],[4242,6],[4476,7]]},"1185":{"position":[[612,5],[865,5],[933,34],[2097,7]]},"1320":{"position":[[1583,6]]},"1324":{"position":[[1240,6]]},"1406":{"position":[[651,5]]},"1479":{"position":[[651,5]]}}}],["score_softmax",{"_index":1358,"t":{"1138":{"position":[[1856,13],[1978,14],[2170,13],[4152,13],[4274,14],[4329,13]]}}}],["score_softmax.t",{"_index":1372,"t":{"1138":{"position":[[2273,18],[4432,18]]}}}],["scores.masked_fill(mask",{"_index":497,"t":{"948":{"position":[[1592,23]]},"1320":{"position":[[1592,23]]}}}],["scores.softmax(dim",{"_index":500,"t":{"948":{"position":[[1637,19]]},"1320":{"position":[[1637,19]]}}}],["scores:\\n",{"_index":1356,"t":{"1138":{"position":[[1730,11],[4100,11]]}}}],["score。分数越小，代表文字和图像更align",{"_index":771,"t":{"1024":{"position":[[1152,25]]}}}],["score中的clip指的就是openai的clip（contrast",{"_index":765,"t":{"1024":{"position":[[929,38]]}}}],["score时，使用每一个query查询对应的key，即query0query^{0}query0只与其他每一个输入向量的key0key^{0}key0做dot",{"_index":1412,"t":{"1140":{"position":[[307,79]]}}}],["score的计算方式是将用于生成图像的文字prompt输入至clip的text",{"_index":768,"t":{"1024":{"position":[[1010,39]]}}}],["score（i",{"_index":945,"t":{"1059":{"position":[[77,9]]},"1185":{"position":[[77,9]]}}}],["score（sc",{"_index":985,"t":{"1059":{"position":[[2017,10]]},"1185":{"position":[[2017,10]]}}}],["score（sifid",{"_index":976,"t":{"1059":{"position":[[1748,12]]},"1185":{"position":[[1748,12]]}}}],["sde",{"_index":2542,"t":{"1454":{"position":[[368,3],[873,4]]}}}],["sdedit",{"_index":2532,"t":{"1454":{"position":[[113,7],[271,6],[572,7]]}}}],["search",{"_index":2797,"t":{"1514":{"position":[[249,6]]}}}],["segment",{"_index":1272,"t":{"1122":{"position":[[328,7]]}}}],["segment_len",{"_index":1269,"t":{"1122":{"position":[[295,12]]}}}],["select",{"_index":346,"t":{"881":{"position":[[0,23],[24,65]]},"1122":{"position":[[47,6]]},"1138":{"position":[[1900,6],[4196,6]]}}}],["self",{"_index":428,"t":{"936":{"position":[[62,4]]},"940":{"position":[[795,4],[918,4],[973,4],[1057,53],[1368,4]]},"1126":{"position":[[29,4],[44,10]]},"1134":{"position":[[2,4],[121,4],[192,8],[338,4]]},"1138":{"position":[[187,4],[1665,4],[2484,14],[3219,4],[4036,4]]},"1140":{"position":[[36,4]]},"1142":{"position":[[0,4]]},"1144":{"position":[[0,11],[127,4]]},"1308":{"position":[[62,4]]},"1312":{"position":[[795,4],[918,4],[973,4],[1057,53],[1368,4]]},"1410":{"position":[[1559,4],[1653,4],[1742,4]]},"1483":{"position":[[1559,4],[1653,4],[1742,4]]}}}],["self).__init__",{"_index":1051,"t":{"1067":{"position":[[346,16]]},"1193":{"position":[[346,16]]},"1270":{"position":[[347,16],[477,16]]}}}],["self.auto_layer_it",{"_index":919,"t":{"1057":{"position":[[199,21]]},"1183":{"position":[[199,21]]}}}],["self.determine_opt_lay",{"_index":922,"t":{"1057":{"position":[[295,27]]},"1183":{"position":[[295,27]]}}}],["self.final_linear",{"_index":1072,"t":{"1067":{"position":[[929,17]]},"1193":{"position":[[929,17]]}}}],["self.generator_trainable.freeze_lay",{"_index":925,"t":{"1057":{"position":[[409,40]]},"1183":{"position":[[409,40]]}}}],["self.generator_trainable.unfreeze_lay",{"_index":920,"t":{"1057":{"position":[[226,42]]},"1183":{"position":[[226,42]]}}}],["self.generator_trainable.unfreeze_layers(train_lay",{"_index":926,"t":{"1057":{"position":[[450,54]]},"1183":{"position":[[450,54]]}}}],["self.linear",{"_index":1067,"t":{"1067":{"position":[[807,11]]},"1193":{"position":[[807,11]]}}}],["self.map",{"_index":1075,"t":{"1067":{"position":[[1058,12]]},"1193":{"position":[[1058,12]]}}}],["self.n_dim",{"_index":1053,"t":{"1067":{"position":[[380,10]]},"1193":{"position":[[380,10]]}}}],["self.net",{"_index":1748,"t":{"1270":{"position":[[494,8]]}}}],["self.net(x",{"_index":1765,"t":{"1270":{"position":[[884,11]]}}}],["self.opt",{"_index":1052,"t":{"1067":{"position":[[363,9]]},"1193":{"position":[[363,9]]}}}],["self.styl",{"_index":908,"t":{"1055":{"position":[[728,10]]},"1181":{"position":[[728,10]]}}}],["self.train",{"_index":918,"t":{"1057":{"position":[[181,13]]},"1183":{"position":[[181,13]]}}}],["self.transformer_encod",{"_index":1061,"t":{"1067":{"position":[[604,24]]},"1193":{"position":[[604,24]]}}}],["semant",{"_index":2560,"t":{"1454":{"position":[[1190,8]]}}}],["sentenc",{"_index":450,"t":{"940":{"position":[[592,10]]},"1081":{"position":[[433,8]]},"1312":{"position":[[592,10]]}}}],["sentencepiec",{"_index":2693,"t":{"1454":{"position":[[8794,24]]}}}],["sentence），都是表示生成的开始。end符号又叫eos符号（end",{"_index":449,"t":{"940":{"position":[[552,36]]},"1312":{"position":[[552,36]]}}}],["sentiment",{"_index":1151,"t":{"1083":{"position":[[737,9],[916,9]]}}}],["seq2seq",{"_index":402,"t":{"927":{"position":[[103,9]]},"1299":{"position":[[103,9]]}}}],["seq_len",{"_index":2522,"t":{"1447":{"position":[[497,7]]}}}],["sequenc",{"_index":401,"t":{"927":{"position":[[94,8]]},"1279":{"position":[[918,8]]},"1299":{"position":[[94,8]]},"1364":{"position":[[918,8]]},"1410":{"position":[[1426,8]]},"1483":{"position":[[1426,8]]}}}],["sequenti",{"_index":1986,"t":{"1324":{"position":[[50,10],[485,10]]}}}],["seri",{"_index":2128,"t":{"1340":{"position":[[695,6]]},"1421":{"position":[[695,6]]}}}],["serv",{"_index":2144,"t":{"1340":{"position":[[1154,7]]},"1342":{"position":[[743,7]]},"1421":{"position":[[1154,7]]},"1423":{"position":[[742,7]]}}}],["set",{"_index":1327,"t":{"1138":{"position":[[634,8]]},"1228":{"position":[[87,4],[153,4],[245,4]]},"1230":{"position":[[6,4],[65,4],[100,4]]}}}],["set_current_attr(attrib(black",{"_index":290,"t":{"844":{"position":[[2028,30],[2106,30]]}}}],["sfid",{"_index":982,"t":{"1059":{"position":[[1847,7]]},"1185":{"position":[[1847,7]]}}}],["sfid（singl",{"_index":1016,"t":{"1059":{"position":[[2420,11]]},"1185":{"position":[[2420,11]]}}}],["sg2_model.pi",{"_index":902,"t":{"1055":{"position":[[541,24]]},"1181":{"position":[[541,24]]}}}],["sg2gener",{"_index":927,"t":{"1057":{"position":[[535,49]]},"1183":{"position":[[535,49]]}}}],["shape",{"_index":1320,"t":{"1138":{"position":[[318,5],[3365,5]]}}}],["shapeoutput=shapeinput−sizekernel+2∗paddingstride+1(1)shape_{output",{"_index":1651,"t":{"1235":{"position":[[202,68]]}}}],["shapeoutput=shapeinput−sizekernel+2∗paddingstride+1(5)shape_{output",{"_index":1671,"t":{"1242":{"position":[[272,68]]}}}],["shape为[5,5,3][5",{"_index":1439,"t":{"1156":{"position":[[4,19]]}}}],["shape为[5,5,4][5",{"_index":1441,"t":{"1156":{"position":[[40,21]]}}}],["shape应为[dim0",{"_index":1869,"t":{"1275":{"position":[[716,25]]}}}],["sharp",{"_index":970,"t":{"1059":{"position":[[1394,5]]},"1185":{"position":[[1394,5]]}}}],["sharpen",{"_index":512,"t":{"950":{"position":[[160,7]]}}}],["shechtman",{"_index":2269,"t":{"1354":{"position":[[572,10]]}}}],["shin",{"_index":2255,"t":{"1354":{"position":[[400,5]]}}}],["shot",{"_index":817,"t":{"1038":{"position":[[36,4]]},"1164":{"position":[[36,4]]},"1349":{"position":[[79,4]]},"1354":{"position":[[89,5],[99,5],[825,5]]},"1406":{"position":[[1089,4],[1302,4]]},"1454":{"position":[[7000,4]]},"1479":{"position":[[1089,4],[1302,4]]}}}],["shot任务，一般是通过有限的目标域训练集资料fin",{"_index":2241,"t":{"1354":{"position":[[111,27]]}}}],["shot和zero",{"_index":2239,"t":{"1354":{"position":[[79,9]]}}}],["shot的背景下，如何实现prompt与源域图像特征的对齐以及prompt",{"_index":2299,"t":{"1359":{"position":[[159,63]]}}}],["showcas",{"_index":2381,"t":{"1406":{"position":[[1074,9]]},"1479":{"position":[[1074,9]]}}}],["shown",{"_index":1274,"t":{"1122":{"position":[[391,5]]}}}],["shrine",{"_index":508,"t":{"950":{"position":[[104,6]]}}}],["sifid",{"_index":981,"t":{"1059":{"position":[[1821,21],[1958,5]]},"1185":{"position":[[1821,21],[1958,5]]}}}],["sigma",{"_index":1467,"t":{"1201":{"position":[[117,6],[136,7]]},"1347":{"position":[[847,14]]},"1428":{"position":[[847,14]]}}}],["sigma_t",{"_index":2652,"t":{"1454":{"position":[[6365,8],[6543,8],[8318,8]]}}}],["sigma_{t",{"_index":2689,"t":{"1454":{"position":[[8426,11]]}}}],["sigmoid",{"_index":1470,"t":{"1201":{"position":[[197,56]]},"1247":{"position":[[36,35]]},"1270":{"position":[[544,13]]},"1347":{"position":[[597,7]]},"1428":{"position":[[597,7]]}}}],["sigmoid函数输出的结果可将其视为probability，而后根据设定的置信度阈值来判断该特征向量对应的标签是1还是0",{"_index":1675,"t":{"1245":{"position":[[61,78]]}}}],["sigmoid改成了relu",{"_index":1607,"t":{"1223":{"position":[[73,26]]}}}],["signal",{"_index":2112,"t":{"1340":{"position":[[288,7]]},"1421":{"position":[[288,7]]}}}],["significantli",{"_index":744,"t":{"1022":{"position":[[772,13]]},"1324":{"position":[[1454,13]]},"1406":{"position":[[541,13]]},"1479":{"position":[[541,13]]}}}],["sim",{"_index":1971,"t":{"1287":{"position":[[390,4],[461,4]]},"1372":{"position":[[390,4],[461,4]]},"1454":{"position":[[2086,4],[4844,4],[6104,4],[8003,4]]}}}],["similar",{"_index":1141,"t":{"1083":{"position":[[530,10]]},"1406":{"position":[[959,7]]},"1479":{"position":[[959,7]]}}}],["similarity（id",{"_index":987,"t":{"1059":{"position":[[2049,14]]},"1185":{"position":[[2049,14]]}}}],["simpl",{"_index":1397,"t":{"1138":{"position":[[3203,6]]},"1354":{"position":[[434,6]]},"1406":{"position":[[273,7]]},"1452":{"position":[[259,6]]},"1479":{"position":[[273,7]]}}}],["simul",{"_index":2555,"t":{"1454":{"position":[[850,10]]}}}],["sin",{"_index":511,"t":{"950":{"position":[[142,5]]},"1347":{"position":[[1257,4]]},"1428":{"position":[[1258,4]]}}}],["sine函数进行编码，对偶数维度使用cosin",{"_index":407,"t":{"934":{"position":[[0,56]]},"1306":{"position":[[0,56]]}}}],["singl",{"_index":975,"t":{"1059":{"position":[[1717,6]]},"1185":{"position":[[1717,6]]}}}],["situation)中。在logo文件夹中挑选想要的logo，在desktop中的icon",{"_index":2778,"t":{"1512":{"position":[[151,51]]}}}],["size",{"_index":736,"t":{"1022":{"position":[[610,4],[644,5],[668,4],[761,4],[816,5]]},"1240":{"position":[[796,4]]},"1454":{"position":[[5521,4]]}}}],["size_{kernel}+2*padding}{stride}+1",{"_index":1653,"t":{"1235":{"position":[[293,34]]},"1242":{"position":[[363,34]]}}}],["sks_ksk",{"_index":580,"t":{"973":{"position":[[94,24]]}}}],["slr(1",{"_index":355,"t":{"893":{"position":[[260,9]]},"898":{"position":[[146,7],[187,20]]}}}],["small",{"_index":1330,"t":{"1138":{"position":[[669,5],[2942,6]]}}}],["snr：参考图像像素值的平方均值与均方误差的比值的对数的10",{"_index":543,"t":{"957":{"position":[[174,42]]}}}],["sobel",{"_index":646,"t":{"999":{"position":[[67,8]]}}}],["softmax",{"_index":1357,"t":{"1138":{"position":[[1799,24],[2914,7]]},"1148":{"position":[[452,7],[480,7]]}}}],["softwar",{"_index":252,"t":{"844":{"position":[[900,9]]}}}],["solid",{"_index":2378,"t":{"1406":{"position":[[1046,5]]},"1479":{"position":[[1046,5]]}}}],["solut",{"_index":1286,"t":{"1124":{"position":[[28,8]]}}}],["someth",{"_index":47,"t":{"816":{"position":[[85,12]]}}}],["sonder",{"_index":27,"t":{"808":{"position":[[28,12]]}}}],["sophisticated），是长度不定的向量序列（sequence）时，cnn",{"_index":1296,"t":{"1129":{"position":[[799,88]]}}}],["sora",{"_index":2752,"t":{"1490":{"position":[[241,4]]}}}],["sort(list.begin",{"_index":45,"t":{"816":{"position":[[39,18]]}}}],["sort(s.begin",{"_index":37,"t":{"814":{"position":[[103,15]]}}}],["sot",{"_index":856,"t":{"1051":{"position":[[132,4],[407,4]]},"1177":{"position":[[132,4],[407,4]]}}}],["sota",{"_index":2160,"t":{"1342":{"position":[[594,4]]},"1347":{"position":[[52,4]]},"1423":{"position":[[594,4]]},"1428":{"position":[[52,4]]},"1433":{"position":[[1537,4]]}}}],["sota的图像生成模型的共同点，并初步了解了diffus",{"_index":652,"t":{"1003":{"position":[[4,53]]}}}],["sourc",{"_index":153,"t":{"840":{"position":[[1221,6]]},"1071":{"position":[[606,7]]},"1197":{"position":[[606,7]]},"1340":{"position":[[1318,6]]},"1421":{"position":[[1318,6]]},"1502":{"position":[[89,6]]}}}],["source_embed",{"_index":875,"t":{"1051":{"position":[[853,16]]},"1177":{"position":[[853,16]]}}}],["source_prompt",{"_index":845,"t":{"1049":{"position":[[56,14],[183,14]]},"1175":{"position":[[56,14],[183,14]]}}}],["source_prompts]).to(devic",{"_index":854,"t":{"1051":{"position":[[94,27]]},"1177":{"position":[[94,27]]}}}],["source_tokenized_prompt",{"_index":851,"t":{"1051":{"position":[[30,24],[757,24]]},"1177":{"position":[[30,24],[757,24]]}}}],["space",{"_index":326,"t":{"861":{"position":[[176,6]]},"913":{"position":[[34,72]]},"919":{"position":[[158,65]]},"1055":{"position":[[266,5],[401,5]]},"1181":{"position":[[266,5],[401,5]]},"1201":{"position":[[124,6]]},"1208":{"position":[[734,6],[744,6],[768,6],[778,6],[870,6],[880,6],[1023,6],[1032,6],[1211,6],[1249,6]]},"1214":{"position":[[36,6],[51,6],[76,6]]},"1279":{"position":[[168,5],[453,6]]},"1324":{"position":[[379,6],[641,5]]},"1340":{"position":[[395,6]]},"1364":{"position":[[168,5],[453,6]]},"1421":{"position":[[395,6]]},"1454":{"position":[[7430,7]]}}}],["spatial",{"_index":2229,"t":{"1349":{"position":[[122,7]]}}}],["spawn",{"_index":274,"t":{"844":{"position":[[1809,6]]}}}],["speaker",{"_index":1247,"t":{"1120":{"position":[[13,7]]},"1122":{"position":[[58,8]]}}}],["specfic",{"_index":1082,"t":{"1069":{"position":[[372,7]]},"1195":{"position":[[372,7]]}}}],["specif",{"_index":1080,"t":{"1069":{"position":[[288,8],[519,8],[850,8]]},"1071":{"position":[[81,8],[137,8]]},"1195":{"position":[[288,8],[519,8],[850,8]]},"1197":{"position":[[81,8],[137,8]]},"1351":{"position":[[11,8]]},"1454":{"position":[[7701,8],[7803,8]]},"1512":{"position":[[142,8]]}}}],["specifi",{"_index":256,"t":{"844":{"position":[[969,9]]}}}],["spectrogram",{"_index":1260,"t":{"1122":{"position":[[126,13],[484,11]]}}}],["speed",{"_index":1923,"t":{"1279":{"position":[[899,5]]},"1340":{"position":[[1201,5]]},"1364":{"position":[[899,5]]},"1406":{"position":[[708,6],[858,6]]},"1421":{"position":[[1201,5]]},"1479":{"position":[[708,6],[858,6]]}}}],["speedup",{"_index":2148,"t":{"1340":{"position":[[1258,8]]},"1421":{"position":[[1258,8]]}}}],["split",{"_index":1617,"t":{"1226":{"position":[[156,5]]}}}],["sqrt[2]{\\sum{[i",{"_index":1492,"t":{"1206":{"position":[[191,16]]}}}],["squar",{"_index":1384,"t":{"1138":{"position":[[2736,6]]}}}],["squeez",{"_index":2918,"t":{"1574":{"position":[[0,7]]}}}],["squeeze挤压操作就是将[b,c,h,w][b",{"_index":2920,"t":{"1574":{"position":[[23,26]]}}}],["ssim",{"_index":545,"t":{"957":{"position":[[299,22]]}}}],["sss",{"_index":2218,"t":{"1347":{"position":[[2265,3]]},"1428":{"position":[[2266,3]]}}}],["sss个时间步，mask",{"_index":463,"t":{"940":{"position":[[1339,17]]},"1312":{"position":[[1339,17]]}}}],["sst",{"_index":1149,"t":{"1083":{"position":[[722,3]]}}}],["ss×s个grid",{"_index":2870,"t":{"1556":{"position":[[174,9]]}}}],["st",{"_index":1139,"t":{"1083":{"position":[[507,3]]}}}],["stabl",{"_index":716,"t":{"1020":{"position":[[0,6]]},"1022":{"position":[[0,10],[303,9]]},"1069":{"position":[[1191,6]]},"1195":{"position":[[1191,6]]},"1322":{"position":[[43,11],[65,22]]},"1452":{"position":[[57,6]]},"1454":{"position":[[3986,6],[4232,6],[4353,6],[5278,6],[5405,6],[8722,6]]}}}],["stablediffus",{"_index":2596,"t":{"1454":{"position":[[3544,15]]}}}],["stack",{"_index":159,"t":{"840":{"position":[[1325,5]]},"1275":{"position":[[15,34]]}}}],["stack和concat",{"_index":1845,"t":{"1275":{"position":[[85,14]]}}}],["stage",{"_index":910,"t":{"1055":{"position":[[946,10]]},"1057":{"position":[[1004,5]]},"1069":{"position":[[330,5],[484,5]]},"1181":{"position":[[946,10]]},"1183":{"position":[[1004,5]]},"1195":{"position":[[330,5],[484,5]]},"1340":{"position":[[974,5]]},"1421":{"position":[[974,5]]}}}],["stand",{"_index":2835,"t":{"1530":{"position":[[742,6]]},"1541":{"position":[[742,6]]}}}],["standard",{"_index":1916,"t":{"1279":{"position":[[718,8]]},"1364":{"position":[[718,8]]},"1406":{"position":[[222,8]]},"1479":{"position":[[222,8]]}}}],["star",{"_index":50,"t":{"820":{"position":[[16,4]]},"908":{"position":[[16,4]]},"1463":{"position":[[16,4]]},"1496":{"position":[[16,4]]},"1587":{"position":[[16,4]]}}}],["starganv2",{"_index":1038,"t":{"1064":{"position":[[58,9]]},"1190":{"position":[[58,9]]}}}],["start_kernel_thread(&project0,0,priority_normal,fals",{"_index":230,"t":{"844":{"position":[[574,55],[2256,55]]}}}],["startoftext",{"_index":870,"t":{"1051":{"position":[[625,11]]},"1177":{"position":[[625,11]]}}}],["state",{"_index":1990,"t":{"1324":{"position":[[131,5],[1223,5]]},"1340":{"position":[[308,5]]},"1421":{"position":[[308,5]]}}}],["status=insert",{"_index":179,"t":{"840":{"position":[[1698,15],[1741,15]]},"846":{"position":[[642,15],[685,15]]}}}],["std",{"_index":6,"t":{"803":{"position":[[41,4]]},"814":{"position":[[41,4]]},"824":{"position":[[41,4]]},"1527":{"position":[[82,4]]}}}],["stl",{"_index":44,"t":{"816":{"position":[[6,12]]}}}],["stochast",{"_index":2533,"t":{"1454":{"position":[[161,10]]}}}],["str(devic",{"_index":1835,"t":{"1270":{"position":[[2612,15]]}}}],["stride=1padding=1,stride=1",{"_index":1445,"t":{"1156":{"position":[[138,26]]}}}],["stride=2",{"_index":1756,"t":{"1270":{"position":[[645,10],[733,10]]}}}],["string",{"_index":33,"t":{"814":{"position":[[59,6]]},"1071":{"position":[[827,6]]},"1197":{"position":[[827,6]]},"1525":{"position":[[0,6],[73,6]]},"1527":{"position":[[87,6],[160,6],[779,6]]}}}],["string}.pt",{"_index":1278,"t":{"1122":{"position":[[423,10],[588,10]]}}}],["stroke",{"_index":2546,"t":{"1454":{"position":[[425,12],[559,7],[715,6],[755,6],[925,6],[1013,6],[1044,6],[1106,6]]}}}],["strong",{"_index":1921,"t":{"1279":{"position":[[865,6]]},"1364":{"position":[[865,6]]}}}],["struct",{"_index":227,"t":{"844":{"position":[[535,6],[2217,6]]}}}],["structur",{"_index":983,"t":{"1059":{"position":[[1994,10]]},"1185":{"position":[[1994,10]]},"1349":{"position":[[130,10]]},"1459":{"position":[[12,9]]}}}],["style",{"_index":897,"t":{"1055":{"position":[[392,8]]},"1057":{"position":[[838,6]]},"1181":{"position":[[392,8]]},"1183":{"position":[[838,6]]},"1406":{"position":[[438,5]]},"1433":{"position":[[836,5]]},"1479":{"position":[[438,5]]}}}],["style_dim",{"_index":905,"t":{"1055":{"position":[[662,10],[673,10]]},"1181":{"position":[[662,10],[673,10]]}}}],["stylegan",{"_index":896,"t":{"1055":{"position":[[191,8],[314,8],[443,8],[781,8],[899,17]]},"1057":{"position":[[706,8],[715,21]]},"1059":{"position":[[26,11],[1036,8],[1576,8]]},"1062":{"position":[[335,8]]},"1064":{"position":[[85,8],[257,8]]},"1181":{"position":[[191,8],[314,8],[443,8],[781,8],[899,17]]},"1183":{"position":[[706,8],[715,21]]},"1185":{"position":[[26,11],[1036,8],[1576,8]]},"1188":{"position":[[335,8]]},"1190":{"position":[[85,8],[257,8]]}}}],["stylegan2",{"_index":1020,"t":{"1062":{"position":[[59,9]]},"1188":{"position":[[59,9]]}}}],["style）类的prompt",{"_index":2291,"t":{"1356":{"position":[[52,28]]}}}],["subject",{"_index":1569,"t":{"1214":{"position":[[11,7],[131,7]]},"1454":{"position":[[5862,7],[5914,23],[6725,7],[6778,7],[6895,8],[7088,7],[7222,7]]}}}],["subsequ",{"_index":2540,"t":{"1454":{"position":[[314,12]]}}}],["subsystem",{"_index":271,"t":{"844":{"position":[[1771,11]]}}}],["such",{"_index":133,"t":{"840":{"position":[[873,4]]},"1324":{"position":[[743,4],[1071,4]]},"1340":{"position":[[879,4]]},"1421":{"position":[[879,4]]},"1454":{"position":[[6831,4]]}}}],["sudo",{"_index":2772,"t":{"1509":{"position":[[59,4]]}}}],["suggest",{"_index":2383,"t":{"1406":{"position":[[1207,7]]},"1479":{"position":[[1207,7]]}}}],["sum",{"_index":23,"t":{"805":{"position":[[161,3],[207,3],[213,3]]},"940":{"position":[[1724,37]]},"1312":{"position":[[1724,37]]}}}],["sum_i^n{p_i}\\space{log_2(p_i",{"_index":1508,"t":{"1208":{"position":[[318,31]]}}}],["sum_i^n{p_i}\\space{log_2({\\frac{1}{p_i",{"_index":1509,"t":{"1208":{"position":[[356,43]]}}}],["sum_{j=1}^i",{"_index":1214,"t":{"1103":{"position":[[574,12]]}}}],["sum{[{i",{"_index":1487,"t":{"1206":{"position":[[91,8]]}}}],["sum{p_i\\spac",{"_index":1528,"t":{"1208":{"position":[[892,14]]}}}],["sum{p_i}\\space{log_2({q_i",{"_index":1541,"t":{"1208":{"position":[[1267,29]]}}}],["sum产生加入噪声后的图像。通常来说，αˉ1\\bar{\\alpha}_1αˉ1​至αˉt\\bar{\\alpha}_tαˉt​是递减的，当在第2步中sample到的ttt越大，则原始图像x0\\mathbf{x}_0x0",{"_index":687,"t":{"1007":{"position":[[564,119]]}}}],["super",{"_index":2040,"t":{"1324":{"position":[[1430,5]]}}}],["super(lenet5",{"_index":1747,"t":{"1270":{"position":[[463,13]]}}}],["super(lenetreshap",{"_index":1742,"t":{"1270":{"position":[[327,19]]}}}],["super(transformermapperv2",{"_index":1050,"t":{"1067":{"position":[[319,26]]},"1193":{"position":[[319,26]]}}}],["support",{"_index":2624,"t":{"1454":{"position":[[5121,7]]}}}],["sur",{"_index":2776,"t":{"1512":{"position":[[105,3]]}}}],["sure",{"_index":1388,"t":{"1138":{"position":[[2790,4]]},"1469":{"position":[[89,4]]}}}],["surpass",{"_index":2360,"t":{"1406":{"position":[[454,7]]},"1479":{"position":[[454,7]]}}}],["surpris",{"_index":1497,"t":{"1208":{"position":[[80,9]]},"1410":{"position":[[1484,10]]},"1483":{"position":[[1484,10]]}}}],["sutskever和geoffrey",{"_index":1602,"t":{"1219":{"position":[[36,18]]}}}],["svd",{"_index":2841,"t":{"1536":{"position":[[845,61]]},"1539":{"position":[[985,61]]}}}],["swiglu",{"_index":2174,"t":{"1347":{"position":[[504,6],[511,11],[904,6]]},"1428":{"position":[[504,6],[511,11],[904,6]]}}}],["swiglu(x)=σ(xw1+b1)⊙(xw2+b2)\\mathrm{swiglu}(x)=\\sigma(xw_1+b_1)\\odot(xw_2+b_2)swiglu(x)=σ(xw1​+b1​)⊙(xw2​+b2",{"_index":2180,"t":{"1347":{"position":[[671,110]]},"1428":{"position":[[671,110]]}}}],["swish",{"_index":2178,"t":{"1347":{"position":[[551,5],[630,5]]},"1428":{"position":[[551,5],[630,5]]}}}],["synthes",{"_index":2547,"t":{"1454":{"position":[[534,12]]}}}],["synthesi",{"_index":1992,"t":{"1324":{"position":[[148,9],[1122,9],[1296,9],[1415,10]]},"1433":{"position":[[2065,10]]},"1454":{"position":[[134,9]]},"1457":{"position":[[241,9]]},"1459":{"position":[[47,9]]}}}],["system",{"_index":132,"t":{"840":{"position":[[864,8],[1501,7]]},"846":{"position":[[445,7]]}}}],["s×ss",{"_index":2869,"t":{"1556":{"position":[[152,14]]}}}],["t",{"_index":1942,"t":{"1285":{"position":[[300,2],[723,2]]},"1287":{"position":[[208,2]]},"1370":{"position":[[300,2],[723,2]]},"1372":{"position":[[208,2]]},"1454":{"position":[[1701,2],[1991,2],[4906,2],[6486,2],[8263,2]]}}}],["t)^2}\\tag{1}de​(p,q)=(x−s)2+(y−t)2​(1",{"_index":532,"t":{"955":{"position":[[227,38]]}}}],["t5",{"_index":2676,"t":{"1454":{"position":[[7462,2],[7584,2],[8819,2]]},"1490":{"position":[[129,2]]}}}],["t<1t",{"_index":1180,"t":{"1099":{"position":[[436,4]]}}}],["t=1t=1t=1时，得到x0\\mathbf{x}_0x0",{"_index":704,"t":{"1009":{"position":[[762,42]]}}}],["t=μ+ν2(25)t=\\frac{\\mu",{"_index":642,"t":{"995":{"position":[[110,21]]}}}],["t>1t",{"_index":1178,"t":{"1099":{"position":[[387,4]]}}}],["t\\bar{\\alpha}_tαˉt",{"_index":1952,"t":{"1285":{"position":[[551,21]]},"1370":{"position":[[551,21]]}}}],["t\\sigma_tσt",{"_index":1969,"t":{"1287":{"position":[[346,13]]},"1372":{"position":[[346,13]]}}}],["t\\vert",{"_index":535,"t":{"955":{"position":[[325,6]]}}}],["t\\vert)\\tag{3}d8​(p,q)=max(∣x−s∣,∣y−t∣)(3",{"_index":539,"t":{"955":{"position":[[431,42]]}}}],["t]t∼uniform[1,t]，xt\\mathbf{x}_txt",{"_index":2578,"t":{"1454":{"position":[[2117,34]]}}}],["tag{10",{"_index":1542,"t":{"1208":{"position":[[1297,8]]}}}],["tag{1}",{"_index":315,"t":{"857":{"position":[[145,12]]}}}],["tag{1}3×3×3×4=108(1",{"_index":1450,"t":{"1158":{"position":[[168,21]]}}}],["tag{1}acc=len(y)∑i(predi​==yi​)​(1",{"_index":1681,"t":{"1251":{"position":[[196,36]]}}}],["tag{1}attention(q,k,v)=softmax(dk​​qkt​)v(1",{"_index":1396,"t":{"1138":{"position":[[3149,45]]}}}],["tag{1}l(y,z)=max(0,−y∗z)(1",{"_index":1719,"t":{"1263":{"position":[[406,28]]}}}],["tag{1}min",{"_index":1577,"t":{"1214":{"position":[[113,10]]}}}],["tag{1}pe(pos,2i)​=sin(100002i/dmodel​pos​)(1)pe(pos,2i+1)=cos(pos100002i/dmodel)(2)pe_{(pos,2i+1)}=cos(\\frac{pos}{10000^{2i/d_{\\mathrm{model",{"_index":410,"t":{"934":{"position":[[309,146]]},"1306":{"position":[[309,146]]}}}],["tag{1}shapeoutput​=strideshapeinput​−sizekernel​+2∗padding​+1(1",{"_index":1654,"t":{"1235":{"position":[[328,65]]}}}],["tag{1}x1[0,:,:]+x2[0,:,:](1",{"_index":2488,"t":{"1441":{"position":[[894,29]]}}}],["tag{1}yi,j​=h,w∑​wi,j,h,w​∗xh,w​(1",{"_index":1659,"t":{"1240":{"position":[[142,36]]}}}],["tag{1}σ(x)=1+e−x1​(1)dσdx",{"_index":1463,"t":{"1201":{"position":[[47,28]]}}}],["tag{2}",{"_index":320,"t":{"859":{"position":[[59,12]]}}}],["tag{2}3×3××3=27(2",{"_index":1454,"t":{"1160":{"position":[[100,19]]}}}],["tag{2}d4​(p,q)=∣x−s∣+∣y−t∣(2",{"_index":536,"t":{"955":{"position":[[332,30]]}}}],["tag{2}dxd",{"_index":1468,"t":{"1201":{"position":[[144,14]]}}}],["tag{2}l(w,b)+2λ​∥w∥12​(2",{"_index":1584,"t":{"1216":{"position":[[110,26]]}}}],["tag{2}pe(pos,2i+1)​=cos(100002i/dmodel​pos​)(2",{"_index":411,"t":{"934":{"position":[[456,48]]},"1306":{"position":[[456,48]]}}}],["tag{2}x1[1,:,:]+x2[1,:,:](2",{"_index":2491,"t":{"1441":{"position":[[972,29]]}}}],["tag{2}yi,j​=h,w∑​wi,j,h,w​∗xh,w​=a,b∑​vi,j,a,b​∗xi+a,j+b​(2",{"_index":1662,"t":{"1240":{"position":[[364,61]]}}}],["tag{3}1×1×3×4=12(3",{"_index":1460,"t":{"1162":{"position":[[112,20]]}}}],["tag{3}a",{"_index":327,"t":{"861":{"position":[[210,11]]}}}],["tag{3}f(x)={0x​x<0x≥0​(3)df(x)dx={0x<01x≥0(4",{"_index":1480,"t":{"1203":{"position":[[78,46]]}}}],["tag{3}pe(pos+k,2i)=pe(pos,2i)×pe(k,2i+1)+pe(pos,2i+1)×pe(k,2i)pe(pos+k,2i+1)=pe(pos,2i+1)×pe(k,2i+1)−pe(pos,2i)×pe(k,2i)​(3",{"_index":419,"t":{"934":{"position":[[927,125]]},"1306":{"position":[[927,125]]}}}],["tag{3}x1[2,:,:]+x2[2,:,:](3",{"_index":2494,"t":{"1441":{"position":[[1050,29]]}}}],["tag{3}yi,j​=a,b∑​vi,j,a,b​∗xi+a,j+b​=a,b∑​va,b​∗xi+a,j+b​(3",{"_index":1665,"t":{"1240":{"position":[[636,61]]}}}],["tag{3}∂w∂​(l(w,b)+2λ​∥w∥12​)=∂w∂l(w,b)​+λw(3)wt+1=(1−ηλ)wt+η∂l(wt,bt)∂wt(4)w_{t+1}=(1",{"_index":1591,"t":{"1216":{"position":[[347,86]]}}}],["tag{4}dxdf(x)​={01​x<0x≥0​(4",{"_index":1483,"t":{"1203":{"position":[[214,30]]}}}],["tag{4}inputembedding(pos,i)=wordembedding(pos,i)+positionencoding(pos,i)(4",{"_index":423,"t":{"934":{"position":[[1233,76]]},"1306":{"position":[[1233,76]]}}}],["tag{4}wt+1​=(1−ηλ)wt​+η∂wt​∂l(wt​,bt​)​(4",{"_index":1597,"t":{"1216":{"position":[[500,43]]}}}],["tag{4}yi,j​=a,b∑​va,b​∗xi+a,j+b​=a=−δ∑δ​b=−δ∑δ​va,b​∗xia​,j+b​(4",{"_index":1670,"t":{"1242":{"position":[[194,66]]}}}],["tag{5}lossmse​=∑[y−f(x)]2(5",{"_index":1489,"t":{"1206":{"position":[[112,29]]}}}],["tag{5}shapeoutput​=strideshapeinput​−sizekernel​+2∗padding​+1(5",{"_index":1672,"t":{"1242":{"position":[[398,65]]}}}],["tag{6}[o1​o2​​]=[α1,1′​α1,2′​​α2,1′​α2,2′​​][v1​v2​​](6",{"_index":490,"t":{"948":{"position":[[732,57]]},"1320":{"position":[[732,57]]}}}],["tag{6}∥y−f(x)∥2​=2∑[y−f(x)]2​(6",{"_index":1493,"t":{"1206":{"position":[[220,33]]}}}],["tag{7",{"_index":1510,"t":{"1208":{"position":[[400,7]]}}}],["tag{7}[o1​o2​​]=[α1,1′​α1,2′​​0α2,2′​​][v1​v2​​](7",{"_index":492,"t":{"948":{"position":[[1026,52]]},"1320":{"position":[[1026,52]]}}}],["take",{"_index":503,"t":{"950":{"position":[[54,4]]},"1134":{"position":[[24,5]]}}}],["target",{"_index":117,"t":{"840":{"position":[[635,6],[828,6],[938,6],[984,6]]},"1351":{"position":[[227,6]]}}}],["target_cc",{"_index":124,"t":{"840":{"position":[[692,9]]}}}],["target_cc_prefix)gcc",{"_index":125,"t":{"840":{"position":[[705,22]]}}}],["target_cc_prefix)ld",{"_index":147,"t":{"840":{"position":[[1063,21]]}}}],["target_class",{"_index":1093,"t":{"1071":{"position":[[634,14]]},"1197":{"position":[[634,14]]}}}],["target_direct",{"_index":940,"t":{"1057":{"position":[[1160,16]]},"1183":{"position":[[1160,16]]}}}],["target_direction：mapp",{"_index":941,"t":{"1057":{"position":[[1186,23]]},"1183":{"position":[[1186,23]]}}}],["target_embed",{"_index":879,"t":{"1051":{"position":[[1005,16]]},"1177":{"position":[[1005,16]]}}}],["target_ld",{"_index":146,"t":{"840":{"position":[[1050,9]]}}}],["target_prompt",{"_index":848,"t":{"1049":{"position":[[115,14]]},"1175":{"position":[[115,14]]}}}],["target_prompts]).to(devic",{"_index":867,"t":{"1051":{"position":[[369,27]]},"1177":{"position":[[369,27]]}}}],["target_tokenized_prompt",{"_index":866,"t":{"1051":{"position":[[305,24],[784,24]]},"1177":{"position":[[305,24],[784,24]]}}}],["task",{"_index":1091,"t":{"1071":{"position":[[585,4]]},"1079":{"position":[[337,11]]},"1083":{"position":[[909,6]]},"1197":{"position":[[585,4]]},"1324":{"position":[[1352,6]]},"1406":{"position":[[1131,5]]},"1454":{"position":[[6755,4]]},"1479":{"position":[[1131,5]]}}}],["teacher",{"_index":477,"t":{"946":{"position":[[146,11]]},"948":{"position":[[429,19]]},"1318":{"position":[[146,11]]},"1320":{"position":[[429,19]]}}}],["techniqu",{"_index":1610,"t":{"1226":{"position":[[29,9]]}}}],["tell",{"_index":510,"t":{"950":{"position":[[130,4]]}}}],["temp",{"_index":2813,"t":{"1525":{"position":[[158,4],[210,4],[253,13],[270,5],[377,4],[516,5]]},"1527":{"position":[[245,4],[297,4],[340,13],[357,5],[464,4],[603,5]]}}}],["templat",{"_index":1078,"t":{"1069":{"position":[[103,25],[632,9],[647,9],[739,18]]},"1195":{"position":[[103,25],[632,9],[647,9],[739,18]]}}}],["tensor",{"_index":2481,"t":{"1441":{"position":[[637,10]]}}}],["tensor(0.2684",{"_index":1559,"t":{"1208":{"position":[[1689,14],[1875,14]]}}}],["tensor(1",{"_index":1885,"t":{"1275":{"position":[[1134,11]]}}}],["tensor(2",{"_index":1887,"t":{"1275":{"position":[[1157,11]]}}}],["tensor(3",{"_index":1889,"t":{"1275":{"position":[[1180,11]]}}}],["tensor(4",{"_index":1886,"t":{"1275":{"position":[[1146,10]]}}}],["tensor(4))8(tensor(2",{"_index":1696,"t":{"1251":{"position":[[475,22]]}}}],["tensor(5",{"_index":1888,"t":{"1275":{"position":[[1169,10]]}}}],["tensor(5))9(tensor(3",{"_index":1697,"t":{"1251":{"position":[[498,22]]}}}],["tensor(6",{"_index":1890,"t":{"1275":{"position":[[1192,10]]}}}],["tensor(6))10'''11a",{"_index":1698,"t":{"1251":{"position":[[521,18]]}}}],["tensor([1",{"_index":1882,"t":{"1275":{"position":[[947,10],[965,10],[1328,11]]}}}],["tensor([3",{"_index":1891,"t":{"1275":{"position":[[1367,11]]}}}],["tensor([4",{"_index":1705,"t":{"1251":{"position":[[686,10]]},"1275":{"position":[[1348,10]]}}}],["tensor([6",{"_index":1707,"t":{"1251":{"position":[[726,10]]},"1275":{"position":[[1387,10]]}}}],["tensor([[9",{"_index":1864,"t":{"1275":{"position":[[558,11]]}}}],["tensor([[[0",{"_index":2477,"t":{"1441":{"position":[[426,14],[541,13]]}}}],["tensorflow框架，可以使用tensorboard",{"_index":1220,"t":{"1107":{"position":[[4,36]]}}}],["tensor的*乘法是对tensor",{"_index":1861,"t":{"1275":{"position":[[454,28]]}}}],["term",{"_index":1304,"t":{"1134":{"position":[[110,6]]}}}],["terminal_proxy.sh",{"_index":2753,"t":{"1500":{"position":[[0,23]]}}}],["test",{"_index":1620,"t":{"1226":{"position":[[224,4],[277,7]]},"1228":{"position":[[82,4],[240,4]]},"1230":{"position":[[95,4],[115,5]]},"1270":{"position":[[1771,5],[2519,4]]}}}],["test_acc",{"_index":1827,"t":{"1270":{"position":[[2367,8],[2453,10]]}}}],["test_acc:.3f",{"_index":1831,"t":{"1270":{"position":[[2528,16]]}}}],["test_it",{"_index":1786,"t":{"1270":{"position":[[1391,10],[2405,10],[2668,9],[2787,10]]}}}],["testdata.json",{"_index":1284,"t":{"1122":{"position":[[557,13]]}}}],["text",{"_index":719,"t":{"1022":{"position":[[78,4],[597,4],[748,4],[837,4]]},"1071":{"position":[[669,4]]},"1089":{"position":[[18,4]]},"1101":{"position":[[413,6],[589,5]]},"1103":{"position":[[698,5]]},"1197":{"position":[[669,4]]},"1203":{"position":[[131,7]]},"1324":{"position":[[1079,4],[1401,4]]},"1340":{"position":[[903,4],[1099,4]]},"1421":{"position":[[903,4],[1099,4]]},"1435":{"position":[[16,4]]},"1452":{"position":[[18,4]]},"1454":{"position":[[43,4],[1138,4],[1452,4],[2223,4],[2416,4],[2659,4],[5827,4],[5938,4],[5978,4],[6023,4],[7849,4]]},"1457":{"position":[[139,4],[172,4]]},"1490":{"position":[[132,4]]},"1514":{"position":[[80,4]]}}}],["text_direct",{"_index":1083,"t":{"1069":{"position":[[616,15]]},"1195":{"position":[[616,15]]}}}],["text_featur",{"_index":942,"t":{"1057":{"position":[[1230,13]]},"1069":{"position":[[243,13]]},"1183":{"position":[[1230,13]]},"1195":{"position":[[243,13]]}}}],["textual",{"_index":1137,"t":{"1083":{"position":[[455,7],[522,7]]}}}],["text{subject",{"_index":1572,"t":{"1214":{"position":[[58,13]]}}}],["theta",{"_index":1576,"t":{"1214":{"position":[[106,6],[192,23]]},"1285":{"position":[[655,8]]},"1370":{"position":[[655,8]]},"1454":{"position":[[2171,14]]}}}],["theta)=\\mathbb{e}_{t",{"_index":2572,"t":{"1454":{"position":[[1896,22]]}}}],["those",{"_index":2374,"t":{"1406":{"position":[[970,5]]},"1479":{"position":[[970,5]]}}}],["thpf",{"_index":627,"t":{"981":{"position":[[522,14]]}}}],["thread",{"_index":229,"t":{"844":{"position":[[556,8],[565,6],[2169,6],[2238,8],[2247,6],[2324,6]]}}}],["through",{"_index":2541,"t":{"1454":{"position":[[356,7]]}}}],["tilde{x}_\\theta",{"_index":2658,"t":{"1454":{"position":[[6503,16]]}}}],["time",{"_index":1416,"t":{"1146":{"position":[[59,6]]},"1148":{"position":[[193,6],[252,6],[359,6]]},"1152":{"position":[[101,6],[141,6],[174,6]]},"1156":{"position":[[94,6]]},"1158":{"position":[[135,6],[144,6],[153,6]]},"1160":{"position":[[70,6],[79,6],[86,6]]},"1162":{"position":[[7,6],[80,6],[89,6],[98,6]]},"1226":{"position":[[243,6],[290,5]]},"1228":{"position":[[187,6]]},"1233":{"position":[[67,6],[98,6]]},"1235":{"position":[[13,6],[22,6],[55,6],[66,6],[77,6],[115,6],[148,6],[158,6],[424,6],[435,6],[444,6],[453,6],[463,6]]},"1324":{"position":[[786,4]]},"1406":{"position":[[422,5]]},"1413":{"position":[[441,6],[450,6],[552,6],[605,6],[832,6],[871,6],[1022,6]]},"1454":{"position":[[1722,6],[2377,6],[2568,6],[2748,6],[2776,6],[3039,6],[3084,6],[3112,6],[3170,6],[3204,6],[3869,5],[5490,6],[5641,6],[5689,6]]},"1479":{"position":[[422,5]]},"1486":{"position":[[441,6],[450,6],[552,6],[605,6],[832,6],[871,6],[1022,6]]},"1551":{"position":[[143,6]]},"1556":{"position":[[167,6]]},"1558":{"position":[[68,6]]},"1568":{"position":[[134,6]]},"1570":{"position":[[70,6],[89,6],[115,6]]}}}],["timer",{"_index":1804,"t":{"1270":{"position":[[1784,6]]}}}],["timer.start",{"_index":1813,"t":{"1270":{"position":[[1950,13]]}}}],["timer.stop",{"_index":1822,"t":{"1270":{"position":[[2142,12]]}}}],["timer.sum():.1f",{"_index":1833,"t":{"1270":{"position":[[2579,16]]}}}],["tip",{"_index":0,"t":{"801":{"position":[[0,3]]},"812":{"position":[[0,3]]},"818":{"position":[[0,3]]},"822":{"position":[[0,3]]},"867":{"position":[[74,3],[247,3],[409,3]]},"874":{"position":[[0,3]]},"876":{"position":[[0,3]]},"879":{"position":[[0,3]]},"886":{"position":[[38,3]]},"898":{"position":[[135,3],[252,3]]},"906":{"position":[[0,3]]},"940":{"position":[[1335,3]]},"950":{"position":[[0,3]]},"971":{"position":[[271,3]]},"979":{"position":[[33,3]]},"1003":{"position":[[0,3]]},"1030":{"position":[[126,3]]},"1073":{"position":[[0,3]]},"1107":{"position":[[0,3]]},"1131":{"position":[[286,3]]},"1154":{"position":[[0,3]]},"1156":{"position":[[0,3]]},"1162":{"position":[[133,3]]},"1210":{"position":[[0,3]]},"1312":{"position":[[1335,3]]},"1354":{"position":[[809,4]]},"1494":{"position":[[0,3]]},"1521":{"position":[[0,3]]},"1527":{"position":[[15,3]]},"1530":{"position":[[736,3]]},"1532":{"position":[[0,3]]},"1536":{"position":[[116,3],[254,3]]},"1539":{"position":[[256,3],[394,3]]},"1541":{"position":[[736,3]]},"1547":{"position":[[16,3],[136,3]]},"1556":{"position":[[0,3],[519,3],[721,3]]},"1560":{"position":[[154,3]]},"1562":{"position":[[0,3]]},"1585":{"position":[[0,3]]}}}],["titile(if",{"_index":2787,"t":{"1514":{"position":[[66,9]]}}}],["titl",{"_index":2782,"t":{"1512":{"position":[[321,7]]},"1514":{"position":[[100,5]]}}}],["title/window",{"_index":2786,"t":{"1514":{"position":[[53,12]]}}}],["title文字不能垂直居中，可以更换为window",{"_index":2781,"t":{"1512":{"position":[[295,25]]}}}],["tlpf",{"_index":607,"t":{"979":{"position":[[945,14]]}}}],["todo(\"start",{"_index":295,"t":{"844":{"position":[[2148,11]]}}}],["token",{"_index":850,"t":{"1051":{"position":[[20,9],[577,8]]},"1053":{"position":[[44,8],[179,8]]},"1177":{"position":[[20,9],[577,8]]},"1179":{"position":[[44,8],[179,8]]},"1279":{"position":[[120,7],[323,5],[563,5],[636,11]]},"1281":{"position":[[174,5],[261,5],[302,5]]},"1283":{"position":[[67,5],[116,5]]},"1285":{"position":[[56,24]]},"1290":{"position":[[76,5],[137,5]]},"1297":{"position":[[34,5]]},"1340":{"position":[[89,5],[411,11],[565,9]]},"1342":{"position":[[575,9]]},"1345":{"position":[[72,12],[250,9]]},"1347":{"position":[[2066,7]]},"1364":{"position":[[120,7],[323,5],[563,5],[636,11]]},"1366":{"position":[[174,5],[261,5],[302,5]]},"1368":{"position":[[67,5],[116,5]]},"1370":{"position":[[56,24]]},"1375":{"position":[[76,5],[137,5]]},"1382":{"position":[[34,5]]},"1406":{"position":[[249,5]]},"1408":{"position":[[37,5]]},"1410":{"position":[[26,5],[186,6],[897,5],[940,5],[1090,5],[1117,5],[1295,6],[1764,6],[1818,6],[1853,6],[1995,5]]},"1413":{"position":[[207,5],[259,5],[283,5],[303,5],[380,5],[401,11]]},"1415":{"position":[[47,16],[244,21],[296,9],[410,9],[668,9]]},"1417":{"position":[[230,5],[300,6]]},"1421":{"position":[[89,5],[411,11],[565,9]]},"1423":{"position":[[575,9]]},"1426":{"position":[[72,12],[250,9]]},"1428":{"position":[[2067,7]]},"1433":{"position":[[1249,8],[1294,6],[1312,6],[1346,5],[1375,15],[1401,12],[1424,12],[1449,5],[1471,5],[1581,5],[1618,6],[1643,5],[1679,37],[1724,5]]},"1452":{"position":[[117,6]]},"1454":{"position":[[1759,5],[1776,5],[7266,21],[7365,6],[7448,6],[7469,9],[7560,5],[7591,9],[7628,5],[8782,9]]},"1479":{"position":[[249,5]]},"1481":{"position":[[37,5]]},"1483":{"position":[[26,5],[186,6],[897,5],[940,5],[1090,5],[1117,5],[1295,6],[1764,6],[1818,6],[1853,6],[1995,5]]},"1486":{"position":[[207,5],[259,5],[283,5],[303,5],[380,5],[401,11]]},"1488":{"position":[[47,16],[244,21],[296,9],[410,9],[668,9]]},"1490":{"position":[[12,9]]},"1492":{"position":[[230,5],[300,6]]}}}],["tokenizer、quant",{"_index":2159,"t":{"1342":{"position":[[388,19]]},"1423":{"position":[[388,19]]}}}],["tokenize后为tensor",{"_index":858,"t":{"1051":{"position":[[161,19],[433,19]]},"1177":{"position":[[161,19],[433,19]]}}}],["tokens。vqgan",{"_index":2460,"t":{"1433":{"position":[[1273,12]]}}}],["token或random",{"_index":1113,"t":{"1081":{"position":[[92,12]]}}}],["token所对应的输出向量，随机初始化两个相同维度的向量，分别与输出向量做dot",{"_index":1162,"t":{"1083":{"position":[[1264,57]]}}}],["token进行遮挡后，喂入encoder中。对于每个被mask掉的词汇，bert",{"_index":1114,"t":{"1081":{"position":[[105,68]]}}}],["token）出发进行思考，产生了自回归模型是否有必要与向量量化（vector",{"_index":1929,"t":{"1281":{"position":[[186,38]]},"1366":{"position":[[186,38]]}}}],["top",{"_index":1182,"t":{"1101":{"position":[[0,3],[73,3],[364,3],[433,3],[492,3]]},"1103":{"position":[[0,3]]},"1105":{"position":[[62,3],[70,3]]}}}],["torch",{"_index":1398,"t":{"1138":{"position":[[3241,5]]},"1201":{"position":[[298,5]]},"1203":{"position":[[289,5]]},"1270":{"position":[[7,5],[37,5],[112,5]]},"1441":{"position":[[285,5]]}}}],["torch.cat([clip.tokenize(p",{"_index":852,"t":{"1051":{"position":[[57,27],[332,27]]},"1177":{"position":[[57,27],[332,27]]}}}],["torch.concat((a",{"_index":1854,"t":{"1275":{"position":[[295,16]]}}}],["torch.linspac",{"_index":1473,"t":{"1201":{"position":[[308,15]]},"1203":{"position":[[299,15]]}}}],["torch.log",{"_index":1548,"t":{"1208":{"position":[[1438,9]]}}}],["torch.log(torch.softmax(predict",{"_index":1555,"t":{"1208":{"position":[[1584,32]]}}}],["torch.nn",{"_index":1471,"t":{"1201":{"position":[[259,8]]},"1203":{"position":[[250,8]]},"1208":{"position":[[1475,8]]},"1270":{"position":[[64,8]]}}}],["torch.nn.crossentropyloss",{"_index":1798,"t":{"1270":{"position":[[1630,27]]}}}],["torch.nn.crossentropyloss相当于torch.softmax",{"_index":1547,"t":{"1208":{"position":[[1394,41]]}}}],["torch.nn.funct",{"_index":1399,"t":{"1138":{"position":[[3254,19]]}}}],["torch.nn.modul",{"_index":1770,"t":{"1270":{"position":[[971,17]]}}}],["torch.nn.nllloss",{"_index":1549,"t":{"1208":{"position":[[1450,17]]}}}],["torch.nn.sequenti",{"_index":1749,"t":{"1270":{"position":[[505,20]]}}}],["torch.optim.sgd(net.paramet",{"_index":1796,"t":{"1270":{"position":[[1582,33]]}}}],["torch.rand(4",{"_index":1846,"t":{"1275":{"position":[[104,13],[143,13]]}}}],["torch.reshape(x",{"_index":2473,"t":{"1441":{"position":[[347,16]]}}}],["torch.size([2",{"_index":1853,"t":{"1275":{"position":[[269,14]]}}}],["torch.size([4",{"_index":1858,"t":{"1275":{"position":[[390,14]]}}}],["torch.stack((a",{"_index":1849,"t":{"1275":{"position":[[182,15]]}}}],["torch.sum(y_hat.argmax(dim=1",{"_index":1783,"t":{"1270":{"position":[[1328,29]]}}}],["torch.tensor([1",{"_index":1557,"t":{"1208":{"position":[[1634,16],[1817,16]]},"1251":{"position":[[371,16]]},"1275":{"position":[[1043,16]]}}}],["torch.tensor([4",{"_index":1690,"t":{"1251":{"position":[[399,16]]},"1275":{"position":[[1071,16]]}}}],["torch.tensor([[0",{"_index":1343,"t":{"1138":{"position":[[962,17],[1142,17],[3546,17],[3726,17]]}}}],["torch.tensor([[0.1",{"_index":1871,"t":{"1275":{"position":[[777,19]]}}}],["torch.tensor([[1",{"_index":1345,"t":{"1138":{"position":[[1052,17],[3636,17]]},"1251":{"position":[[542,17]]},"1275":{"position":[[1211,17]]}}}],["torch.tensor([[2",{"_index":1553,"t":{"1208":{"position":[[1537,17],[1772,17]]}}}],["torch.tensor([[3",{"_index":1862,"t":{"1275":{"position":[[487,17]]}}}],["torch.tensor([[4",{"_index":1700,"t":{"1251":{"position":[[584,17]]},"1275":{"position":[[1252,17]]}}}],["torch.tensor(input",{"_index":1321,"t":{"1138":{"position":[[395,20],[3442,20]]}}}],["torch.tensor(np.arange(9",{"_index":2472,"t":{"1441":{"position":[[295,26]]}}}],["torchaudio",{"_index":829,"t":{"1043":{"position":[[74,10]]},"1169":{"position":[[74,10]]}}}],["torchvis",{"_index":828,"t":{"1043":{"position":[[62,11]]},"1169":{"position":[[62,11]]}}}],["tpuv4",{"_index":2692,"t":{"1454":{"position":[[8699,5]]}}}],["tqdm",{"_index":834,"t":{"1043":{"position":[[147,4]]},"1169":{"position":[[147,4]]}}}],["trade",{"_index":2221,"t":{"1347":{"position":[[2451,29]]},"1428":{"position":[[2452,29]]}}}],["train",{"_index":767,"t":{"1024":{"position":[[987,12]]},"1057":{"position":[[334,5]]},"1122":{"position":[[340,9]]},"1138":{"position":[[835,9]]},"1183":{"position":[[334,5]]},"1226":{"position":[[214,5]]},"1228":{"position":[[144,8]]},"1230":{"position":[[0,5],[14,5]]},"1270":{"position":[[1758,6],[2492,5]]},"1324":{"position":[[522,8],[714,8]]},"1340":{"position":[[484,8],[980,8]]},"1354":{"position":[[799,9]]},"1410":{"position":[[1518,7]]},"1421":{"position":[[484,8],[980,8]]},"1454":{"position":[[3403,8],[3721,8],[3792,7]]},"1483":{"position":[[1518,7]]}}}],["train(lenet",{"_index":1841,"t":{"1270":{"position":[[2762,12]]}}}],["train(net",{"_index":1784,"t":{"1270":{"position":[[1368,10]]}}}],["train_acc",{"_index":1825,"t":{"1270":{"position":[[2187,9],[2349,10]]}}}],["train_acc:.3f",{"_index":1830,"t":{"1270":{"position":[[2502,16]]}}}],["train_it",{"_index":1785,"t":{"1270":{"position":[[1379,11],[2656,11],[2775,11]]}}}],["train_l",{"_index":1823,"t":{"1270":{"position":[[2155,7],[2339,9]]}}}],["train_l:.3f",{"_index":1829,"t":{"1270":{"position":[[2477,14]]}}}],["train_lay",{"_index":921,"t":{"1057":{"position":[[280,12],[379,12],[394,14]]},"1183":{"position":[[280,12],[379,12],[394,14]]}}}],["trained的cnn，该cnn通常使用预训练的incept",{"_index":756,"t":{"1024":{"position":[[89,33]]}}}],["training&fin",{"_index":1157,"t":{"1083":{"position":[[1030,13]]}}}],["tran",{"_index":2276,"t":{"1354":{"position":[[691,5],[707,5]]}}}],["transform",{"_index":398,"t":{"927":{"position":[[20,23]]},"932":{"position":[[0,11]]},"944":{"position":[[2,11],[104,11],[486,18],[609,11]]},"1014":{"position":[[0,11]]},"1067":{"position":[[114,14],[240,13],[463,20],[587,16]]},"1085":{"position":[[412,11]]},"1118":{"position":[[56,12]]},"1120":{"position":[[57,11],[101,12]]},"1144":{"position":[[93,11]]},"1152":{"position":[[340,11]]},"1193":{"position":[[114,14],[240,13],[463,20],[587,16]]},"1299":{"position":[[20,23]]},"1304":{"position":[[0,11]]},"1316":{"position":[[2,11],[104,11],[486,18],[609,11]]},"1399":{"position":[[154,20],[175,11],[247,11],[306,11]]},"1401":{"position":[[29,11],[155,11]]},"1403":{"position":[[120,11]]},"1406":{"position":[[330,12],[472,12],[782,11]]},"1413":{"position":[[217,28]]},"1415":{"position":[[270,12],[320,11]]},"1433":{"position":[[866,13],[1209,12]]},"1435":{"position":[[47,16],[95,12]]},"1452":{"position":[[85,11],[128,11],[173,12],[190,11]]},"1479":{"position":[[330,12],[472,12],[782,11]]},"1486":{"position":[[217,28]]},"1488":{"position":[[270,12],[320,11]]}}}],["transformer_lay",{"_index":1056,"t":{"1067":{"position":[[484,17]]},"1193":{"position":[[484,17]]}}}],["transformerencoder(transformer_lay",{"_index":1062,"t":{"1067":{"position":[[631,37]]},"1193":{"position":[[631,37]]}}}],["transformerencoderlayer(d_model=512",{"_index":1057,"t":{"1067":{"position":[[504,36]]},"1193":{"position":[[504,36]]}}}],["transformermapperv2(nn.modul",{"_index":1042,"t":{"1067":{"position":[[78,31]]},"1193":{"position":[[78,31]]}}}],["transformers）本身是一种预训练的模型架构，通常是在大规模无标签数据上进行预训练，然后在特定任务上进行微调。bert",{"_index":1108,"t":{"1079":{"position":[[98,92]]}}}],["transformer中有lay",{"_index":1054,"t":{"1067":{"position":[[413,18]]},"1193":{"position":[[413,18]]}}}],["transformer实战练习，代码见github",{"_index":1241,"t":{"1118":{"position":[[5,28]]}}}],["transformer是sequ",{"_index":400,"t":{"927":{"position":[[70,20]]},"1299":{"position":[[70,20]]}}}],["transformer由encoder和decoder组成，编码器和解码器都包含6个block",{"_index":403,"t":{"929":{"position":[[0,58]]},"1301":{"position":[[0,58]]}}}],["transformer的推理阶段，自回归类型的decoder根据分词方式的不同，一个词汇一个词汇的输出，将当前时间步之前生成的所有词汇作为输入load进入decoder中。但在训练时如果遵从同样的生成范式会大大降低效率，并且面临则一步错步步错的风险（error",{"_index":475,"t":{"946":{"position":[[0,131]]},"1318":{"position":[[0,131]]}}}],["transformer训练时采取的teach",{"_index":455,"t":{"940":{"position":[[826,33]]},"1312":{"position":[[826,33]]}}}],["transformer（residu",{"_index":2346,"t":{"1399":{"position":[[120,20]]}}}],["treat",{"_index":108,"t":{"840":{"position":[[336,7]]}}}],["treebank",{"_index":1152,"t":{"1083":{"position":[[747,33]]}}}],["truncat",{"_index":873,"t":{"1051":{"position":[[725,19]]},"1177":{"position":[[725,19]]}}}],["trung",{"_index":2275,"t":{"1354":{"position":[[685,5],[730,5]]}}}],["truth",{"_index":786,"t":{"1026":{"position":[[755,6]]},"1081":{"position":[[248,21]]},"1454":{"position":[[6676,5],[7889,5]]}}}],["truth一次性喂到decod",{"_index":479,"t":{"946":{"position":[[176,38]]},"1318":{"position":[[176,38]]}}}],["truth做bc",{"_index":2855,"t":{"1549":{"position":[[64,9]]}}}],["truth做cross",{"_index":1170,"t":{"1087":{"position":[[119,11]]}}}],["truth对nois",{"_index":814,"t":{"1036":{"position":[[556,11]]}}}],["truth就是sampl",{"_index":779,"t":{"1026":{"position":[[437,18]]}}}],["truth就是第3步中sample出的噪声ϵ\\epsilon",{"_index":690,"t":{"1007":{"position":[[779,32]]}}}],["truth的bc",{"_index":2858,"t":{"1549":{"position":[[119,9]]}}}],["truth的策略，但是还额外需要一个encoder来产生lat",{"_index":781,"t":{"1026":{"position":[[553,34]]}}}],["tsinghua",{"_index":839,"t":{"1045":{"position":[[30,8]]},"1171":{"position":[[30,8]]}}}],["ttt",{"_index":1177,"t":{"1099":{"position":[[367,17]]},"1285":{"position":[[594,3],[772,3]]},"1287":{"position":[[365,3]]},"1370":{"position":[[594,3],[772,3]]},"1372":{"position":[[365,3]]},"1454":{"position":[[1529,3],[1745,3]]}}}],["ttt从t,…,1t,\\ldots,1t,…,1范围循环ttt",{"_index":693,"t":{"1009":{"position":[[54,33]]}}}],["ttt是从1,…,t1,\\ldots,t1,…,t范围中sample出的一个integ",{"_index":680,"t":{"1007":{"position":[[50,46]]}}}],["ttt计数的循环，若t>1t>1t>1，则从normal",{"_index":694,"t":{"1009":{"position":[[88,34]]}}}],["ttt，ztz_tzt",{"_index":2612,"t":{"1454":{"position":[[4637,12]]}}}],["tune",{"_index":474,"t":{"944":{"position":[[580,28]]},"1071":{"position":[[1119,9]]},"1079":{"position":[[267,13]]},"1197":{"position":[[1119,9]]},"1316":{"position":[[580,28]]},"1354":{"position":[[139,10],[158,54],[459,6]]},"1454":{"position":[[2731,4],[5820,6]]}}}],["tuning范式与scratch范式的训练效果做了对比，其中scratch范式即使用传统的随机初始化的方式从头训练整个分类网络。可以看到预训练&微调的训练范式可以加速模型的收敛（converg",{"_index":1158,"t":{"1083":{"position":[[1044,109]]}}}],["turn",{"_index":2027,"t":{"1324":{"position":[[979,4]]}}}],["tutori",{"_index":2524,"t":{"1449":{"position":[[16,9],[95,9]]}}}],["two",{"_index":2137,"t":{"1340":{"position":[[970,3]]},"1406":{"position":[[1246,3]]},"1421":{"position":[[970,3]]},"1454":{"position":[[3351,3]]},"1479":{"position":[[1246,3]]}}}],["tx0+1−αˉtϵ,t)∥2(1)\\nabla_{\\theta}\\left\\|\\boldsymbol{\\epsilon",{"_index":683,"t":{"1007":{"position":[[196,71]]}}}],["type(m",{"_index":1789,"t":{"1270":{"position":[[1451,7],[1475,7]]}}}],["typic",{"_index":1899,"t":{"1279":{"position":[[78,9]]},"1324":{"position":[[343,9]]},"1364":{"position":[[78,9]]}}}],["t}\\left",{"_index":2619,"t":{"1454":{"position":[[4857,10]]}}}],["t}\\left[\\left\\|\\varepsilon",{"_index":1940,"t":{"1285":{"position":[[240,26]]},"1370":{"position":[[240,26]]}}}],["t∈tt",{"_index":2607,"t":{"1454":{"position":[[4463,4]]}}}],["t∼uniform⁡[1,t]t",{"_index":2576,"t":{"1454":{"position":[[2069,16]]}}}],["u",{"_index":738,"t":{"1022":{"position":[[638,1],[696,1],[810,1]]},"1435":{"position":[[38,1]]},"1452":{"position":[[3,1]]},"1514":{"position":[[347,1]]},"1547":{"position":[[0,4],[125,1]]}}}],["u\\ell_uℓu",{"_index":2216,"t":{"1347":{"position":[[2221,11]]},"1428":{"position":[[2222,11]]}}}],["ui",{"_index":1037,"t":{"1064":{"position":[[43,2],[241,2],[266,12],[279,2]]},"1190":{"position":[[43,2],[241,2],[266,12],[279,2]]}}}],["uncertainti",{"_index":1502,"t":{"1208":{"position":[[139,12]]}}}],["unchang",{"_index":1685,"t":{"1251":{"position":[[322,9]]}}}],["uncom",{"_index":191,"t":{"840":{"position":[[1907,9]]},"846":{"position":[[851,9]]}}}],["uncondit",{"_index":379,"t":{"921":{"position":[[145,13],[171,13]]},"1324":{"position":[[1369,13]]}}}],["undefin",{"_index":150,"t":{"840":{"position":[[1101,9]]}}}],["understand",{"_index":750,"t":{"1022":{"position":[[887,13]]},"1083":{"position":[[125,13]]}}}],["unfreez",{"_index":917,"t":{"1057":{"position":[[161,8],[271,8],[507,8]]},"1183":{"position":[[161,8],[271,8],[507,8]]}}}],["unfreeze，然后对更新哪些层做出选择，承担选择任务的功能函数：model.zssgan.zssgan.determine_opt_lay",{"_index":915,"t":{"1057":{"position":[[70,77]]},"1183":{"position":[[70,77]]}}}],["unicod",{"_index":2675,"t":{"1454":{"position":[[7402,7]]}}}],["unifi",{"_index":676,"t":{"1005":{"position":[[833,7]]},"1406":{"position":[[1431,7]]},"1433":{"position":[[2025,5]]},"1479":{"position":[[1431,7]]}}}],["uniform",{"_index":2673,"t":{"1454":{"position":[[7338,7]]}}}],["uniqu",{"_index":2664,"t":{"1454":{"position":[[6718,6]]}}}],["unit",{"_index":2177,"t":{"1347":{"position":[[544,6]]},"1428":{"position":[[544,6]]}}}],["unrealist",{"_index":2557,"t":{"1454":{"position":[[913,11]]}}}],["unset",{"_index":2760,"t":{"1500":{"position":[[222,5],[238,5],[255,5]]}}}],["until",{"_index":1725,"t":{"1263":{"position":[[646,5]]}}}],["unzipped_list",{"_index":2511,"t":{"1445":{"position":[[389,14],[448,15]]}}}],["up",{"_index":2370,"t":{"1406":{"position":[[907,2]]},"1479":{"position":[[907,2]]}}}],["updat",{"_index":1633,"t":{"1230":{"position":[[47,6]]}}}],["update='append",{"_index":1231,"t":{"1114":{"position":[[261,16],[336,16]]},"1116":{"position":[[227,16]]}}}],["upgrad",{"_index":2713,"t":{"1469":{"position":[[59,7]]}}}],["url：rec",{"_index":2448,"t":{"1431":{"position":[[8,10]]}}}],["url：video",{"_index":2468,"t":{"1437":{"position":[[8,9]]}}}],["us",{"_index":4,"t":{"803":{"position":[[25,5]]},"814":{"position":[[25,5]]},"824":{"position":[[25,5]]},"840":{"position":[[760,4],[1206,4],[2025,6]]},"844":{"position":[[931,4]]},"846":{"position":[[969,6]]},"1120":{"position":[[53,3]]},"1138":{"position":[[705,5],[2658,3]]},"1226":{"position":[[39,4],[250,5]]},"1228":{"position":[[70,4],[132,4],[215,4]]},"1230":{"position":[[126,4]]},"1279":{"position":[[354,5],[472,5],[973,3]]},"1364":{"position":[[354,5],[472,5],[973,3]]},"1410":{"position":[[1628,4],[1730,4]]},"1454":{"position":[[3460,3],[3540,3],[3662,3],[7332,5],[7442,5]]},"1459":{"position":[[125,5]]},"1469":{"position":[[7,3],[97,3],[152,6]]},"1483":{"position":[[1628,4],[1730,4]]},"1527":{"position":[[66,5]]}}}],["usag",{"_index":2126,"t":{"1340":{"position":[[653,5]]},"1421":{"position":[[653,5]]}}}],["user",{"_index":2536,"t":{"1454":{"position":[[222,4]]}}}],["usual",{"_index":1329,"t":{"1138":{"position":[[661,7]]}}}],["utils/text_templates.pi",{"_index":1077,"t":{"1069":{"position":[[76,23]]},"1195":{"position":[[76,23]]}}}],["utkarsh",{"_index":2259,"t":{"1354":{"position":[[497,12]]}}}],["uttr",{"_index":1276,"t":{"1122":{"position":[[410,4],[575,4]]}}}],["ux+vy)}dudv\\tag{8}f(x,y)=∫−∞∞​∫−∞∞​f(u,v)ej2π(ux+vy)dudv(8",{"_index":561,"t":{"961":{"position":[[817,60]]}}}],["ux+vy)}dxdy\\tag{7}f(u,v)=∫−∞∞​∫−∞∞​f(x,y)e−j2π(ux+vy)dxdy(7",{"_index":558,"t":{"961":{"position":[[637,61]]}}}],["ux}du\\tag{6}f(x)=∫−∞∞​f(u)ej2πuxdu(6",{"_index":554,"t":{"961":{"position":[[480,37]]}}}],["ux}dx\\tag{5}f(u)=∫−∞+∞​f(x)e−j2πuxdx(5",{"_index":551,"t":{"961":{"position":[[364,39]]}}}],["ux}{n}}\\tag{10}f(x)=n1​u=0∑n−1​f(u)ejn2πux​(10",{"_index":568,"t":{"961":{"position":[[1100,47]]}}}],["ux}{n}}\\tag{9}f(u)=x=0∑n−1​f(x)e−jn2πux​(9",{"_index":565,"t":{"961":{"position":[[965,43]]}}}],["v",{"_index":1315,"t":{"1138":{"position":[[81,1]]},"1144":{"position":[[278,1]]},"1148":{"position":[[562,1],[592,1]]}}}],["v1",{"_index":2864,"t":{"1556":{"position":[[22,12]]}}}],["v1.5",{"_index":2629,"t":{"1454":{"position":[[5422,4]]}}}],["v2.4.3",{"_index":2703,"t":{"1465":{"position":[[21,6]]}}}],["v3",{"_index":947,"t":{"1059":{"position":[[126,2],[1319,2],[1348,2]]},"1185":{"position":[[126,2],[1319,2],[1348,2]]},"1467":{"position":[[4,2],[22,9]]},"1469":{"position":[[149,2],[163,4]]}}}],["v3.5.2",{"_index":2704,"t":{"1465":{"position":[[32,16]]}}}],["v3模型。在计算fid时，生成图像和真实图像分别输入到预训练的cnn中，提取出各自的特征表示向量（representation）。这两个represent",{"_index":757,"t":{"1024":{"position":[[123,105]]}}}],["v]^{h_k",{"_index":2430,"t":{"1413":{"position":[[823,8]]},"1486":{"position":[[823,8]]}}}],["v_n",{"_index":314,"t":{"857":{"position":[[138,6]]},"861":{"position":[[171,4],[203,6]]}}}],["v_t",{"_index":312,"t":{"857":{"position":[[128,4]]},"859":{"position":[[53,5]]},"861":{"position":[[193,4]]}}}],["vae",{"_index":2337,"t":{"1393":{"position":[[87,3],[105,3],[249,3],[506,3]]},"1395":{"position":[[13,3],[87,3],[247,3]]},"1397":{"position":[[13,3],[168,3],[270,3],[312,3]]},"1399":{"position":[[291,3]]},"1410":{"position":[[1388,3],[1686,4],[1721,3]]},"1433":{"position":[[263,3],[310,3],[322,3],[331,3],[335,25],[380,3],[401,8],[410,64],[510,3],[521,3],[549,3],[610,3],[727,3],[891,3],[2098,15]]},"1483":{"position":[[1388,3],[1686,4],[1721,3]]}}}],["vaes（vari",{"_index":2340,"t":{"1393":{"position":[[512,16]]}}}],["vae、gan以及diffus",{"_index":712,"t":{"1018":{"position":[[0,18]]}}}],["vae、vqvae（2017",{"_index":2453,"t":{"1433":{"position":[[233,21]]}}}],["vae在训练过程中，期待ecoder输入多张图片后，输出的向量在一起符合某个随机分布（e.g",{"_index":801,"t":{"1030":{"position":[[130,47]]}}}],["vae的数学原理，从vae到diffus",{"_index":673,"t":{"1005":{"position":[[735,38]]}}}],["vae（vector",{"_index":2338,"t":{"1393":{"position":[[177,10],[318,10]]}}}],["vae）与diffus",{"_index":669,"t":{"1005":{"position":[[591,14]]}}}],["vae）的训练策略是使用encoder将输入图像对应（嵌入）到一个符合某随机分布的向量，再将该向量作为decoder的输入，加上文字prompt",{"_index":800,"t":{"1030":{"position":[[39,86]]}}}],["vae，vq",{"_index":2341,"t":{"1395":{"position":[[240,6]]}}}],["valid",{"_index":1281,"t":{"1122":{"position":[[474,5]]},"1226":{"position":[[13,10],[139,10]]},"1228":{"position":[[34,11]]},"1230":{"position":[[54,10]]}}}],["valu",{"_index":1341,"t":{"1138":{"position":[[947,6],[1270,6],[1326,6],[1441,7],[1993,18],[2153,6],[2189,6],[2255,9],[3531,6],[3834,6],[3890,6],[4005,7],[4312,6],[4348,6],[4414,9]]},"1279":{"position":[[161,6],[446,6],[629,6],[1026,6]]},"1364":{"position":[[161,6],[446,6],[629,6],[1026,6]]}}}],["value^{n",{"_index":1410,"t":{"1140":{"position":[[262,8]]}}}],["vanilla",{"_index":2107,"t":{"1340":{"position":[[209,7]]},"1421":{"position":[[209,7]]}}}],["var",{"_index":2157,"t":{"1342":{"position":[[192,3]]},"1406":{"position":[[42,6],[403,4],[537,3],[752,3],[910,3],[1062,3],[1215,3]]},"1410":{"position":[[851,3],[864,3],[1018,3],[1787,3]]},"1413":{"position":[[1092,3]]},"1415":{"position":[[266,3],[292,3],[316,3]]},"1417":{"position":[[413,3],[506,3]]},"1423":{"position":[[192,3]]},"1433":{"position":[[991,9],[1001,21]]},"1475":{"position":[[260,4],[294,4],[333,4],[373,4]]},"1479":{"position":[[42,6],[403,4],[537,3],[752,3],[910,3],[1062,3],[1215,3]]},"1483":{"position":[[851,3],[864,3],[1018,3],[1787,3]]},"1486":{"position":[[1092,3]]},"1488":{"position":[[266,3],[292,3],[316,3]]},"1490":{"position":[[27,3],[252,3]]},"1492":{"position":[[413,3],[506,3]]}}}],["varepsilon",{"_index":2617,"t":{"1454":{"position":[[4832,11],[4868,11]]}}}],["varepsilon_\\theta",{"_index":1954,"t":{"1285":{"position":[[627,23]]},"1370":{"position":[[627,23]]}}}],["varepsilon_\\theta\\left(x_t",{"_index":1941,"t":{"1285":{"position":[[267,27]]},"1370":{"position":[[267,27]]}}}],["varepsilonxt​=αˉt​x​+1−αˉt",{"_index":1951,"t":{"1285":{"position":[[517,33]]},"1370":{"position":[[517,33]]}}}],["vari",{"_index":2667,"t":{"1454":{"position":[[6868,6]]}}}],["variant",{"_index":1918,"t":{"1279":{"position":[[793,9]]},"1364":{"position":[[793,9]]},"1393":{"position":[[5,7]]},"1433":{"position":[[281,7]]}}}],["variat",{"_index":666,"t":{"1005":{"position":[[545,25]]},"1030":{"position":[[0,18]]},"1393":{"position":[[198,11],[339,11]]}}}],["variou",{"_index":2039,"t":{"1324":{"position":[[1344,7]]}}}],["vec",{"_index":2819,"t":{"1527":{"position":[[820,4]]}}}],["vec.empti",{"_index":2824,"t":{"1527":{"position":[[971,13]]}}}],["vec.push_back(remaind",{"_index":2822,"t":{"1527":{"position":[[904,25]]}}}],["vec.rbegin",{"_index":2825,"t":{"1527":{"position":[[1024,13]]}}}],["vec.rend",{"_index":2826,"t":{"1527":{"position":[[1044,11]]}}}],["vector",{"_index":931,"t":{"1057":{"position":[[845,10]]},"1183":{"position":[[845,10]]},"1277":{"position":[[87,6]]},"1279":{"position":[[103,6],[815,6]]},"1362":{"position":[[87,6]]},"1364":{"position":[[103,6],[815,6]]},"1433":{"position":[[693,7]]}}}],["vector<int",{"_index":55,"t":{"824":{"position":[[59,11]]},"1527":{"position":[[808,11]]}}}],["vector）zzz",{"_index":1930,"t":{"1283":{"position":[[92,10]]},"1368":{"position":[[92,10]]}}}],["veri",{"_index":198,"t":{"840":{"position":[[2018,4]]},"846":{"position":[[962,4]]},"1138":{"position":[[2937,4]]}}}],["verifi",{"_index":2142,"t":{"1340":{"position":[[1122,6]]},"1406":{"position":[[738,8]]},"1421":{"position":[[1122,6]]},"1479":{"position":[[738,8]]}}}],["version",{"_index":2720,"t":{"1469":{"position":[[193,8]]}}}],["version/src/project0/build目录下执行mak",{"_index":102,"t":{"840":{"position":[[43,35]]}}}],["version/src/projecti/build",{"_index":84,"t":{"830":{"position":[[287,33]]},"834":{"position":[[114,29]]}}}],["version/src/projecti/build文件夹下进行，即要在终端中通过cd",{"_index":303,"t":{"846":{"position":[[17,49]]}}}],["version/src/projecti/build目录下创建.bochsrc",{"_index":161,"t":{"840":{"position":[[1379,41]]}}}],["version/src/projecti/build目录下的makefi",{"_index":116,"t":{"840":{"position":[[593,39],[1158,39]]}}}],["version/src/projecti/build目录下的makefie文件(由于每个project下都存在一个对应的makefil",{"_index":110,"t":{"840":{"position":[[369,71]]}}}],["version/src/projecti/src/geeko",{"_index":83,"t":{"830":{"position":[[232,37]]}}}],["version/src/projecti/src/geekos/main.c",{"_index":203,"t":{"844":{"position":[[9,40]]}}}],["version/src/目录下会存在project0",{"_index":80,"t":{"830":{"position":[[131,26]]}}}],["vert",{"_index":1573,"t":{"1214":{"position":[[83,5]]},"1216":{"position":[[92,5],[273,5]]}}}],["vert^2_1",{"_index":1574,"t":{"1214":{"position":[[91,9]]},"1216":{"position":[[100,9],[281,9]]}}}],["vert_2",{"_index":1491,"t":{"1206":{"position":[[181,7]]}}}],["vertic",{"_index":2788,"t":{"1514":{"position":[[124,11]]}}}],["vga_update_interv",{"_index":185,"t":{"840":{"position":[[1805,20]]},"846":{"position":[[749,20]]}}}],["vgaromimag",{"_index":168,"t":{"840":{"position":[[1509,12]]},"846":{"position":[[453,12]]}}}],["vi",{"_index":1224,"t":{"1114":{"position":[[26,3]]},"1116":{"position":[[26,3]]}}}],["via",{"_index":2227,"t":{"1349":{"position":[[110,3]]},"1354":{"position":[[627,3]]}}}],["video",{"_index":2405,"t":{"1410":{"position":[[1680,5]]},"1449":{"position":[[121,5]]},"1457":{"position":[[14,5],[51,6],[74,5],[132,6],[147,5],[177,5],[235,5],[280,6]]},"1459":{"position":[[41,5],[111,5]]},"1483":{"position":[[1680,5]]}}}],["viet",{"_index":2277,"t":{"1354":{"position":[[697,4]]}}}],["vis.line([0",{"_index":1226,"t":{"1114":{"position":[[116,14]]}}}],["vis.line([[0",{"_index":1235,"t":{"1116":{"position":[[41,14]]}}}],["vis.line([loss.item",{"_index":1232,"t":{"1114":{"position":[[289,23]]}}}],["visdom",{"_index":1222,"t":{"1109":{"position":[[12,6]]},"1112":{"position":[[0,28],[37,6]]},"1114":{"position":[[5,6],[19,6],[32,8]]},"1116":{"position":[[5,6],[19,6],[32,8]]}}}],["visdom.serv",{"_index":1223,"t":{"1112":{"position":[[54,13]]}}}],["visual",{"_index":2024,"t":{"1324":{"position":[[892,6]]},"1340":{"position":[[144,6],[281,6],[1080,6],[1338,6]]},"1406":{"position":[[11,6],[352,6],[1409,6]]},"1408":{"position":[[168,6]]},"1421":{"position":[[144,6],[281,6],[1080,6],[1338,6]]},"1433":{"position":[[1266,6],[1636,6],[1672,6],[1717,6]]},"1479":{"position":[[11,6],[352,6],[1409,6]]},"1481":{"position":[[168,6]]}}}],["vit",{"_index":2456,"t":{"1433":{"position":[[668,8],[938,16]]}}}],["viz.line([[y1",{"_index":1239,"t":{"1116":{"position":[[177,14]]}}}],["viz.line([real_y_data",{"_index":1229,"t":{"1114":{"position":[[208,23]]}}}],["vllm",{"_index":2163,"t":{"1342":{"position":[[761,4]]},"1423":{"position":[[760,4]]}}}],["vnv_nvn",{"_index":351,"t":{"893":{"position":[[68,21]]}}}],["voc1voc_1voc1​词汇向量输入时，在decoder中，voc1voc_1voc1",{"_index":486,"t":{"948":{"position":[[177,87]]},"1320":{"position":[[177,87]]}}}],["vocabulari",{"_index":441,"t":{"940":{"position":[[6,16]]},"1312":{"position":[[6,16]]},"1454":{"position":[[7248,10]]}}}],["vocabulary）是一个包含了在特定语言或任务中所有可能出现的所有单词或标记的集合。在自然语言处理（nlp",{"_index":442,"t":{"940":{"position":[[23,95]]},"1312":{"position":[[23,95]]}}}],["voc中，类别种类为20类，因此在预测阶段输出的[7",{"_index":2881,"t":{"1556":{"position":[[423,27]]}}}],["void",{"_index":205,"t":{"844":{"position":[[82,4],[1294,4],[1833,4]]}}}],["voxceleb1",{"_index":1253,"t":{"1122":{"position":[[24,10],[72,10]]}}}],["vq",{"_index":2336,"t":{"1393":{"position":[[78,8],[102,2],[161,15],[310,7],[503,2]]},"1395":{"position":[[10,2],[79,7],[237,2]]},"1397":{"position":[[10,2],[165,2],[267,2],[309,2]]},"1399":{"position":[[288,2]]},"1413":{"position":[[1241,2],[1261,2]]},"1415":{"position":[[12,2],[44,2],[220,9],[407,2],[665,2]]},"1433":{"position":[[260,2],[307,2],[328,2],[377,2],[507,2],[525,23],[553,56],[690,2],[724,2],[821,2],[888,2]]},"1486":{"position":[[1241,2],[1261,2]]},"1488":{"position":[[12,2],[44,2],[220,9],[407,2],[665,2]]}}}],["vqgan",{"_index":2164,"t":{"1345":{"position":[[5,5]]},"1397":{"position":[[0,5],[63,15]]},"1410":{"position":[[1033,5],[1212,5],[1331,5]]},"1415":{"position":[[346,5]]},"1426":{"position":[[5,5]]},"1433":{"position":[[897,5],[1238,10]]},"1483":{"position":[[1033,5],[1212,5],[1331,5]]},"1488":{"position":[[346,5]]}}}],["vqgan、vqvae、var、maskgit",{"_index":1928,"t":{"1281":{"position":[[108,23]]},"1366":{"position":[[108,23]]}}}],["vqgan（2021",{"_index":2455,"t":{"1433":{"position":[[650,17]]}}}],["vqgan（2022",{"_index":2457,"t":{"1433":{"position":[[677,12]]}}}],["vqgan（vector",{"_index":2342,"t":{"1397":{"position":[[101,17]]}}}],["vqgan，rq",{"_index":2347,"t":{"1399":{"position":[[297,8]]}}}],["vqvae",{"_index":2334,"t":{"1393":{"position":[[31,5]]},"1395":{"position":[[0,5]]},"1410":{"position":[[1505,5]]},"1415":{"position":[[64,5]]},"1483":{"position":[[1505,5]]},"1488":{"position":[[64,5]]},"1490":{"position":[[6,5]]}}}],["vqvae、vqgan",{"_index":2323,"t":{"1386":{"position":[[517,47]]}}}],["vqvae、vqga、dal",{"_index":2155,"t":{"1342":{"position":[[64,21]]},"1423":{"position":[[64,21]]}}}],["vtv_tvt",{"_index":350,"t":{"893":{"position":[[47,20]]}}}],["vt​∪vn​)∗(1",{"_index":316,"t":{"857":{"position":[[158,15]]}}}],["vt​∪vn​)∗(3",{"_index":328,"t":{"861":{"position":[[229,15]]}}}],["vt∗(2)",{"_index":317,"t":{"859":{"position":[[6,9]]}}}],["vt∗​(2",{"_index":321,"t":{"859":{"position":[[72,9]]}}}],["vt∪vn)∗(1",{"_index":309,"t":{"857":{"position":[[73,13]]}}}],["vt∪vn)∗(3)\\alpha",{"_index":325,"t":{"861":{"position":[[115,19]]}}}],["vvv",{"_index":1187,"t":{"1101":{"position":[[216,3]]},"1103":{"position":[[166,3]]}}}],["w",{"_index":895,"t":{"1055":{"position":[[185,1],[200,36],[390,1],[407,3],[411,1],[452,3],[770,1],[808,1],[818,1],[853,1],[887,1],[897,1]]},"1057":{"position":[[533,1],[585,1],[664,1],[737,6],[896,15],[951,1]]},"1062":{"position":[[72,1],[346,1],[618,1]]},"1067":{"position":[[13,1]]},"1181":{"position":[[185,1],[200,36],[390,1],[407,3],[411,1],[452,3],[770,1],[808,1],[818,1],[853,1],[887,1],[897,1]]},"1183":{"position":[[533,1],[585,1],[664,1],[737,6],[896,15],[951,1]]},"1188":{"position":[[72,1],[346,1],[618,1]]},"1193":{"position":[[13,1]]},"1214":{"position":[[89,1]]},"1216":{"position":[[98,1],[237,2],[279,1],[332,2],[345,1]]},"1235":{"position":[[451,1]]},"1263":{"position":[[564,1],[608,1],[612,1]]},"1413":{"position":[[448,1]]},"1447":{"position":[[170,3]]},"1486":{"position":[[448,1]]},"1556":{"position":[[308,2]]}}}],["w')o(co​×ci​×h×w×h′×w",{"_index":1656,"t":{"1235":{"position":[[470,23]]}}}],["w'co​×h′×w",{"_index":1649,"t":{"1235":{"position":[[165,11]]}}}],["w(l(w,b)+λ2∥w∥12)=∂l(w,b)∂w+λw(3)\\frac{\\partial}{\\parti",{"_index":1586,"t":{"1216":{"position":[[177,59]]}}}],["w1,p1),(w2,p2),…,(wv,pv)}\\left\\{\\left(w_1",{"_index":1203,"t":{"1103":{"position":[[207,44]]}}}],["w1,w2w_{1",{"_index":2181,"t":{"1347":{"position":[[789,17]]},"1428":{"position":[[789,17]]}}}],["w][b,1,h,w]的tensor",{"_index":2906,"t":{"1564":{"position":[[196,46]]}}}],["w][b,1,h,w]的tensor，再将二者concat后通过7×77",{"_index":2910,"t":{"1568":{"position":[[97,36]]}}}],["w][b,c,h,w]分别经过最大池化和平均池化来压缩空间维度、学习通道之间的特征，得到[b,c,1,1][b",{"_index":2907,"t":{"1566":{"position":[[24,56]]}}}],["w][b,c,h,w]分别经过最大池化和平均池化（通过torch.max和torch.mean函数实现）得到[b,1,h,w][b",{"_index":2909,"t":{"1568":{"position":[[24,66]]}}}],["w][b,c,h,w]的特征图通过池化挤压宽高维度，得到[b,c,1,1][b",{"_index":2921,"t":{"1574":{"position":[[56,40]]}}}],["w][b,c,h,w]经过空间注意力机制算法得到[b,1,h,w][b",{"_index":2905,"t":{"1564":{"position":[[153,36]]}}}],["w][b,c,h,w]经过通道注意力机制算法得到[b,c,1,1][b",{"_index":2903,"t":{"1564":{"position":[[30,36]]}}}],["w_2\\\\1&d(u,v",{"_index":638,"t":{"983":{"position":[[525,14]]}}}],["w_2\\end{cases}\\tag{24}h(u,v)=⎩⎨⎧​101​d(u,v)<w1​w1​⩽d(u,v)≤w2​d(u,v)>w2​​(24",{"_index":639,"t":{"983":{"position":[[540,76]]}}}],["w_code",{"_index":1021,"t":{"1062":{"position":[[143,20]]},"1188":{"position":[[143,20]]}}}],["w_encod",{"_index":1024,"t":{"1062":{"position":[[292,9]]},"1188":{"position":[[292,9]]}}}],["w_i",{"_index":1198,"t":{"1101":{"position":[[603,3]]},"1103":{"position":[[712,3]]}}}],["w_k",{"_index":2435,"t":{"1413":{"position":[[1029,3]]},"1486":{"position":[[1029,3]]}}}],["w_key",{"_index":1342,"t":{"1138":{"position":[[954,5],[1293,5],[3538,5],[3857,5]]}}}],["w_khk​×wk",{"_index":2419,"t":{"1413":{"position":[[559,10],[878,10]]},"1486":{"position":[[559,10],[878,10]]}}}],["w_k}rk​∈[v]hk​×wk",{"_index":2431,"t":{"1413":{"position":[[839,18]]},"1486":{"position":[[839,18]]}}}],["w_queri",{"_index":1344,"t":{"1138":{"position":[[1042,7],[1318,7],[3626,7],[3882,7]]}}}],["w_t",{"_index":1596,"t":{"1216":{"position":[[495,4]]},"1454":{"position":[[6496,3],[8273,3]]}}}],["w_valu",{"_index":1346,"t":{"1138":{"position":[[1132,7],[1344,7],[3716,7],[3908,7]]}}}],["w_{2}w1​,w2",{"_index":2182,"t":{"1347":{"position":[[807,12]]},"1428":{"position":[[807,12]]}}}],["w_{t",{"_index":2686,"t":{"1454":{"position":[[8379,6]]}}}],["wall",{"_index":156,"t":{"840":{"position":[[1266,4],[1315,4]]}}}],["warn",{"_index":106,"t":{"840":{"position":[[321,8]]}}}],["waveform",{"_index":1258,"t":{"1122":{"position":[[107,9]]}}}],["wci​×h×w",{"_index":1642,"t":{"1235":{"position":[[29,8]]}}}],["web",{"_index":1036,"t":{"1064":{"position":[[39,3]]},"1190":{"position":[[39,3]]}}}],["web_ui",{"_index":1040,"t":{"1064":{"position":[[298,8]]},"1190":{"position":[[298,8]]}}}],["weight",{"_index":1328,"t":{"1138":{"position":[[649,7],[917,7],[2144,8],[3501,7],[4303,8]]},"1251":{"position":[[336,7]]},"1475":{"position":[[420,7]]}}}],["weighted_valu",{"_index":1371,"t":{"1138":{"position":[[2237,15],[2330,16],[4396,15],[4489,16]]}}}],["weighted_values.sum(dim=0",{"_index":1376,"t":{"1138":{"position":[[2576,26],[4534,26]]}}}],["welcom",{"_index":2731,"t":{"1475":{"position":[[101,8]]}}}],["well",{"_index":1382,"t":{"1138":{"position":[[2712,4]]},"1406":{"position":[[397,5]]},"1454":{"position":[[7513,5]]},"1479":{"position":[[397,5]]}}}],["werror",{"_index":114,"t":{"840":{"position":[[507,6]]}}}],["whether",{"_index":2106,"t":{"1340":{"position":[[201,7]]},"1421":{"position":[[201,7]]}}}],["while(1",{"_index":212,"t":{"844":{"position":[[150,8],[1362,8]]}}}],["wh×w",{"_index":2421,"t":{"1413":{"position":[[612,4]]},"1486":{"position":[[612,4]]}}}],["wide",{"_index":1913,"t":{"1279":{"position":[[687,4]]},"1364":{"position":[[687,4]]}}}],["win='win_id",{"_index":1227,"t":{"1114":{"position":[[137,13],[247,13],[322,13]]},"1116":{"position":[[68,13],[213,13]]}}}],["window",{"_index":2768,"t":{"1507":{"position":[[0,20]]}}}],["winograd",{"_index":1155,"t":{"1083":{"position":[[812,8]]}}}],["wisdom",{"_index":1897,"t":{"1279":{"position":[[13,6]]},"1364":{"position":[[13,6]]}}}],["wise",{"_index":421,"t":{"934":{"position":[[1086,10]]},"1055":{"position":[[1076,4]]},"1057":{"position":[[1129,4],[1253,15]]},"1069":{"position":[[441,4],[599,4]]},"1181":{"position":[[1076,4]]},"1183":{"position":[[1129,4],[1253,15]]},"1195":{"position":[[441,4],[599,4]]},"1306":{"position":[[1086,10]]},"1551":{"position":[[73,7]]}}}],["wise）以及逐点（point",{"_index":2860,"t":{"1551":{"position":[[57,15]]}}}],["without",{"_index":1894,"t":{"1277":{"position":[[79,7]]},"1324":{"position":[[295,7]]},"1340":{"position":[[253,7]]},"1362":{"position":[[79,7]]},"1410":{"position":[[1581,7]]},"1421":{"position":[[253,7]]},"1454":{"position":[[7421,8]]},"1457":{"position":[[164,7]]},"1483":{"position":[[1581,7]]}}}],["wi​∈ck",{"_index":1200,"t":{"1101":{"position":[[640,7]]}}}],["wi​∈cp",{"_index":1219,"t":{"1103":{"position":[[749,7]]}}}],["wi∈ckp_i^{\\prime}=\\frac{p_i}{\\sum_{w_j",{"_index":1195,"t":{"1101":{"position":[[536,38]]}}}],["wi∈cpp_i^{\\prime}=\\frac{p_i}{\\sum_{w_j",{"_index":1216,"t":{"1103":{"position":[[645,38]]}}}],["wl",{"_index":2930,"t":{"1592":{"position":[[321,65]]}}}],["wl/wait",{"_index":2929,"t":{"1592":{"position":[[305,10]]}}}],["wnli（winograd",{"_index":1153,"t":{"1083":{"position":[[781,13]]}}}],["word",{"_index":424,"t":{"936":{"position":[[0,9]]},"1085":{"position":[[186,4]]},"1308":{"position":[[0,9]]}}}],["work",{"_index":122,"t":{"840":{"position":[[681,5]]},"1279":{"position":[[289,5],[950,4]]},"1324":{"position":[[708,5]]},"1364":{"position":[[289,5],[950,4]]},"1410":{"position":[[1614,4],[1725,4]]},"1454":{"position":[[7507,5]]},"1461":{"position":[[106,4]]},"1469":{"position":[[207,5]]},"1483":{"position":[[1614,4],[1725,4]]}}}],["work.109",{"_index":145,"t":{"840":{"position":[[1040,9]]}}}],["workshop",{"_index":2257,"t":{"1354":{"position":[[480,10]]}}}],["worship",{"_index":506,"t":{"950":{"position":[[78,7]]}}}],["write",{"_index":192,"t":{"840":{"position":[[1925,5]]},"846":{"position":[[869,5]]}}}],["written",{"_index":2603,"t":{"1454":{"position":[[3928,7]]}}}],["www和偏置项bbb",{"_index":1721,"t":{"1263":{"position":[[458,39]]}}}],["w∥12​≤θ(1",{"_index":1578,"t":{"1214":{"position":[[142,11]]}}}],["w∥12≤θ(1)min",{"_index":1570,"t":{"1214":{"position":[[22,13]]}}}],["w为bbox的宽高，c为该bbox是否存在object",{"_index":2879,"t":{"1556":{"position":[[338,30]]}}}],["w归约为文法开始符号",{"_index":364,"t":{"898":{"position":[[103,31]]}}}],["x",{"_index":12,"t":{"803":{"position":[[100,1],[124,2],[147,1],[155,1]]},"805":{"position":[[91,2],[114,1],[131,1]]},"955":{"position":[[308,1],[414,1]]},"1201":{"position":[[43,3],[304,1]]},"1203":{"position":[[43,1],[51,1],[55,1],[179,1],[191,1],[295,1]]},"1270":{"position":[[382,3],[873,3],[1092,2],[1135,1],[1157,1],[1162,2],[1171,1],[1917,3],[1986,2]]},"1441":{"position":[[291,1],[343,1],[386,4],[504,4]]},"1445":{"position":[[142,5],[278,4],[506,5]]}}}],["x)=11+e−x(1)\\sigma(x",{"_index":1461,"t":{"1201":{"position":[[0,23]]}}}],["x)=\\mathbb{e}_{\\varepsilon",{"_index":1939,"t":{"1285":{"position":[[212,27]]},"1370":{"position":[[212,27]]}}}],["x)\\mu(x)μ(x",{"_index":2203,"t":{"1347":{"position":[[1716,14],[1816,14]]},"1428":{"position":[[1717,14],[1817,14]]}}}],["x)\\sigma(x)σ(x",{"_index":2204,"t":{"1347":{"position":[[1733,17],[1836,18]]},"1428":{"position":[[1734,17],[1837,18]]}}}],["x,y)(x",{"_index":521,"t":{"953":{"position":[[0,41],[90,15]]}}}],["x.reshap",{"_index":1744,"t":{"1270":{"position":[[393,10]]}}}],["x.shape[0",{"_index":1821,"t":{"1270":{"position":[[2099,11]]}}}],["x.to(devic",{"_index":1776,"t":{"1270":{"position":[[1139,13],[1175,12],[1993,13]]}}}],["x0\\mathbf{x}_0x0​和ϵ\\epsilonϵ根据权重αˉ1,αˉ2,...αˉt\\bar{\\alpha}_1,\\bar{\\alpha}_2,...\\bar{\\alpha}_tαˉ1​,αˉ2​,...αˉt​做weight",{"_index":686,"t":{"1007":{"position":[[441,122]]}}}],["x0x_0x0",{"_index":1974,"t":{"1287":{"position":[[436,8]]},"1372":{"position":[[436,8]]}}}],["x0∼p(x∣z)x_0",{"_index":1975,"t":{"1287":{"position":[[448,12]]},"1372":{"position":[[448,12]]}}}],["x1",{"_index":2475,"t":{"1441":{"position":[[381,2],[611,2]]}}}],["x1,...,xt−1][x_1,...,x_{t",{"_index":2451,"t":{"1433":{"position":[[85,26]]}}}],["x1,x2,...,xt−1)(x_1",{"_index":2400,"t":{"1410":{"position":[[1126,21]]},"1483":{"position":[[1126,21]]}}}],["x1[0",{"_index":2501,"t":{"1441":{"position":[[1195,21],[1232,20]]}}}],["x1[0,:,:]+x2[0,:,:](1)x1[0",{"_index":2486,"t":{"1441":{"position":[[846,27]]}}}],["x1[1,:,:]+x2[1,:,:](2)x1[1",{"_index":2489,"t":{"1441":{"position":[[924,27]]}}}],["x1[2,:,:]+x2[2,:,:](3)x1[2",{"_index":2492,"t":{"1441":{"position":[[1002,27]]}}}],["x1与x2在第0维度上维度相同，所以python",{"_index":2485,"t":{"1441":{"position":[[798,47]]}}}],["x2",{"_index":2478,"t":{"1441":{"position":[[499,2],[616,2]]}}}],["x2[0",{"_index":2487,"t":{"1441":{"position":[[882,5],[1220,8]]}}}],["x2[0,0",{"_index":2502,"t":{"1441":{"position":[[1256,15]]}}}],["x2[1",{"_index":2490,"t":{"1441":{"position":[[960,5]]}}}],["x2[2",{"_index":2493,"t":{"1441":{"position":[[1038,5]]}}}],["x86",{"_index":77,"t":{"830":{"position":[[89,12]]},"840":{"position":[[835,5]]}}}],["x86/elf",{"_index":131,"t":{"840":{"position":[[854,7]]}}}],["x86_64与i386",{"_index":115,"t":{"840":{"position":[[561,16]]}}}],["x<t=(x1,x2,…,xt−1)x_{<t}=\\left(x_1",{"_index":2317,"t":{"1386":{"position":[[281,35]]}}}],["x=(x1,x2,…,xn×n)x=(x_1,x_2,\\ldots,x_{n\\tim",{"_index":2406,"t":{"1410":{"position":[[1925,44]]},"1483":{"position":[[1925,44]]}}}],["x=(x1,x2,…,xt)x=\\left(x_1",{"_index":2312,"t":{"1386":{"position":[[131,26]]}}}],["x=σ∗z+μx=\\sigma",{"_index":2099,"t":{"1336":{"position":[[106,15]]}}}],["x\\mathbf{x}x",{"_index":2579,"t":{"1454":{"position":[[2158,12],[6649,12]]}}}],["x^θ\\hat{x}_\\thetax",{"_index":2641,"t":{"1454":{"position":[[6053,33]]}}}],["x_2",{"_index":2313,"t":{"1386":{"position":[[158,4],[317,4]]},"1410":{"position":[[1148,4]]},"1483":{"position":[[1148,4]]}}}],["x_t\\right)x=(x1​,x2​,…,xt",{"_index":2314,"t":{"1386":{"position":[[171,27]]}}}],["x_{<i",{"_index":2326,"t":{"1386":{"position":[[625,6]]}}}],["x_{<t}\\right)p(x)=t=1∏t​p(xt​∣x<t",{"_index":2322,"t":{"1386":{"position":[[481,35]]}}}],["x_{<t}\\right)p(xt​∣x<t",{"_index":2316,"t":{"1386":{"position":[[243,24]]}}}],["x_{pr",{"_index":2688,"t":{"1454":{"position":[[8417,6],[8459,6]]}}}],["x_{t",{"_index":2318,"t":{"1386":{"position":[[330,4]]},"1410":{"position":[[1158,4]]},"1483":{"position":[[1158,4]]}}}],["xavier",{"_index":1335,"t":{"1138":{"position":[[761,6]]}}}],["xavier和kaim",{"_index":1339,"t":{"1138":{"position":[[845,58]]}}}],["xgen=x^θ(ϵ,c)x_{gen",{"_index":2646,"t":{"1454":{"position":[[6224,20]]}}}],["xi",{"_index":1724,"t":{"1263":{"position":[[621,2]]}}}],["xi,jx_{i",{"_index":2328,"t":{"1386":{"position":[[666,9]]}}}],["xix_ixi",{"_index":2195,"t":{"1347":{"position":[[1321,8]]},"1428":{"position":[[1322,8]]}}}],["xlim=[1",{"_index":1802,"t":{"1270":{"position":[[1714,8]]}}}],["xor异或等非线性问题，导致第一次ai",{"_index":1728,"t":{"1265":{"position":[[43,35]]}}}],["xpr=x^θ(zt1,cpr)x_{pr",{"_index":2678,"t":{"1454":{"position":[[7898,22]]}}}],["xt=αˉtx+1−αˉtεx_t=\\sqrt{\\bar{\\alpha}_t",{"_index":1948,"t":{"1285":{"position":[[451,38]]},"1370":{"position":[[451,38]]}}}],["xt\\mathbf{x}_txt​代表上一步骤中输出的降噪后的图像，xt−1\\mathbf{x}_{t",{"_index":701,"t":{"1009":{"position":[[484,54]]}}}],["xtx_txt",{"_index":1958,"t":{"1285":{"position":[[754,8]]},"1370":{"position":[[754,8]]},"1386":{"position":[[378,8]]},"1410":{"position":[[1096,8]]},"1433":{"position":[[137,10]]},"1483":{"position":[[1096,8]]}}}],["xt−1=1αt(xt−1−αt1−αˉtϵθ(xt,t))+σtz(2)\\mathbf{x}_{t",{"_index":696,"t":{"1009":{"position":[[203,50]]}}}],["xt−1=1αt(xt−1−αt1−αˉtϵθ(xt∣t,z))+σtδx_{t",{"_index":1961,"t":{"1287":{"position":[[56,40]]},"1372":{"position":[[56,40]]}}}],["xt∣t,z)\\varepsilon_\\theta\\left(x_t",{"_index":1956,"t":{"1285":{"position":[[677,40]]},"1370":{"position":[[677,40]]}}}],["xt∼n(0,i)x_t",{"_index":1970,"t":{"1287":{"position":[[377,12]]},"1372":{"position":[[377,12]]}}}],["xxl",{"_index":2677,"t":{"1454":{"position":[[7465,3],[7587,3],[8822,4]]}}}],["xxx",{"_index":946,"t":{"1059":{"position":[[109,3]]},"1185":{"position":[[109,3]]},"1285":{"position":[[141,3]]},"1347":{"position":[[420,6],[782,6],[1705,3],[1757,3]]},"1370":{"position":[[141,3]]},"1386":{"position":[[399,3]]},"1428":{"position":[[420,6],[782,6],[1706,3],[1758,3]]},"1454":{"position":[[4381,13]]}}}],["xxx.plasmoid",{"_index":2802,"t":{"1514":{"position":[[349,12]]}}}],["xxxxxxxxxx19",{"_index":1687,"t":{"1251":{"position":[[353,12]]}}}],["xxx的可能取值为x=x1,x2,...,xnx=x_1,x_2,...,x_nx=x1​,x2​,...,xn​，而取值事件xix_ixi​发生的概率为pip_ipi",{"_index":1503,"t":{"1208":{"position":[[152,106]]}}}],["x{\\sqrt{\\frac1n\\sum_{i=1}^nx_i^2}}*grmsnorm(x)=n1​∑i=1n​xi2​​x​∗g",{"_index":2172,"t":{"1347":{"position":[[354,65]]},"1428":{"position":[[354,65]]}}}],["x}+\\sqrt{1",{"_index":1949,"t":{"1285":{"position":[[490,10]]},"1370":{"position":[[490,10]]}}}],["x∈rdx",{"_index":1931,"t":{"1285":{"position":[[19,5]]},"1370":{"position":[[19,5]]}}}],["x为100时，sigmoid(x)就接近于0",{"_index":1475,"t":{"1201":{"position":[[353,24]]}}}],["x和i",{"_index":525,"t":{"953":{"position":[[124,26]]}}}],["x方向和i",{"_index":577,"t":{"966":{"position":[[17,24]]}}}],["y",{"_index":14,"t":{"803":{"position":[[111,1],[134,1],[138,1],[175,2]]},"805":{"position":[[101,1],[105,1]]},"955":{"position":[[323,1],[429,1]]},"1206":{"position":[[172,1]]},"1270":{"position":[[1095,1],[1188,1],[1237,3],[1317,3],[1361,2],[1921,2],[1989,1],[2051,2],[2127,3]]},"1445":{"position":[[148,4],[292,4],[512,4]]},"1556":{"position":[[302,2]]}}}],["y)(x,i",{"_index":524,"t":{"953":{"position":[[106,17]]}}}],["y)(x,y)方向上以及亮度函数f(x,y)f(x",{"_index":522,"t":{"953":{"position":[[42,26]]}}}],["y)(x,y)，点qqq的坐标为(s,t)(s,t)(s,t",{"_index":529,"t":{"955":{"position":[[138,32]]}}}],["y)f(x,i",{"_index":523,"t":{"953":{"position":[[69,16]]}}}],["y*z",{"_index":1718,"t":{"1263":{"position":[[401,4]]}}}],["y.numel",{"_index":1779,"t":{"1270":{"position":[[1241,10],[2131,10]]}}}],["y.to(devic",{"_index":1777,"t":{"1270":{"position":[[1192,12],[2007,12]]}}}],["y2",{"_index":1240,"t":{"1116":{"position":[[192,5]]}}}],["y_hat",{"_index":1815,"t":{"1270":{"position":[[2020,5]]}}}],["yay",{"_index":100,"t":{"838":{"position":[[87,3]]},"1509":{"position":[[28,3]]}}}],["yi",{"_index":1722,"t":{"1263":{"position":[[590,2],[616,2],[636,2]]}}}],["yi,j=∑a,bva,b∗xi+a,j+b=∑a=−δδ∑b=−δδva,b∗xia,j+b(4)y_{i,j}=\\sum_{a,b}{v_{a,b}*x_{i+a,j+b}}=\\sum_{a",{"_index":1667,"t":{"1242":{"position":[[32,98]]}}}],["yi,j=∑a,bvi,j,a,b∗xi+a,j+b=∑a,bva,b∗xi+a,j+b(3)y_{i,j}=\\sum_{a,b}{v_{i,j,a,b}*x_{i+a,j+b}}=\\sum_{a,b}{v_{a,b}*x_{i+a,j+b",{"_index":1664,"t":{"1240":{"position":[[513,122]]}}}],["yi,j=∑h,wwi,j,h,w∗xh,w(1)y_{i,j}=\\sum_{h,w}{w_{i,j,h,w}*x_{h,w",{"_index":1658,"t":{"1240":{"position":[[77,64]]}}}],["yi,j=∑h,wwi,j,h,w∗xh,w=∑a,bvi,j,a,b∗xi+a,j+b(2)y_{i,j}=\\sum_{h,w}{w_{i,j,h,w}*x_{h,w}}=\\sum_{a,b}{v_{i,j,a,b}*x_{i+a,j+b",{"_index":1661,"t":{"1240":{"position":[[241,122]]}}}],["yijun",{"_index":2261,"t":{"1354":{"position":[[516,5]]}}}],["yiy_{i}yi",{"_index":1119,"t":{"1081":{"position":[[383,20]]}}}],["yolov1",{"_index":2865,"t":{"1556":{"position":[[43,14],[58,14],[523,25]]}}}],["yolov2引入了anchor机制代替bbox，将图像划分为13×1313",{"_index":2892,"t":{"1558":{"position":[[30,37]]}}}],["yolov5使用cspnet实现特征融合，csp",{"_index":2898,"t":{"1560":{"position":[[9,52]]}}}],["yolo损失函数分为分类损失以及回归损失，可以在分类损失中引入foc",{"_index":2917,"t":{"1572":{"position":[[50,36]]}}}],["yong",{"_index":2266,"t":{"1354":{"position":[[554,4]]}}}],["yyi",{"_index":948,"t":{"1059":{"position":[[146,71]]},"1185":{"position":[[146,71]]}}}],["yyy是样本的真实标签，zzz",{"_index":1720,"t":{"1263":{"position":[[435,22]]}}}],["y−f(x)∥2=∑[y−f(x)]22(6)\\vert",{"_index":1490,"t":{"1206":{"position":[[142,29]]}}}],["y为bbox左上角坐标，h",{"_index":2878,"t":{"1556":{"position":[[323,14]]}}}],["y轴数据、x轴数据，win参数是窗口的唯一标识，opt可选字典中可以给出窗口的title和legend",{"_index":1225,"t":{"1114":{"position":[[49,66]]}}}],["z",{"_index":894,"t":{"1055":{"position":[[179,1],[243,1],[254,1],[272,3],[276,1],[323,3],[349,1],[428,1],[764,1],[800,1],[837,1],[883,1]]},"1181":{"position":[[179,1],[243,1],[254,1],[272,3],[276,1],[323,3],[349,1],[428,1],[764,1],[800,1],[837,1],[883,1]]},"1336":{"position":[[124,1]]},"1445":{"position":[[153,4],[306,4],[517,4]]}}}],["z(k)z(k)z(k",{"_index":585,"t":{"973":{"position":[[396,33]]}}}],["z)p(x∣z",{"_index":1937,"t":{"1285":{"position":[[167,9]]},"1287":{"position":[[26,8]]},"1370":{"position":[[167,9]]},"1372":{"position":[[26,8]]}}}],["z)x0​∼p(x∣z",{"_index":1977,"t":{"1287":{"position":[[475,14]]},"1372":{"position":[[475,14]]}}}],["z=e(x)z=\\mathcal{e}(x)z=e(x",{"_index":2606,"t":{"1454":{"position":[[4419,28]]}}}],["z=w∗x+bz=w*x+bz=w∗x+b",{"_index":1715,"t":{"1263":{"position":[[217,60]]}}}],["z\\right)\\right)+\\sigma_t",{"_index":1965,"t":{"1287":{"position":[[211,24]]},"1372":{"position":[[211,24]]}}}],["z\\right)\\right\\|^2\\right]l(z,x)=eε,t​[∥ε−εθ​(xt​∣t,z)∥2",{"_index":1943,"t":{"1285":{"position":[[303,56]]},"1370":{"position":[[303,56]]}}}],["z\\right)εθ​(xt​∣t,z",{"_index":1957,"t":{"1285":{"position":[[726,20]]},"1370":{"position":[[726,20]]}}}],["zero",{"_index":816,"t":{"1038":{"position":[[31,4]]},"1164":{"position":[[31,4]]},"1354":{"position":[[820,4]]},"1406":{"position":[[1084,4],[1297,4]]},"1479":{"position":[[1084,4],[1297,4]]}}}],["zhang",{"_index":2243,"t":{"1354":{"position":[[229,6],[243,6],[595,6]]}}}],["zhou等人首先在图像分类任务中采用上下文优化，在词嵌入空间中对具有连续向量的上下文词进行建模。随后prompt",{"_index":2294,"t":{"1356":{"position":[[185,56]]}}}],["zi",{"_index":1723,"t":{"1263":{"position":[[595,2]]}}}],["zip",{"_index":1883,"t":{"1275":{"position":[[993,5],[999,39]]},"1445":{"position":[[0,68],[83,8],[160,15],[522,36]]}}}],["zip(*zipped_list",{"_index":2512,"t":{"1445":{"position":[[406,18]]}}}],["zip(a",{"_index":1692,"t":{"1251":{"position":[[427,6],[626,6]]},"1275":{"position":[[1099,6],[1293,6]]}}}],["zip(list1",{"_index":2507,"t":{"1445":{"position":[[191,10]]}}}],["zipped_list",{"_index":2506,"t":{"1445":{"position":[[176,12],[239,13]]}}}],["zip将list1、list2和list3",{"_index":2510,"t":{"1445":{"position":[[311,42]]}}}],["zizhao",{"_index":2244,"t":{"1354":{"position":[[236,6]]}}}],["zsh的配置文件：~/.zshrc",{"_index":2765,"t":{"1502":{"position":[[37,17]]}}}],["zt1∼n(0,i)z_{t_1",{"_index":2681,"t":{"1454":{"position":[[7985,17]]}}}],["zt:=αtx+σtϵz_t",{"_index":2649,"t":{"1454":{"position":[[6325,14]]}}}],["ztz_tzt",{"_index":2609,"t":{"1454":{"position":[[4517,8],[5230,8],[5316,8]]}}}],["zzz",{"_index":1935,"t":{"1285":{"position":[[135,3],[778,6]]},"1370":{"position":[[135,3],[778,6]]},"1410":{"position":[[263,3]]},"1454":{"position":[[4500,3]]},"1483":{"position":[[263,3]]}}}],["zzz带入阈值函数，如符号函数sign(z)sign(z)sign(z",{"_index":1716,"t":{"1263":{"position":[[278,56]]}}}],["z∈rdz",{"_index":1933,"t":{"1285":{"position":[[81,5]]},"1370":{"position":[[81,5]]}}}],["z空间与w",{"_index":885,"t":{"1055":{"position":[[0,8]]},"1181":{"position":[[0,8]]}}}],["z空间到w",{"_index":886,"t":{"1055":{"position":[[11,10]]},"1181":{"position":[[11,10]]}}}]],"pipeline":["stemmer"]}}]